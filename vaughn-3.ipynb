{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb900036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import typing\n",
    "import json\n",
    "import pathlib\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import transformers\n",
    "import transformers.modeling_outputs\n",
    "import transformers.configuration_utils\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f873bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_KAGGLE = \"KAGGLE_DOCKER_IMAGE\" in os.environ\n",
    "\n",
    "DATASETS = pathlib.Path(\n",
    "    \".\"\n",
    "    if not IS_KAGGLE\n",
    "    else \"/kaggle/input/influencers-or-observers-predicting-social-roles/Kaggle2025\"\n",
    ")\n",
    "\n",
    "DATASET_TRAIN = DATASETS / \"train.jsonl\"\n",
    "DATASET_KAGGLE = DATASETS / \"kaggle_test.jsonl\"\n",
    "\n",
    "CACHE_DIR = pathlib.Path(\".\")\n",
    "VERSION = \"v12-vaughn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fc921d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e20b6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f28f61",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e16f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: pathlib.Path, cache: bool = False) -> pd.DataFrame:\n",
    "    path_pq = (CACHE_DIR / path.name).with_stem(f\"{path.stem}_raw\").with_suffix(\".parquet\")\n",
    "    \n",
    "    if cache and path_pq.exists():\n",
    "        return pd.read_parquet(path_pq)\n",
    "    \n",
    "    # This leaves things to be desired, since there's no way to specify dtypes\n",
    "    # and it assumes float instead of int, causing a loss in precision...\n",
    "    # But I guess it only matters for ids, which we'll probably discard in preprocessing anyway\n",
    "    result = pd.json_normalize(list(map(json.loads, path.read_bytes().splitlines())))\n",
    "    \n",
    "    if cache:\n",
    "        result.to_parquet(path_pq)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d1df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_json(DATASET_TRAIN, cache=True)\n",
    "kaggle_data = load_json(DATASET_KAGGLE, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bcf662",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ea7df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # For technical reasons, any text columns we want to use should have no dots in their names.\n",
    "    # The simplest way to achieve this is to replace all dots indiscriminately.\n",
    "    \n",
    "    df = df.rename(columns=lambda x: x.replace(\".\", \"_\"))\n",
    "    \n",
    "    df[\"is_reply\"] = df[\"in_reply_to_status_id\"].notna()\n",
    "    \n",
    "    df = df.drop(columns=[\n",
    "        \"in_reply_to_status_id_str\",\n",
    "        # \"in_reply_to_status_id\",\n",
    "        \"in_reply_to_user_id_str\",\n",
    "        \"in_reply_to_user_id\",\n",
    "        \"quoted_status_id_str\",\n",
    "        \"quoted_status_id\",\n",
    "        \"id_str\",\n",
    "        \"quoted_status_in_reply_to_status_id_str\",\n",
    "        \"quoted_status_in_reply_to_status_id\",\n",
    "        \"quoted_status_in_reply_to_user_id_str\",\n",
    "        \"quoted_status_in_reply_to_user_id\",\n",
    "        \"quoted_status_id_str\",\n",
    "        \"quoted_status_id\",\n",
    "        \"quoted_status_user_id_str\",\n",
    "        \"quoted_status_user_id\",\n",
    "        # \"quoted_status_permalink_expanded\",\n",
    "        \"quoted_status_permalink_display\",\n",
    "        \"quoted_status_permalink_url\",\n",
    "        \"quoted_status_quoted_status_id\",\n",
    "        \"quoted_status_quoted_status_id_str\",\n",
    "        # \"quoted_status_place_id\",\n",
    "        # \"place_id\",\n",
    "        \"lang\",  # Always \"fr\"\n",
    "        \"retweeted\",  # Always False\n",
    "        \"filter_level\",  # Always \"low\"\n",
    "        \"geo\",  # Always None\n",
    "        \"place\",  # Always None\n",
    "        \"coordinates\",  # Always None\n",
    "        \"contributors\",  # Always None\n",
    "        \"quote_count\",  # Always 0\n",
    "        \"reply_count\",  # Always 0\n",
    "        \"retweet_count\",  # Always 0\n",
    "        \"favorite_count\",  # Always 0\n",
    "        \"favorited\",  # Always False\n",
    "        \"quoted_status_geo\",  # Always None\n",
    "        \"quoted_status_place\",  # Always None\n",
    "        \"quoted_status_coordinates\",  # Always None\n",
    "        \"quoted_status_retweeted\",  # Always False\n",
    "        \"quoted_status_filter_level\",  # Always \"low\"\n",
    "        \"quoted_status_contributors\",  # Always None\n",
    "        \"quoted_status_user_utc_offset\",  # Always None\n",
    "        \"quoted_status_user_lang\",  # Always None\n",
    "        \"quoted_status_user_time_zone\",  # Always None\n",
    "        \"quoted_status_user_follow_request_sent\",  # Always None\n",
    "        \"quoted_status_user_following\",  # Always None\n",
    "        \"quoted_status_user_notifications\",  # Always None\n",
    "        \"user_default_profile_image\",  # Always False\n",
    "        \"user_protected\",  # Always False\n",
    "        \"user_contributors_enabled\",  # Always False\n",
    "        \"user_lang\",  # Always None\n",
    "        \"user_notifications\",  # Always None\n",
    "        \"user_following\",  # Always None\n",
    "        \"user_utc_offset\",  # Always None\n",
    "        \"user_time_zone\",  # Always None\n",
    "        \"user_follow_request_sent\",  # Always None\n",
    "    ])\n",
    "    \n",
    "    df[\"full_text\"] = df.apply(lambda tweet: extract_full_text(tweet), axis=1)\n",
    "    \n",
    "    source_split = df[\"source\"].str.removeprefix(\"<a href=\\\"\").str.removesuffix(\"</a>\").str.split(\"\\\" rel=\\\"nofollow\\\">\").map(lambda x: x if len(x) == 2 else pd.NA)\n",
    "    df[\"source_url\"] = source_split.map(lambda x: x[0], na_action=\"ignore\")\n",
    "    df[\"source_name\"] = source_split.map(lambda x: x[1], na_action=\"ignore\")\n",
    "    \n",
    "    df[\"misc_text\"] = df.apply(\n",
    "        lambda x: \"via: {0}; reply: @{1}; quote: @{2} {3}\".format(x[\"source_name\"], x[\"in_reply_to_screen_name\"], x[\"quoted_status_user_screen_name\"], x[\"quoted_status_user_name\"]), axis=1,\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_full_text(tweet: pd.Series) -> str:\n",
    "    text: str = tweet[\"text\"]\n",
    "    \n",
    "    if not pd.isna(tweet[\"extended_tweet_full_text\"]):\n",
    "        text = tweet[\"extended_tweet_full_text\"]\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1ee966",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(\"label\", axis=1)\n",
    "y_train = train_data[\"label\"]\n",
    "\n",
    "X_kaggle = kaggle_data\n",
    "\n",
    "X_train = preprocess(X_train)\n",
    "X_kaggle = preprocess(X_kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c8d7e",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda51842",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be1585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Made this a class to hold all the caches. It may resemble an nn.Module, but isn't one!\n",
    "class FeatureExtractor:\n",
    "    text_encoder_name: str | None\n",
    "    text_tokenizer: nn.Module | None\n",
    "    text_encoder: nn.Module | None\n",
    "    text_config: transformers.configuration_utils.PretrainedConfig | None\n",
    "    text_enc_cache_path: pathlib.Path | None\n",
    "    \n",
    "    # New attribute to hold pre-computed embeddings\n",
    "    text_encodings: dict[str, dict[str, torch.Tensor]]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        text_encoder_name: str | None = None,\n",
    "        text_enc_cache_path: pathlib.Path | None = None,\n",
    "        device: torch.device = device,\n",
    "    ):\n",
    "        # super().__init__() # Removed this line as FeatureExtractor is not an nn.Module\n",
    "        self.device = device\n",
    "        self.means = None\n",
    "        self.stds = None\n",
    "        self.afm_cache = {}\n",
    "        self.text_enc_cache_path = text_enc_cache_path\n",
    "        self.text_encoder_name = text_encoder_name\n",
    "        self.train() # Default to training mode\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "    \n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            \"means\": self.means,\n",
    "            \"stds\": self.stds,\n",
    "            \"afm_cache\": self.afm_cache,\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.means = state_dict[\"means\"]\n",
    "        self.stds = state_dict[\"stds\"]\n",
    "        self.afm_cache = state_dict[\"afm_cache\"]\n",
    "    \n",
    "    def dims(self) -> dict[str, int]:\n",
    "        return {\n",
    "            \"md\": len(self.METADATA_FIELDS),\n",
    "        } | {\n",
    "            field: self.embed_size\n",
    "            for field in self.TEXT_FIELDS\n",
    "        }\n",
    "    \n",
    "    def extract(self, df: pd.DataFrame, split_name: str) -> dict[str, typing.Union[torch.Tensor, pd.Series]]:\n",
    "        \"\"\"\n",
    "        Extracts features. Now returns metadata tensor and raw text Series \n",
    "        for deferred encoding (i.e., inside the DataLoader loop).\n",
    "        \"\"\"\n",
    "        result: dict[str, typing.Union[torch.Tensor, pd.Series]] = {}\n",
    "\n",
    "        # 1. Metadata extraction (Keep as-is)\n",
    "        result[\"md\"] = self.extract_raw_metadata(df) \n",
    "\n",
    "        # 2. Text data (The raw text data is returned instead of an embedded tensor)\n",
    "        for col_name in self.TEXT_FIELDS:\n",
    "            # Apply fillna to ensure no NaNs in the list passed to the tokenizer\n",
    "            # This is critical for batch tokenization later\n",
    "            result[col_name] = df[col_name].fillna(\"\") \n",
    "\n",
    "        return result\n",
    "    \n",
    "    def extract_raw_metadata(self, df: pd.DataFrame) -> torch.Tensor:\n",
    "        # ... (Keep existing implementation of extract_raw_metadata)\n",
    "        md_cols: list[pd.Series] = []\n",
    "\n",
    "        for fn, col_name in tqdm(self.METADATA_FIELDS, desc=\"Extracting metadata\"):\\\n",
    "            md_cols.append(fn(self, df[col_name]))\n",
    "        \n",
    "        md: pd.DataFrame = pd.concat(md_cols, axis=1)\n",
    "        \n",
    "        if self.training:\n",
    "            self.means = md.mean().fillna(0)\n",
    "            self.stds = md.std().fillna(1)\n",
    "            self.stds = self.stds.replace(0, 1)\n",
    "        \n",
    "        assert self.means is not None and self.stds is not None, \"You forgot to train/load the feature extractor\"\n",
    "\n",
    "        md = (md - self.means) / self.stds\n",
    "\n",
    "        return torch.from_numpy(md.to_numpy()).float().to(self.device)\n",
    "    \n",
    "    def apply_fill_mean(\n",
    "        self,\n",
    "        col: pd.Series,\n",
    "        func: typing.Callable[[typing.Any], typing.Any],\n",
    "    ) -> pd.Series:\n",
    "        col = col.map(func, na_action=\"ignore\")\n",
    "        \n",
    "        key = (col.name, func.__name__)\n",
    "        if self.training:\n",
    "            self.afm_cache[key] = col.mean()\n",
    "        assert key in self.afm_cache, \"You forgot to train/load the feature extractor\"\n",
    "        \n",
    "        return col.fillna(self.afm_cache[key])\n",
    "    \n",
    "    def md_bool(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(lambda x: (1 if x else -1), na_action=\"ignore\").fillna(0)\n",
    "\n",
    "    def md_len(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(len, na_action=\"ignore\").fillna(0)\n",
    "\n",
    "    def md_time(self, col: pd.Series) -> pd.Series:\n",
    "        return self.apply_fill_mean(col, lambda x: time.mktime(time.strptime(x, \"%a %b %d %H:%M:%S %z %Y\")))\n",
    "\n",
    "    def md_num(self, col: pd.Series) -> pd.Series:\n",
    "        return self.apply_fill_mean(col, pd.to_numeric)\n",
    "\n",
    "    def md_place(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(lambda x: int(x, 16), na_action=\"ignore\").fillna(0)\n",
    "    \n",
    "    METADATA_FIELDS: list[tuple[typing.Callable[[FeatureExtractor, pd.Series], pd.Series], str]] = [\n",
    "        (md_bool, \"is_quote_status\"),\n",
    "        (md_bool, \"is_reply\"),\n",
    "        (md_bool, \"possibly_sensitive\"),\n",
    "        (md_bool, \"quoted_status_user_verified\"),\n",
    "        (md_bool, \"user_is_translator\"),\n",
    "        (md_bool, \"user_geo_enabled\"),\n",
    "        (md_bool, \"user_profile_use_background_image\"),\n",
    "        (md_bool, \"user_default_profile\"),\n",
    "        \n",
    "        (md_len, \"full_text\"),\n",
    "        (md_len, \"source_name\"),\n",
    "        (md_len, \"in_reply_to_screen_name\"),\n",
    "        (md_len, \"quoted_status_extended_tweet_entities_urls\"),\n",
    "        (md_len, \"quoted_status_extended_tweet_entities_user_mentions\"),\n",
    "        (md_len, \"quoted_status_extended_tweet_full_text\"),\n",
    "        (md_len, \"quoted_status_entities_urls\"),\n",
    "        (md_len, \"quoted_status_user_profile_image_url_https\"),\n",
    "        (md_len, \"quoted_status_user_profile_background_image_url\"),\n",
    "        (md_len, \"quoted_status_user_profile_background_image_url_https\"),\n",
    "        (md_len, \"quoted_status_user_screen_name\"),\n",
    "        (md_len, \"quoted_status_user_name\"),\n",
    "        (md_len, \"entities_hashtags\"),\n",
    "        (md_len, \"entities_user_mentions\"),\n",
    "        (md_len, \"user_profile_image_url_https\"),\n",
    "        (md_len, \"user_profile_background_image_url\"),\n",
    "        (md_len, \"user_description\"),\n",
    "        (md_len, \"user_translator_type\"),\n",
    "        (md_len, \"user_url\"),\n",
    "        (md_len, \"user_profile_banner_url\"),\n",
    "        (md_len, \"user_location\"),\n",
    "        (md_len, \"display_text_range\"),\n",
    "        (md_len, \"extended_tweet_entities_urls\"),\n",
    "        (md_len, \"extended_tweet_entities_hashtags\"),\n",
    "        (md_len, \"extended_tweet_entities_user_mentions\"),\n",
    "        (md_len, \"quoted_status_permalink_expanded\"),\n",
    "        \n",
    "        (md_time, \"created_at\"),\n",
    "        (md_time, \"user_created_at\"),\n",
    "        (md_time, \"quoted_status_created_at\"),\n",
    "        (md_time, \"quoted_status_user_created_at\"),\n",
    "        \n",
    "        (md_num, \"user_statuses_count\"),\n",
    "        (md_num, \"user_listed_count\"),\n",
    "        (md_num, \"user_favourites_count\"),\n",
    "        (md_num, \"user_profile_background_tile\"),\n",
    "        (md_num, \"quoted_status_quote_count\"),\n",
    "        (md_num, \"quoted_status_user_followers_count\"),\n",
    "        (md_num, \"quoted_status_user_favourites_count\"),\n",
    "        (md_num, \"in_reply_to_status_id\"),\n",
    "        \n",
    "        (md_place, \"quoted_status_place_id\"),\n",
    "        (md_place, \"place_id\"),\n",
    "    ]\n",
    "\n",
    "    TEXT_FIELDS: list[str] = [\n",
    "        \"full_text\",\n",
    "        \"user_description\",\n",
    "        \"misc_text\",\n",
    "        # \"source_name\",\n",
    "        # \"in_reply_to_screen_name\",\n",
    "        # \"quoted_status_user_screen_name\",\n",
    "        # \"quoted_status_user_name\",\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bc90d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In TweetDataset class (Cell 1bc90d5a)\n",
    "class TweetDataset(Dataset):\n",
    "    # Update features type hint\n",
    "    features: dict[str, typing.Union[torch.Tensor, pd.Series]] \n",
    "    labels: torch.Tensor\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        features: dict[str, typing.Union[torch.Tensor, pd.Series]],\n",
    "        labels: pd.Series,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        self.features = features\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long, device=device)\n",
    "        # Identify which features are raw text (Pandas Series)\n",
    "        self.raw_text_keys = [k for k, v in features.items() if isinstance(v, pd.Series)] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features[\"md\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"features\": {\n",
    "                # Fetch tensor slice or raw string from Series\n",
    "                key: val.iloc[idx] if key in self.raw_text_keys else val[idx] \n",
    "                for key, val in self.features.items()\n",
    "            },\n",
    "            \"label\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "# Replace existing `collate_fn` (Cell 1bc90d5a)\n",
    "def make_collate_fn(\n",
    "    tokenizer: AutoTokenizer, \n",
    "    device: torch.device, \n",
    "    text_fields: list[tuple[str, int | None]], \n",
    "    max_length: int,\n",
    ") -> typing.Callable:\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        # 1. Stack metadata tensors (already on device)\n",
    "        md_features = torch.stack([x[\"features\"][\"md\"] for x in batch])\n",
    "\n",
    "        output_features = {\"md\": md_features}\n",
    "\n",
    "        # 2. Batch tokenize raw text strings\n",
    "        for col_name in text_fields:\n",
    "            raw_text_batch = [x[\"features\"][col_name] for x in batch]\n",
    "\n",
    "            # Tokenize the batch of strings for this column\n",
    "            tokenized = tokenizer(\n",
    "                raw_text_batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=max_length,\n",
    "            ).to(device)\n",
    "\n",
    "            # Store the tokenized output (input_ids and attention_mask)\n",
    "            output_features[col_name] = tokenized \n",
    "\n",
    "        labels = torch.stack([x[\"label\"] for x in batch])\n",
    "        return output_features, labels\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "\n",
    "class TweetClassifier(nn.Module):\n",
    "    feature_sizes: dict[str, int]\n",
    "    \n",
    "    layer1: nn.ModuleDict\n",
    "    fc2: nn.Linear\n",
    "    fc3: nn.Linear\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        text_encoder_name: str, # NEW: to load the encoder\n",
    "        feature_sizes: dict[str, int],\n",
    "        text_fields: list[str],\n",
    "        hidden_dim: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. NEW: Load and register the text encoder\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_encoder_name)\n",
    "        self.text_config = self.text_encoder.config\n",
    "        self.text_fields = text_fields\n",
    "        self.embed_size = self.text_config.hidden_size\n",
    "\n",
    "        # 2. Adjust feature sizes for text embeddings\n",
    "        # The size will now be the BERT hidden size if compression is None\n",
    "        text_feature_sizes = {\n",
    "             field: self.embed_size\n",
    "             for field in text_fields\n",
    "        }\n",
    "        # Add text feature sizes to the metadata size\n",
    "        feature_sizes = feature_sizes | text_feature_sizes\n",
    "\n",
    "        self.feature_sizes = feature_sizes\n",
    "\n",
    "        # 3. Initialize rest of the model (as before)\n",
    "        self.layer1 = nn.ModuleDict()\n",
    "        for name, size in self.feature_sizes.items():\n",
    "            self.layer1[name] = nn.Sequential(\n",
    "                nn.Linear(size, hidden_dim), # Output of this sequential block is hidden_dim (512)\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        \n",
    "        def _add(name, dropout: float):\n",
    "            self.layer1[name] = nn.Sequential(\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(feature_sizes[name], hidden_dim),\n",
    "            )\n",
    "        \n",
    "        _add(\"md\", 0.1)\n",
    "        _add(\"full_text\", 0.1)\n",
    "        _add(\"user_description\", 0.37)\n",
    "        _add(\"misc_text\", 0.3)\n",
    "\n",
    "        total_input_dim = len(self.feature_sizes) * hidden_dim\n",
    "        \n",
    "        self.fc2 = nn.Linear(total_input_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, NUM_CLASSES)\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def forward(self, features: dict[str, typing.Union[torch.Tensor, dict[str, torch.Tensor]]]) -> dict[str, torch.Tensor]:\n",
    "        \n",
    "        # 1. Process text data using the encoder (this is where training happens)\n",
    "        text_embeddings = {}\n",
    "\n",
    "        for col_name in self.text_fields:\n",
    "            if col_name not in features: continue\n",
    "\n",
    "            tokenized_inputs = features[col_name] # input_ids, attention_mask\n",
    "\n",
    "            # Run through BERT\n",
    "            # Since the model is in .train() mode by default, gradients are tracked here.\n",
    "            outputs: transformers.modeling_outputs.BaseModelOutput = self.text_encoder(**tokenized_inputs)\n",
    "            last_hidden: torch.Tensor = outputs.last_hidden_state\n",
    "            mask: torch.Tensor = tokenized_inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "\n",
    "            # Mean pooling logic\n",
    "            masked_hidden = last_hidden * mask\n",
    "            summed = masked_hidden.sum(dim=1)\n",
    "            counts = mask.sum(dim=1)\n",
    "\n",
    "            # Handle division by zero for empty sequences\n",
    "            embeddings = summed / torch.clamp(counts, min=1e-9) \n",
    "\n",
    "            # NOTE: Any compression (like PCA) must be replaced by a trainable layer (Linear)\n",
    "            # or skipped, as PCA breaks the gradient flow. Assuming compress=None for now.\n",
    "\n",
    "            text_embeddings[col_name] = embeddings\n",
    "\n",
    "        # 2. Process metadata and combine features (as before)\n",
    "        x_processed = []\n",
    "\n",
    "        # Process metadata\n",
    "        x_processed.append(self.layer1[\"md\"](features[\"md\"]))\n",
    "\n",
    "        # Process text features (now embeddings)\n",
    "        for col_name in text_embeddings.keys():\n",
    "            x_processed.append(self.layer1[col_name](text_embeddings[col_name]))\n",
    "\n",
    "        x = torch.cat(x_processed, dim=-1)\n",
    "\n",
    "        # 3. Final layers\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "\n",
    "        return {\"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7a47ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: TweetClassifier,\n",
    "    train_ds: Dataset,\n",
    "    val_ds: Dataset,\n",
    "    epochs: int = 3,\n",
    "    lr: float = 2e-4,\n",
    "    weight_decay: float = 0.01,  # TODO: Lower?\n",
    "    max_grad_norm: float = 1.0,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    "    optimizer: torch.optim.Optimizer | None = None,\n",
    "    checkpoints_path: pathlib.Path | str | None = \".\",\n",
    "    return_best: bool = False,\n",
    ") -> TweetClassifier:\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    if optimizer is None:\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_file: pathlib.Path | None = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        status_bar = tqdm(train_loader, desc=\"Training\")\n",
    "\n",
    "        for features, labels in status_bar:\n",
    "            features: dict[str, torch.Tensor]\n",
    "            labels: torch.Tensor\n",
    "            features = {k: v.to(device) for k, v in features.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            out = model(features)\n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            status_bar.set_postfix({\"loss\": total_loss / (status_bar.n + 1)})\n",
    "        \n",
    "        print(f\"Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        val_metrics = evaluate_model(\n",
    "            model=model,\n",
    "            val_ds=val_ds,\n",
    "            device=device,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['acc']:.4f}\")\n",
    "\n",
    "        if checkpoints_path is not None:\n",
    "            ckpt = pathlib.Path(checkpoints_path) / f\"epoch_{epoch:02}.pt\"\n",
    "            torch.save(model.state_dict(), ckpt)\n",
    "            print(f\"Checkpoint saved to {ckpt}\")\n",
    "            \n",
    "            if val_metrics[\"loss\"] < best_val_loss:\n",
    "                best_val_loss = val_metrics[\"loss\"]\n",
    "                best_model_file = ckpt\n",
    "\n",
    "    if return_best and best_model_file is not None:\n",
    "        print(f\"Best model: {best_model_file}\")\n",
    "        model.load_state_dict(torch.load(best_model_file))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: TweetClassifier,\n",
    "    val_ds: Dataset,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> tuple[float, float]:\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        status_bar = tqdm(val_loader, desc=\"Evaluating\")\n",
    "        \n",
    "        for features, labels in status_bar:\n",
    "            features: dict[str, torch.Tensor]\n",
    "            labels: torch.Tensor\n",
    "            features = {k: v.to(device) for k, v in features.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = model(features)\n",
    "            logits: torch.Tensor = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            count += labels.size(0)\n",
    "            \n",
    "            status_bar.set_postfix({\"loss\": total_loss / (status_bar.n + 1), \"acc\": correct / count})\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / len(val_loader),\n",
    "        \"acc\": correct / count,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "558a176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_model(\n",
    "    model: TweetClassifier,\n",
    "    feature_extractor: FeatureExtractor,\n",
    "    df: pd.DataFrame,\n",
    "    out_file: pathlib.Path,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> pd.Series:\n",
    "    \n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    # 1. Setup Data Loader with Lazy Extracted Features\n",
    "    # The features are extracted/loaded from cache here:\n",
    "    infer_features = feature_extractor.extract(df, 'infer')\n",
    "    infer_ds = TweetDataset(\n",
    "        infer_features, \n",
    "        pd.Series(torch.zeros(len(df), dtype=torch.long)), # Placeholder labels\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        infer_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = torch.zeros(len(df), dtype=torch.long)\n",
    "    offset = 0\n",
    "    \n",
    "    # 2. Run Model Inference\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(data_loader, desc=\"Inferring\"):\n",
    "            \n",
    "            out = model(features)\n",
    "            logits = out[\"logits\"]\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            \n",
    "            predictions[offset: offset + len(preds)] = preds.cpu()\n",
    "            offset += len(preds)\n",
    "            \n",
    "    # --- USER-LEVEL RECONCILIATION ---\n",
    "    \n",
    "    # Copy the input dataframe and attach the single-tweet predictions\n",
    "    df = df.copy()\n",
    "    df[\"pred_label\"] = pd.Series(predictions).astype(int)\n",
    "\n",
    "    # Reconciliation between same users\n",
    "    same_user_key = [\"user_created_at\", \"user_profile_image_url\"]\n",
    "    \n",
    "    # Step A: Count predicted labels (0 or 1) for each unique user key\n",
    "    per_user_stats: dict[tuple[str, str], list[int]] = dict()\n",
    "    for _, row in df.iterrows():\n",
    "        # .setdefault returns [count_label_0, count_label_1]\n",
    "        per_user_stats.setdefault(tuple(row[same_user_key].tolist()), [0, 0])[int(row[\"pred_label\"])] += 1\n",
    "    \n",
    "    # Step B: Determine the reconciled label for users with conflicting predictions\n",
    "    per_user_correct: dict[tuple[str, str], int] = dict()\n",
    "    for key, stats in per_user_stats.items():\n",
    "        # The original code only calculates the majority/tie-breaker if both labels were seen (conflict)\n",
    "        if stats[0] == 0 or stats[1] == 0:\n",
    "            continue # Skip users with unanimous predictions\n",
    "        \n",
    "        # Calculate majority vote (0 or 1), or randomly pick on a tie\n",
    "        per_user_correct[key] = np.select(\n",
    "            [stats[0] > stats[1], stats[1] > stats[0]],\n",
    "            [0, 1],\n",
    "            default=np.random.randint(0, 2),\n",
    "        )\n",
    "    \n",
    "    del per_user_stats\n",
    "    \n",
    "    # Step C: Apply the reconciled prediction back to the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        key = tuple(row[same_user_key].tolist())\n",
    "        if key in per_user_correct:\n",
    "            # Overwrite the prediction with the reconciled label\n",
    "            df.at[idx, \"pred_label\"] = per_user_correct[key]\n",
    "    \n",
    "    # 3. Save to Output File\n",
    "    if out_file is not None:\n",
    "        output = df[[\"challenge_id\", \"pred_label\"]]\n",
    "        output.columns = [\"ID\", \"Prediction\"]\n",
    "        output.to_csv(out_file, index=False)\n",
    "\n",
    "    return df[\"pred_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957dd8c",
   "metadata": {},
   "source": [
    "# Test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9ff6068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [ almanach/camembertav2-base ] =====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting metadata:  10%|█         | 5/48 [00:00<00:01, 42.64it/s]/tmp/ipykernel_325423/1021053081.py:109: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return col.map(len, na_action=\"ignore\").fillna(0)\n",
      "Extracting metadata: 100%|██████████| 48/48 [00:05<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2215 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x2048 and 512x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m val_ds   \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mSubset(full_train_ds, val_idx)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Uncomment the following lines to run training/inference\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoints_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_best\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), model_folder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_ds, val_ds, epochs, lr, weight_decay, max_grad_norm, device, batch_size, optimizer, checkpoints_path, return_best)\u001b[0m\n\u001b[1;32m     42\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 46\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m logits \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     49\u001b[0m loss: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n",
      "File \u001b[0;32m~/.conda/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 99\u001b[0m, in \u001b[0;36mTweetClassifier.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     96\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(x_processed, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# 3. Final layers\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    100\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m: logits}\n",
      "File \u001b[0;32m~/.conda/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x2048 and 512x512)"
     ]
    }
   ],
   "source": [
    "text_encoder_name = \"almanach/camembertav2-base\"\n",
    "print(f\"\\n===== [ {text_encoder_name} ] =====\\n\")\n",
    "\n",
    "model_folder = pathlib.Path(f\"./models/{VERSION}/\") / text_encoder_name.split(\"/\")[-1]\n",
    "model_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Initialize FeatureExtractor\n",
    "feature_extractor = FeatureExtractor(\n",
    "    text_encoder_name=\"camembert/camembert-base\", # This name is now only used for lookup\n",
    "    text_enc_cache_path=pathlib.Path(f\"./text_enc_cache\"),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load metadata normalization stats (means/stds) if available\n",
    "f_ext_ckpt = model_folder / \"feature_extractor.ckpt\"\n",
    "if f_ext_ckpt.exists():\n",
    "    # Load state_dict, which includes means, stds, and afm_cache\n",
    "    feature_extractor.load_state_dict(torch.load(f_ext_ckpt, weights_only=False))\n",
    "\n",
    "feature_extractor.train()\n",
    "\n",
    "# Extract/Load the full training set features (text and metadata)\n",
    "# This call will now return the metadata tensor and the raw text pandas Series\n",
    "full_train_features = feature_extractor.extract(X_train, 'train')\n",
    "\n",
    "# Save metadata normalization stats (means/stds) after extraction/computation\n",
    "torch.save(feature_extractor.state_dict(), f_ext_ckpt)\n",
    "\n",
    "text_encoder_name = \"camembert/camembert-base\"\n",
    "text_fields = feature_extractor.TEXT_FIELDS \n",
    "metadata_size = full_train_features[\"md\"].shape[1]\n",
    "\n",
    "# Pass only the metadata size to the base feature_sizes dict\n",
    "feature_sizes = {\"md\": metadata_size} \n",
    "\n",
    "model = TweetClassifier(\n",
    "    text_encoder_name=text_encoder_name, # Pass the encoder name\n",
    "    feature_sizes=feature_sizes,\n",
    "    text_fields=text_fields, # Pass the text fields list\n",
    "    hidden_dim=512,\n",
    ")\n",
    "\n",
    "# Initialize tokenizer and create collate_fn\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_encoder_name)\n",
    "max_length = model.text_config.max_position_embeddings # Get the correct max length\n",
    "\n",
    "# The collate_fn now handles tokenization on the CPU/GPU border\n",
    "collate_fn = make_collate_fn(tokenizer, device, text_fields, max_length)\n",
    "\n",
    "# Create full dataset and split\n",
    "full_train_ds = TweetDataset(full_train_features, y_train, device=device)\n",
    "\n",
    "\n",
    "# Split into train and validation sets\n",
    "user_descs = pd.Series(X_train['user_description']).fillna('__MISSING__').factorize()[0]\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "train_idx, val_idx = next(splitter.split(X_train, y_train, groups=user_descs))\n",
    "\n",
    "train_ds = torch.utils.data.Subset(full_train_ds, train_idx)\n",
    "val_ds   = torch.utils.data.Subset(full_train_ds, val_idx)\n",
    "\n",
    "# Uncomment the following lines to run training/inference\n",
    "model = train_model(model, train_ds, val_ds, lr=2e-4, epochs=10, batch_size=64, device=device, checkpoints_path=model_folder, return_best=True)\n",
    "torch.save(model.state_dict(), model_folder / \"best_model.ckpt\")\n",
    "torch.cuda.empty_cache()\n",
    "# Inference requires extracting features for X_kaggle with split_name='infer'\n",
    "feature_extractor.eval()\n",
    "infer_features = feature_extractor.extract(X_kaggle, 'infer')\n",
    "infer_ds = TweetDataset(infer_features, pd.Series(torch.zeros(len(X_kaggle), dtype=torch.long)), device=device)\n",
    "infer_with_model(model, feature_extractor, X_kaggle, batch_size=64, device=device, out_file=model_folder / f\"predictions-{VERSION}.csv\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad414f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_folder = pathlib.Path(f\"./models/{VERSION}/camembertav2-base/\")\n",
    "# feature_extractor = FeatureExtractor(text_encoder_name=\"almanach/camembertav2-base\", text_enc_cache_path=model_folder / \"text_enc_cache\", device=device)\n",
    "# feature_extractor.load_state_dict(torch.load(model_folder / \"feature_extractor.ckpt\", weights_only=False))\n",
    "# model = TweetClassifier(\n",
    "#     feature_sizes=feature_extractor.dims(),\n",
    "#     hidden_dim=512,\n",
    "# ).to(device)\n",
    "# model.load_state_dict(torch.load(model_folder / \"epoch_05.pt\"))\n",
    "# good_predictions = infer_with_model(model, feature_extractor, X_kaggle, batch_size=64, device=device, out_file=model_folder / \"predictions-v10-e09.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e31a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train           # None user IDs :    28273\n",
      "                  # valid user IDs:   126641\n",
      "                  # unique IDs    :    25461\n",
      "                  % with banner :   81.75%\n",
      "\n",
      "Kaggle_test       # None user IDs :    18986\n",
      "                  # valid user IDs:    84394\n",
      "                  # unique IDs    :    16954\n",
      "                  % with banner :   81.63%\n",
      "\n",
      "Overlap           user IDs        :        0\n",
      "\n",
      "X_train           # None descs :    24430\n",
      "                  # valid descs:   130484\n",
      "                  # unique descs    :    41234\n",
      "                  % with desc :   84.23%\n",
      "\n",
      "Kaggle_test       # None descs :    18986\n",
      "                  # valid descs:    84394\n",
      "                  # unique descs    :    16954\n",
      "                  % with desc :   81.63%\n",
      "\n",
      "X_train           # None user_created_at :        0\n",
      "                  # valid user_created_at:   154914\n",
      "                  % with user_created_at :  100.00%\n",
      "\n",
      "                  # unique user_created_at:    30696\n",
      "\n",
      "                  # duplicate user_created_at:   124218\n",
      "\n",
      "X_train           # no desc & no banner:    11955\n",
      "                  % no desc & no banner:    7.72%\n",
      "\n",
      "                  # unique user_created_at (no desc & no banner):     2685\n",
      "\n",
      "Label 1           # source contains 'TweetDeck':     2231\n",
      "                  % source contains 'TweetDeck':    3.09%\n",
      "\n",
      "Label 0           # source contains 'TweetDeck':      327\n",
      "                  % source contains 'TweetDeck':    0.40%\n",
      "\n",
      "TweetDeck Users   % with label 1:   87.22%\n",
      "\n",
      "TweetDeck Users   % of all users:    1.65%\n",
      "\n",
      "Source Name       # unique values:      407\n",
      "\n",
      "Source Name Values Unique Values and Counts:\n",
      "Source Name          | Total Count  | % with Label = 1\n",
      "-------------------------------------------------------\n",
      "'Twitter for iPhone' | 49536        |      53.7     %\n",
      "'Twitter Web App   ' | 45072        |      45.8     %\n",
      "'Twitter for Android' | 44903        |      32.2     %\n",
      "'Twitter for iPad  ' | 3520         |      29.1     %\n",
      "'TweetDeck         ' | 2558         |      87.2     %\n",
      "'Hootsuite Inc.    ' | 1352         |      91.0     %\n",
      "'dlvr.it           ' | 1011         |      90.1     %\n",
      "'IFTTT             ' | 748          |      69.8     %\n",
      "'WordPress.com     ' | 717          |      76.7     %\n",
      "'Buffer            ' | 451          |      87.6     %\n",
      "'Echobox           ' | 385          |     100.0     %\n",
      "'AgoraPulse Manager' | 243          |      92.2     %\n",
      "'Twitter Media Studio' | 222          |     100.0     %\n",
      "'Paper.li          ' | 212          |      36.8     %\n",
      "'Twitter for Mac   ' | 147          |      41.5     %\n",
      "'Tweetbot for iΟS  ' | 142          |      66.2     %\n",
      "'Sprout Social     ' | 107          |     100.0     %\n",
      "'Zapier.com        ' | 102          |      78.4     %\n",
      "'Nonli             ' | 90           |     100.0     %\n",
      "'Sociallymap       ' | 84           |      77.4     %\n",
      "'Swello            ' | 77           |     100.0     %\n",
      "'Blog2Social APP   ' | 74           |      74.3     %\n",
      "'The Tweeted Times ' | 73           |      58.9     %\n",
      "'Scoop.it          ' | 70           |      80.0     %\n",
      "'Wildmoka          ' | 67           |     100.0     %\n",
      "'Mashup Web        ' | 64           |      92.2     %\n",
      "'LinkedIn          ' | 62           |      32.3     %\n",
      "'OverBlog Kiwi     ' | 62           |      66.1     %\n",
      "'Twitter for Advertisers (legacy)' | 56           |      98.2     %\n",
      "'PimpMySocial Workshop' | 48           |     100.0     %\n",
      "'FS Poster         ' | 47           |      68.1     %\n",
      "'Revive Social App ' | 42           |      38.1     %\n",
      "'Limber.app        ' | 38           |      63.2     %\n",
      "'HubSpot           ' | 37           |      78.4     %\n",
      "'Tweetbot for Mac  ' | 35           |      80.0     %\n",
      "'La Manche Libre New App' | 34           |     100.0     %\n",
      "'Link account with KUKU.io' | 31           |     100.0     %\n",
      "'Instagram         ' | 29           |      44.8     %\n",
      "'SociabbleApp      ' | 28           |      0.0      %\n",
      "'Twitter for Advertisers' | 27           |     100.0     %\n",
      "'Twibble.io        ' | 26           |      76.9     %\n",
      "'Twidere for Android' | 25           |      16.0     %\n",
      "'Twitter pour iPhone ' | 23           |      0.0      %\n",
      "'TweetCaster for Android' | 21           |      47.6     %\n",
      "'Tumblr            ' | 21           |      52.4     %\n",
      "'Facelift-Cloud    ' | 21           |     100.0     %\n",
      "'Echofon           ' | 20           |      70.0     %\n",
      "'TW Blue           ' | 20           |      50.0     %\n",
      "'Radio King LiveTweet' | 19           |      52.6     %\n",
      "'Plume for Android ' | 18           |      77.8     %\n",
      "'Twitterrific for iOS' | 17           |      29.4     %\n",
      "'FeedPress Publisher' | 17           |     100.0     %\n",
      "'Falcon Social Media Management ' | 16           |     100.0     %\n",
      "'Fenix 2           ' | 16           |      31.2     %\n",
      "'Twitter for ZTE   ' | 15           |      0.0      %\n",
      "'Twitter Media Studio - LiveCut' | 15           |     100.0     %\n",
      "'SocialFlow        ' | 15           |     100.0     %\n",
      "'Mobile Web (M2)   ' | 15           |      0.0      %\n",
      "'Wordtwitt for ROI ' | 15           |     100.0     %\n",
      "'Mastodon-Twitter Crossposter' | 15           |      6.7      %\n",
      "'Salesforce - Social Studio' | 15           |     100.0     %\n",
      "'SEMrush Social Media Tool' | 14           |      64.3     %\n",
      "'Zoho Social       ' | 14           |     100.0     %\n",
      "'Sendible          ' | 13           |      76.9     %\n",
      "'Periscope         ' | 13           |      53.8     %\n",
      "'SocialPilot.co    ' | 13           |     100.0     %\n",
      "'Socialbakers      ' | 12           |     100.0     %\n",
      "'ContentStudio.io  ' | 12           |     100.0     %\n",
      "'Publer            ' | 11           |      18.2     %\n",
      "'Loomly            ' | 11           |     100.0     %\n",
      "'Fastwittr         ' | 10           |      50.0     %\n",
      "'Mailchimp         ' | 10           |      40.0     %\n",
      "'Twitter for Weenect' | 10           |      0.0      %\n",
      "'Linky for iOS     ' | 10           |      50.0     %\n",
      "'Oktopost          ' | 10           |      50.0     %\n",
      "'Radio.co now playing' | 10           |      0.0      %\n",
      "'EveryoneSocial    ' | 10           |      0.0      %\n",
      "'Hearsay Social    ' | 10           |      0.0      %\n",
      "'True Anthem       ' | 10           |     100.0     %\n",
      "'ContentCal Studio ' | 10           |     100.0     %\n",
      "'Crowdfire App     ' | 9            |      33.3     %\n",
      "'Mac4Ever CMS      ' | 9            |     100.0     %\n",
      "'Yuzz.it Pro       ' | 9            |     100.0     %\n",
      "'RC Engage Digital EU' | 9            |     100.0     %\n",
      "'CoSchedule        ' | 9            |     100.0     %\n",
      "'Sprinklr          ' | 8            |     100.0     %\n",
      "'Sud Info          ' | 8            |     100.0     %\n",
      "'Freshdesk         ' | 8            |     100.0     %\n",
      "'Twitterrific for Mac' | 8            |     100.0     %\n",
      "'CentralCharts     ' | 8            |      62.5     %\n",
      "'Microsoft Power Platform' | 8            |     100.0     %\n",
      "'MacroDroid App    ' | 7            |     100.0     %\n",
      "'Talon Android     ' | 7            |      28.6     %\n",
      "'Meltwater Social  ' | 7            |     100.0     %\n",
      "'Twitter Ads       ' | 7            |     100.0     %\n",
      "'Alcmeon Account Management' | 6            |     100.0     %\n",
      "'OBI4wan           ' | 6            |     100.0     %\n",
      "'Podcast - Radio Addict' | 6            |      0.0      %\n",
      "'Yuzzit Pro        ' | 6            |     100.0     %\n",
      "'Luffy Twitter     ' | 5            |      0.0      %\n",
      "'orthodoxie_com_app_auto_publish' | 5            |     100.0     %\n",
      "'BotNagew          ' | 5            |      0.0      %\n",
      "'Sorel-Tracy Express' | 5            |     100.0     %\n",
      "'Nouveau DiploMix  ' | 5            |     100.0     %\n",
      "'Auto Publish WP Twitter 2' | 5            |     100.0     %\n",
      "'publication bons plans' | 5            |     100.0     %\n",
      "'LaterMedia        ' | 5            |      80.0     %\n",
      "'Le Nouveau Monde  ' | 5            |     100.0     %\n",
      "'Miguel_le_slip    ' | 5            |      0.0      %\n",
      "'Dici - Radio      ' | 5            |     100.0     %\n",
      "'ActuSuisse        ' | 5            |     100.0     %\n",
      "'BuzzInsolite      ' | 5            |     100.0     %\n",
      "'Freie-Welt        ' | 5            |      0.0      %\n",
      "'ecnouvs           ' | 5            |      0.0      %\n",
      "'ChtiSebCC         ' | 5            |      0.0      %\n",
      "'WP to Twitter Pro ' | 5            |     100.0     %\n",
      "'TubeBuddy         ' | 5            |      0.0      %\n",
      "'20sEmy            ' | 5            |      0.0      %\n",
      "'Point du Lac Saint-Jean' | 5            |     100.0     %\n",
      "'Picta Presse      ' | 5            |     100.0     %\n",
      "'coronavirused     ' | 5            |     100.0     %\n",
      "'WeLoveTennis      ' | 5            |     100.0     %\n",
      "'Dynamic Signal    ' | 5            |      0.0      %\n",
      "'PulpNews          ' | 5            |     100.0     %\n",
      "'FxBookLTTG        ' | 5            |     100.0     %\n",
      "'connectionivoirienne' | 5            |     100.0     %\n",
      "'https://media27.info' | 5            |     100.0     %\n",
      "'AutoTweetBAQ      ' | 5            |     100.0     %\n",
      "'Alert Infos       ' | 5            |      0.0      %\n",
      "'le1               ' | 5            |     100.0     %\n",
      "'actualitesdudroit ' | 5            |     100.0     %\n",
      "'RTL info pour Tweeter' | 5            |     100.0     %\n",
      "'Marsad Tunisia    ' | 5            |     100.0     %\n",
      "'Article19ma       ' | 5            |     100.0     %\n",
      "'SocialAutoPosterMagCentre' | 5            |     100.0     %\n",
      "'MyBotMyzen2       ' | 5            |      0.0      %\n",
      "'COVID19-Updates   ' | 5            |     100.0     %\n",
      "'StockTwits Web    ' | 5            |     100.0     %\n",
      "'App pour FS Poster' | 5            |     100.0     %\n",
      "'Flamingo for Android' | 5            |      60.0     %\n",
      "'La Chronique auto tweet' | 5            |     100.0     %\n",
      "'Canva             ' | 5            |      60.0     %\n",
      "'ISTFecamp         ' | 5            |     100.0     %\n",
      "'France Covid      ' | 5            |     100.0     %\n",
      "'TanjiroTwitter    ' | 5            |      0.0      %\n",
      "'L'Echo de la Rive-Nord' | 5            |     100.0     %\n",
      "'Puremédias (publi articles)' | 5            |     100.0     %\n",
      "'Moutons Enragés   ' | 5            |     100.0     %\n",
      "'PJL_PPL_app       ' | 5            |     100.0     %\n",
      "'Africa News Hub   ' | 5            |     100.0     %\n",
      "'Ostadi            ' | 5            |      0.0      %\n",
      "'BlogLesCrises     ' | 5            |     100.0     %\n",
      "'avmtest           ' | 5            |     100.0     %\n",
      "'innovscovid19     ' | 5            |      0.0      %\n",
      "'SocialRabbit Plugin' | 5            |      0.0      %\n",
      "'SNK367            ' | 5            |      0.0      %\n",
      "'AmazingContent    ' | 5            |      0.0      %\n",
      "'Hellsing Twitter  ' | 5            |      0.0      %\n",
      "'Le Doutotron      ' | 5            |     100.0     %\n",
      "'Cheap Bots, Done Quick!' | 5            |     100.0     %\n",
      "'divergence_app    ' | 5            |     100.0     %\n",
      "'Figaro International Méthode App' | 5            |     100.0     %\n",
      "'Desinfos          ' | 5            |     100.0     %\n",
      "'covid19_bot_tx    ' | 5            |     100.0     %\n",
      "'Story Chief       ' | 5            |     100.0     %\n",
      "'Mdq Canada        ' | 5            |     100.0     %\n",
      "'feed-bountou      ' | 5            |     100.0     %\n",
      "'Raprnbmag         ' | 5            |     100.0     %\n",
      "'Nouvelles sur RIMQ' | 5            |     100.0     %\n",
      "'Planate School    ' | 5            |      0.0      %\n",
      "'Netvibes Widget   ' | 5            |     100.0     %\n",
      "'Interkinois       ' | 5            |      0.0      %\n",
      "'FavOLT-Tweets     ' | 5            |     100.0     %\n",
      "'FashionMag        ' | 5            |     100.0     %\n",
      "'Houssen Mohsinaly ' | 5            |     100.0     %\n",
      "'TopHashtags       ' | 5            |     100.0     %\n",
      "'Gamepush2         ' | 5            |     100.0     %\n",
      "'happyhomeinc      ' | 5            |     100.0     %\n",
      "'Le Journal de Joliette' | 5            |     100.0     %\n",
      "'Journal de l'agence' | 5            |     100.0     %\n",
      "'LesObservateurs   ' | 5            |     100.0     %\n",
      "'OxfordBlue-Twitter' | 5            |     100.0     %\n",
      "'soccerGames       ' | 5            |      0.0      %\n",
      "'fdesouche.com     ' | 5            |     100.0     %\n",
      "'Senejournal Tweets' | 5            |     100.0     %\n",
      "'covid_coulsim     ' | 5            |     100.0     %\n",
      "'Sprinklr Publishing' | 5            |     100.0     %\n",
      "'OLPlusApp         ' | 5            |     100.0     %\n",
      "'ZinfosTwitter     ' | 5            |     100.0     %\n",
      "'ThreadReaderApp   ' | 5            |     100.0     %\n",
      "'AutoTweetFCV      ' | 5            |     100.0     %\n",
      "'Real Madrid Hoy   ' | 5            |     100.0     %\n",
      "'Chambly Express   ' | 5            |     100.0     %\n",
      "'Neomedia-VS       ' | 5            |     100.0     %\n",
      "'SartheInfosBot 1.0' | 5            |     100.0     %\n",
      "'MoneyVox          ' | 5            |     100.0     %\n",
      "'NewsBKFrance      ' | 5            |     100.0     %\n",
      "'Devdiscourse News Desk' | 5            |     100.0     %\n",
      "'HealthBuzz Tweets ' | 5            |      0.0      %\n",
      "'RTS Multimédia MédiaBus (Sports)' | 5            |     100.0     %\n",
      "'Alertes SNCF Transilien' | 5            |     100.0     %\n",
      "'Tunis Tribune status updater' | 5            |     100.0     %\n",
      "'Bitly             ' | 5            |      0.0      %\n",
      "'Pepsnews          ' | 5            |     100.0     %\n",
      "'InfoDimanche.com  ' | 5            |     100.0     %\n",
      "'TVMag Méthode App ' | 5            |     100.0     %\n",
      "'TAG.FR PRODUCTION ' | 5            |     100.0     %\n",
      "'tryrequest        ' | 5            |      0.0      %\n",
      "'autotweet scheduler' | 5            |     100.0     %\n",
      "'Wordpress TW Macon News FR' | 5            |     100.0     %\n",
      "'potins.net        ' | 5            |     100.0     %\n",
      "'R analyses for twitter' | 5            |     100.0     %\n",
      "'Marseille Football' | 5            |     100.0     %\n",
      "'Next_Alert        ' | 5            |     100.0     %\n",
      "'game2_lazbt       ' | 5            |      0.0      %\n",
      "'BasketEurope      ' | 5            |     100.0     %\n",
      "'FranceNetInfos    ' | 5            |     100.0     %\n",
      "'L'important       ' | 5            |     100.0     %\n",
      "'orsonebooks       ' | 5            |     100.0     %\n",
      "'dzonline application' | 5            |     100.0     %\n",
      "'http://www.mysweetimmo.com' | 5            |     100.0     %\n",
      "'RTL sport         ' | 5            |     100.0     %\n",
      "'UN Talent         ' | 5            |     100.0     %\n",
      "'Afropages         ' | 5            |     100.0     %\n",
      "'MyHeadlinez       ' | 5            |     100.0     %\n",
      "'autourdebordeaux  ' | 5            |     100.0     %\n",
      "'AutoPublishPNCContact' | 5            |     100.0     %\n",
      "'Sunubuzz          ' | 5            |     100.0     %\n",
      "'MichMich75000     ' | 5            |      0.0      %\n",
      "'Purepeople (publi articles)' | 5            |     100.0     %\n",
      "'twittbot.net      ' | 5            |     100.0     %\n",
      "'IIIPRS            ' | 5            |     100.0     %\n",
      "'theinfomaker.com  ' | 5            |     100.0     %\n",
      "'Postify1          ' | 5            |     100.0     %\n",
      "'News Releases Auto Tweeter' | 5            |     100.0     %\n",
      "'Javascript Newss  ' | 5            |     100.0     %\n",
      "'Lelision.com      ' | 5            |      0.0      %\n",
      "'Pause Fun         ' | 5            |     100.0     %\n",
      "'emploisjob        ' | 5            |      0.0      %\n",
      "'Nessma TV         ' | 5            |     100.0     %\n",
      "'Seinemaritime76   ' | 5            |     100.0     %\n",
      "'Montceau News     ' | 5            |     100.0     %\n",
      "'Buzzagain         ' | 5            |     100.0     %\n",
      "'Info-Flash-Write-Local-Pages' | 5            |     100.0     %\n",
      "'FlashNewsBot      ' | 5            |     100.0     %\n",
      "'Basketusa WP      ' | 5            |     100.0     %\n",
      "'Covid 19 France   ' | 5            |     100.0     %\n",
      "'Agenparl          ' | 5            |     100.0     %\n",
      "'quarantineOpportunity' | 5            |     100.0     %\n",
      "'Teledakar-officiel' | 5            |     100.0     %\n",
      "'KultureGeek.fr    ' | 5            |     100.0     %\n",
      "'Bot Vichy         ' | 5            |     100.0     %\n",
      "'KoolSaina.com     ' | 5            |     100.0     %\n",
      "'Telebruxelles_web ' | 5            |     100.0     %\n",
      "'Boursier.com      ' | 5            |     100.0     %\n",
      "'app-twitter-novelle' | 5            |     100.0     %\n",
      "'AkroRadio         ' | 5            |      0.0      %\n",
      "'Neutron Jimm      ' | 5            |      0.0      %\n",
      "'CFL_RssToTwitter  ' | 5            |     100.0     %\n",
      "'Dici TV           ' | 5            |     100.0     %\n",
      "'Accelerate Twitter Demo1' | 5            |      0.0      %\n",
      "'FFr Tweet this    ' | 5            |      0.0      %\n",
      "'Malivox.net       ' | 5            |     100.0     %\n",
      "'WhiteBeard CMS    ' | 5            |     100.0     %\n",
      "'The New Cameroon  ' | 5            |      0.0      %\n",
      "'newsnet-app       ' | 5            |     100.0     %\n",
      "'COVID Articles on SP Journals' | 5            |     100.0     %\n",
      "'Application @morandiniblog' | 5            |     100.0     %\n",
      "'Alertes RERB      ' | 5            |     100.0     %\n",
      "'T7.com            ' | 5            |     100.0     %\n",
      "'L'Observateur V5  ' | 5            |     100.0     %\n",
      "'La Commère 43     ' | 5            |     100.0     %\n",
      "'Walfoot           ' | 5            |     100.0     %\n",
      "'Cawbird           ' | 5            |      0.0      %\n",
      "'Titi Tweet        ' | 5            |      0.0      %\n",
      "'Poteaux carrés    ' | 5            |     100.0     %\n",
      "'LOTTA INTERNATIONALE' | 5            |     100.0     %\n",
      "' Accelerate Twitter Demo 4' | 5            |      0.0      %\n",
      "'Schtroumpsifier   ' | 5            |     100.0     %\n",
      "'URLs_fr           ' | 5            |     100.0     %\n",
      "'twitterpsfr       ' | 5            |     100.0     %\n",
      "'overall-twitter   ' | 5            |     100.0     %\n",
      "'TOPGYN CAPA       ' | 5            |      0.0      %\n",
      "'EnBeauce.com REST API v 1.1' | 5            |     100.0     %\n",
      "'Figaro Economie Méthode App' | 5            |     100.0     %\n",
      "'TwinyBots         ' | 5            |      0.0      %\n",
      "'Eauto Check       ' | 5            |     100.0     %\n",
      "'journal tunisie   ' | 5            |     100.0     %\n",
      "'Geek-tech         ' | 5            |     100.0     %\n",
      "'TweetPony         ' | 5            |      0.0      %\n",
      "'Atlas Magazine    ' | 5            |     100.0     %\n",
      "'Hospimedia        ' | 5            |     100.0     %\n",
      "'site reve86       ' | 5            |     100.0     %\n",
      "'LuckyTheo64       ' | 5            |      0.0      %\n",
      "'Qui a TT?         ' | 5            |     100.0     %\n",
      "'ahlam ahlam       ' | 5            |      0.0      %\n",
      "'yoyoyo_v3         ' | 5            |      0.0      %\n",
      "'Fenix for Android ' | 4            |     100.0     %\n",
      "'Stats Covid France' | 4            |     100.0     %\n",
      "'Basket Infos Twitter' | 4            |     100.0     %\n",
      "'Docteur imago     ' | 4            |     100.0     %\n",
      "'TweetGram.me      ' | 4            |     100.0     %\n",
      "'OLPlusFem         ' | 4            |     100.0     %\n",
      "'Scopalto          ' | 4            |     100.0     %\n",
      "'fr.allAfrica.com  ' | 4            |     100.0     %\n",
      "'AujourduiPress    ' | 4            |      0.0      %\n",
      "'jimwebfeed        ' | 4            |     100.0     %\n",
      "'MI Connect        ' | 4            |     100.0     %\n",
      "'94 Citoyens       ' | 4            |     100.0     %\n",
      "'Integromat        ' | 4            |     100.0     %\n",
      "'wiw Social Streams' | 4            |     100.0     %\n",
      "'TwidereX-Android  ' | 4            |      0.0      %\n",
      "'OS X              ' | 4            |      0.0      %\n",
      "'Figaro Vox Media App' | 4            |     100.0     %\n",
      "'recurpost.com     ' | 4            |     100.0     %\n",
      "'Melody HCR        ' | 4            |     100.0     %\n",
      "'Melody_CorseMatin ' | 4            |     100.0     %\n",
      "'twitter lba       ' | 4            |     100.0     %\n",
      "'BourseTrading     ' | 4            |     100.0     %\n",
      "'lacryptomonnaie   ' | 4            |     100.0     %\n",
      "'Revive pour duperrin.com' | 4            |     100.0     %\n",
      "'Neatly For BlackBerry 10' | 4            |     100.0     %\n",
      "'vivafrik          ' | 4            |     100.0     %\n",
      "'AlwihdApp         ' | 3            |     100.0     %\n",
      "'TF Tweets Auto    ' | 3            |     100.0     %\n",
      "'Instapaper        ' | 3            |      0.0      %\n",
      "'Gazette du Palais ' | 3            |     100.0     %\n",
      "'wtbass            ' | 3            |     100.0     %\n",
      "'Djib_s            ' | 3            |     100.0     %\n",
      "'Sharee Advocacy   ' | 3            |     100.0     %\n",
      "'thread.center     ' | 3            |      0.0      %\n",
      "'Carte de circulation interactive' | 3            |     100.0     %\n",
      "'Clarabridge Engage' | 3            |     100.0     %\n",
      "'eClincher         ' | 3            |     100.0     %\n",
      "'iOS               ' | 3            |     100.0     %\n",
      "'Iconosquare.com   ' | 3            |     100.0     %\n",
      "'tweetinfluence123 ' | 3            |      0.0      %\n",
      "'senedirect app    ' | 3            |     100.0     %\n",
      "'Blogs Ouest France' | 3            |     100.0     %\n",
      "'Odoo Social       ' | 3            |      0.0      %\n",
      "'Mafrique          ' | 3            |     100.0     %\n",
      "'Post Planner Inc. ' | 3            |     100.0     %\n",
      "'Figaro Etudiant Media App' | 3            |     100.0     %\n",
      "'TwitPane for Android' | 3            |      0.0      %\n",
      "'PreProd app senti ' | 2            |      0.0      %\n",
      "'Planable          ' | 2            |     100.0     %\n",
      "'Amelya Bot        ' | 2            |     100.0     %\n",
      "'MTV English News  ' | 2            |     100.0     %\n",
      "'Zlappo.com        ' | 2            |     100.0     %\n",
      "'Atlantico         ' | 2            |     100.0     %\n",
      "'Twitter Web Client' | 2            |      0.0      %\n",
      "'Khoros            ' | 2            |     100.0     %\n",
      "'MaLigue2 Auto publication' | 2            |     100.0     %\n",
      "'RT News Sharing   ' | 2            |      0.0      %\n",
      "'LCI Twitter Kit   ' | 2            |     100.0     %\n",
      "'TwitPanePlus      ' | 2            |      0.0      %\n",
      "'blogSpirit        ' | 2            |     100.0     %\n",
      "'SocialHub by maloon' | 2            |     100.0     %\n",
      "'http://www.gerontonews.com' | 2            |     100.0     %\n",
      "'Twitter pour le Web ' | 2            |      0.0      %\n",
      "'Zonebourse App    ' | 2            |      0.0      %\n",
      "'TestLimitDM       ' | 2            |     100.0     %\n",
      "'theglobe          ' | 2            |     100.0     %\n",
      "'Nintendo Switch Share' | 2            |     100.0     %\n",
      "'Schweizer Parlament ParlCH' | 2            |     100.0     %\n",
      "'UberSocial© PRO   ' | 2            |     100.0     %\n",
      "'ReviveOldPost2    ' | 2            |     100.0     %\n",
      "'Diaspora* at Framasphere' | 2            |     100.0     %\n",
      "'rtweet_token_sc   ' | 2            |     100.0     %\n",
      "'KultureGeek | Djibs' | 2            |     100.0     %\n",
      "'Nuzzel            ' | 2            |     100.0     %\n",
      "'Sud éducation 75  ' | 1            |     100.0     %\n",
      "'Publer.io         ' | 1            |      0.0      %\n",
      "'Twidere X Android ' | 1            |      0.0      %\n",
      "'TIC Mag           ' | 1            |     100.0     %\n",
      "'MyTwt_mycron      ' | 1            |     100.0     %\n",
      "'Les-Crises.fr0    ' | 1            |     100.0     %\n",
      "'Polyalert-Live    ' | 1            |     100.0     %\n",
      "'Twitlonger        ' | 1            |     100.0     %\n",
      "'Salon Beige       ' | 1            |     100.0     %\n",
      "'The Tweeted Times Mobile' | 1            |     100.0     %\n",
      "'Aviary (for iOS and macOS)' | 1            |     100.0     %\n",
      "'CanadaLIve        ' | 1            |      0.0      %\n",
      "'Alerte_Huit_lemonde' | 1            |     100.0     %\n",
      "'BirdSite          ' | 1            |     100.0     %\n",
      "'Vimeo             ' | 1            |     100.0     %\n",
      "'Reddit Official   ' | 1            |      0.0      %\n",
      "'Khoros Publishing ' | 1            |     100.0     %\n",
      "'Postoplan         ' | 1            |      0.0      %\n",
      "'PlayStation®Network' | 1            |     100.0     %\n",
      "'dirico            ' | 1            |     100.0     %\n",
      "'Future CAC40      ' | 1            |     100.0     %\n",
      "'WP Auto T         ' | 1            |     100.0     %\n",
      "'Skyrock           ' | 1            |      0.0      %\n",
      "'Tweepsmap         ' | 1            |     100.0     %\n",
      "'post auto heidi news' | 1            |     100.0     %\n",
      "'TelphoneFR        ' | 1            |      0.0      %\n",
      "'HashCut           ' | 1            |     100.0     %\n",
      "'Marocafrik.com    ' | 1            |     100.0     %\n",
      "'Auto publication quotidienne ' | 1            |     100.0     %\n",
      "'Streamlabs Twitter' | 1            |     100.0     %\n",
      "'Hypefury          ' | 1            |     100.0     %\n",
      "'chirr.app         ' | 1            |      0.0      %\n",
      "'redouad_twitter   ' | 1            |     100.0     %\n",
      "'Info241_tweet     ' | 1            |     100.0     %\n",
      "'Alerte Nouvelles  ' | 1            |     100.0     %\n",
      "'FCM Live tweet    ' | 1            |     100.0     %\n"
     ]
    }
   ],
   "source": [
    "# Extract user_ids\n",
    "def get_user_id(url):\n",
    "    return url.split(\"/\")[4] if pd.notna(url) else None\n",
    "\n",
    "# Training set\n",
    "train_ids = X_train[\"user_profile_banner_url\"].map(get_user_id)\n",
    "print(f\"{'X_train':<17} # None user IDs : {train_ids.isna().sum():>8}\")\n",
    "print(f\"{'':<17} # valid user IDs: {train_ids.notna().sum():>8}\")\n",
    "print(f\"{'':<17} # unique IDs    : {train_ids.nunique():>8}\")\n",
    "# Print percent of users that have a banner\n",
    "percent_with_banner = (train_ids.notna().sum() / len(train_ids)) * 100\n",
    "print(f\"{'':<17} % with banner : {percent_with_banner:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# Kaggle test set\n",
    "kaggle_ids = kaggle_data[\"user.profile_banner_url\"].map(get_user_id)\n",
    "print(f\"{'Kaggle_test':<17} # None user IDs : {kaggle_ids.isna().sum():>8}\")\n",
    "print(f\"{'':<17} # valid user IDs: {kaggle_ids.notna().sum():>8}\")\n",
    "print(f\"{'':<17} # unique IDs    : {kaggle_ids.nunique():>8}\")\n",
    "# Print percent of users that have a banner\n",
    "percent_with_banner = (kaggle_ids.notna().sum() / len(kaggle_ids)) * 100\n",
    "print(f\"{'':<17} % with banner : {percent_with_banner:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# Overlap\n",
    "train_set = set(train_ids.dropna())\n",
    "kaggle_set = set(kaggle_ids.dropna())\n",
    "overlap = train_set.intersection(kaggle_set)\n",
    "print(f\"{'Overlap':<17} user IDs        : {len(overlap):>8}\")\n",
    "print()\n",
    "\n",
    "# Training set\n",
    "train_ids = X_train[\"user_description\"]\n",
    "print(f\"{'X_train':<17} # None descs : {train_ids.isna().sum():>8}\")\n",
    "print(f\"{'':<17} # valid descs: {train_ids.notna().sum():>8}\")\n",
    "print(f\"{'':<17} # unique descs    : {train_ids.nunique():>8}\")\n",
    "# Print percent of users that have a description\n",
    "percent_with_desc = (train_ids.notna().sum() / len(train_ids)) * 100\n",
    "print(f\"{'':<17} % with desc : {percent_with_desc:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# Kaggle test set\n",
    "kaggle_ids = kaggle_data[\"user.profile_banner_url\"].map(get_user_id)\n",
    "print(f\"{'Kaggle_test':<17} # None descs : {kaggle_ids.isna().sum():>8}\")\n",
    "print(f\"{'':<17} # valid descs: {kaggle_ids.notna().sum():>8}\")\n",
    "print(f\"{'':<17} # unique descs    : {kaggle_ids.nunique():>8}\")\n",
    "# Print percent of users that have a description\n",
    "percent_with_desc = (kaggle_ids.notna().sum() / len(kaggle_ids)) * 100\n",
    "print(f\"{'':<17} % with desc : {percent_with_desc:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# What percent of users have a user_created_at value?\n",
    "\n",
    "train_user_created_at = X_train[\"user_created_at\"]\n",
    "print(f\"{'X_train':<17} # None user_created_at : {train_user_created_at.isna().sum():>8}\")\n",
    "print(f\"{'':<17} # valid user_created_at: {train_user_created_at.notna().sum():>8}\")\n",
    "percent_with_user_created_at = (train_user_created_at.notna().sum() / len(train_user_created_at)) * 100\n",
    "print(f\"{'':<17} % with user_created_at : {percent_with_user_created_at:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# How many user_created_at are unique values?\n",
    "print(f\"{'':<17} # unique user_created_at: {train_user_created_at.nunique():>8}\")\n",
    "print()\n",
    "\n",
    "# How many user_created_at are not unique values?\n",
    "duplicate_user_created_at = len(train_user_created_at) - train_user_created_at.nunique()\n",
    "print(f\"{'':<17} # duplicate user_created_at: {duplicate_user_created_at:>8}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# What percent of tweets have no description and no banner?\n",
    "no_desc_and_no_banner = X_train[\"user_description\"].isna() & X_train[\"user_profile_banner_url\"].isna()\n",
    "count_no_desc_and_no_banner = no_desc_and_no_banner.sum()\n",
    "percent_no_desc_and_no_banner = (count_no_desc_and_no_banner / len(X_train)) * 100\n",
    "print(f\"{'X_train':<17} # no desc & no banner: {count_no_desc_and_no_banner:>8}\")\n",
    "print(f\"{'':<17} % no desc & no banner: {percent_no_desc_and_no_banner:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# Among users that have no description and no banner, how many unique user_created_at values are there?\n",
    "unique_user_created_at_no_desc_and_no_banner = X_train.loc[no_desc_and_no_banner, \"user_created_at\"].nunique()\n",
    "print(f\"{'':<17} # unique user_created_at (no desc & no banner): {unique_user_created_at_no_desc_and_no_banner:>8}\")\n",
    "print()\n",
    "\n",
    "# For users whose label is 1, what percent have a \"source\" field containing \"TweetDeck\"?\n",
    "label_1_source_tweetdeck = X_train.loc[y_train == 1, \"source\"].str.contains(\"TweetDeck\", na=False)\n",
    "count_label_1_source_tweetdeck = label_1_source_tweetdeck.sum()\n",
    "total_label_1 = (y_train == 1).sum()\n",
    "percent_label_1_source_tweetdeck = (count_label_1_source_tweetdeck / total_label_1) * 100\n",
    "print(f\"{'Label 1':<17} # source contains 'TweetDeck': {count_label_1_source_tweetdeck:>8}\")\n",
    "print(f\"{'':<17} % source contains 'TweetDeck': {percent_label_1_source_tweetdeck:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# For users whose label is 0, what percent have a \"source\" field containing \"TweetDeck\"?\n",
    "label_0_source_tweetdeck = X_train.loc[y_train == 0, \"source\"].str.contains(\"TweetDeck\", na=False)\n",
    "count_label_0_source_tweetdeck = label_0_source_tweetdeck.sum()\n",
    "total_label_0 = (y_train == 0).sum()\n",
    "percent_label_0_source_tweetdeck = (count_label_0_source_tweetdeck / total_label_0) * 100\n",
    "print(f\"{'Label 0':<17} # source contains 'TweetDeck': {count_label_0_source_tweetdeck:>8}\")\n",
    "print(f\"{'':<17} % source contains 'TweetDeck': {percent_label_0_source_tweetdeck:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# For uses who have \"TweetDeck\" in their source, what percent have label 1?\n",
    "total_tweetdeck = label_0_source_tweetdeck.sum() + label_1_source_tweetdeck.sum()\n",
    "percent_tweetdeck_label_1 = (count_label_1_source_tweetdeck / total_tweetdeck) * 100\n",
    "print(f\"{'TweetDeck Users':<17} % with label 1: {percent_tweetdeck_label_1:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# What percent of users use TweetDeck as their source?\n",
    "total_tweetdeck = label_0_source_tweetdeck.sum() + label_1_source_tweetdeck.sum()\n",
    "percent_tweetdeck_users = (total_tweetdeck / len(X_train)) * 100\n",
    "print(f\"{'TweetDeck Users':<17} % of all users: {percent_tweetdeck_users:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# How many unique values are there in \"source_name\" field?\n",
    "source_name_unique = X_train[\"source_name\"].nunique()\n",
    "print(f\"{'Source Name':<17} # unique values: {source_name_unique:>8}\")\n",
    "print()\n",
    "\n",
    "# List all unique values of \"source_name\" field along with their counts, sorted by count descending\n",
    "source_name_counts = X_train[\"source_name\"].value_counts()\n",
    "print(f\"{'Source Name Values':<17} Unique Values and Counts:\")\n",
    "\n",
    "# 1. Combine the source names and labels into a single DataFrame\n",
    "# Ensure X_train and y_train have the same index for correct alignment\n",
    "df_combined = pd.DataFrame({\n",
    "    \"source_name\": X_train[\"source_name\"],\n",
    "    \"label\": y_train\n",
    "})\n",
    "\n",
    "# 2. Calculate the total count for each source_name\n",
    "source_name_counts = df_combined[\"source_name\"].value_counts()\n",
    "\n",
    "# 3. Calculate the count where 'label' == 1 for each source_name\n",
    "# Group by \"source_name\" and sum the \"label\" (summing 1s gives the count of 1s)\n",
    "label_1_counts = df_combined.groupby(\"source_name\")[\"label\"].sum()\n",
    "\n",
    "# 4. Calculate the percentage of label == 1 for each source_name\n",
    "label_1_percentages = ((label_1_counts / source_name_counts) * 100).round(1)\n",
    "\n",
    "# 5. Print the results, sorted by total count descending\n",
    "print(f\"{'Source Name':<20} | {'Total Count':<12} | % with Label = 1\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for source_name, total_count in source_name_counts.items():\n",
    "    # Retrieve the calculated percentage for the current source_name\n",
    "    percentage = label_1_percentages.get(source_name, 0.0)\n",
    "    print(f\"'{source_name:<18}' | {total_count:<12} | {percentage:^14.1f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e500dab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

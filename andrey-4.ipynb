{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb900036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import typing\n",
    "import json\n",
    "import pathlib\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import transformers\n",
    "import transformers.modeling_outputs\n",
    "import transformers.configuration_utils\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f873bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_KAGGLE = \"KAGGLE_DOCKER_IMAGE\" in os.environ\n",
    "\n",
    "DATASETS = pathlib.Path(\n",
    "    \".\"\n",
    "    if not IS_KAGGLE\n",
    "    else \"/kaggle/input/influencers-or-observers-predicting-social-roles/Kaggle2025\"\n",
    ")\n",
    "\n",
    "DATASET_TRAIN = DATASETS / \"train.jsonl\"\n",
    "DATASET_KAGGLE = DATASETS / \"kaggle_test.jsonl\"\n",
    "\n",
    "CACHE_DIR = pathlib.Path(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7fc921d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e20b6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f28f61",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e16f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: pathlib.Path, cache: bool = False) -> pd.DataFrame:\n",
    "    path_pq = (CACHE_DIR / path.name).with_stem(f\"{path.stem}_raw\").with_suffix(\".parquet\")\n",
    "    \n",
    "    if cache and path_pq.exists():\n",
    "        return pd.read_parquet(path_pq)\n",
    "    \n",
    "    # This leaves things to be desired, since there's no way to specify dtypes\n",
    "    # and it assumes float instead of int, causing a loss in precision...\n",
    "    # But I guess it only matters for ids, which we'll probably discard in preprocessing anyway\n",
    "    result = pd.json_normalize(list(map(json.loads, path.read_bytes().splitlines())))\n",
    "    \n",
    "    if cache:\n",
    "        result.to_parquet(path_pq)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "80d1df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_json(DATASET_TRAIN, cache=True)\n",
    "kaggle_data = load_json(DATASET_KAGGLE, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bcf662",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4ea7df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # For technical reasons, any text columns we want to use should have no dots in their names.\n",
    "    # The simplest way to achieve this is to replace all dots indiscriminately.\n",
    "    \n",
    "    df = df.rename(columns=lambda x: x.replace(\".\", \"_\"))\n",
    "    \n",
    "    df[\"is_reply\"] = df[\"in_reply_to_status_id\"].notna()\n",
    "    \n",
    "    df = df.drop(columns=[\n",
    "        \"in_reply_to_status_id_str\",\n",
    "        # \"in_reply_to_status_id\",\n",
    "        \"in_reply_to_user_id_str\",\n",
    "        \"in_reply_to_user_id\",\n",
    "        \"quoted_status_id_str\",\n",
    "        \"quoted_status_id\",\n",
    "        \"id_str\",\n",
    "        \"quoted_status_in_reply_to_status_id_str\",\n",
    "        \"quoted_status_in_reply_to_status_id\",\n",
    "        \"quoted_status_in_reply_to_user_id_str\",\n",
    "        \"quoted_status_in_reply_to_user_id\",\n",
    "        \"quoted_status_id_str\",\n",
    "        \"quoted_status_id\",\n",
    "        \"quoted_status_user_id_str\",\n",
    "        \"quoted_status_user_id\",\n",
    "        # \"quoted_status_permalink_expanded\",\n",
    "        \"quoted_status_permalink_display\",\n",
    "        \"quoted_status_permalink_url\",\n",
    "        \"quoted_status_quoted_status_id\",\n",
    "        \"quoted_status_quoted_status_id_str\",\n",
    "        # \"quoted_status_place_id\",\n",
    "        # \"place_id\",\n",
    "        \"lang\",  # Always \"fr\"\n",
    "        \"retweeted\",  # Always False\n",
    "        \"filter_level\",  # Always \"low\"\n",
    "        \"geo\",  # Always None\n",
    "        \"place\",  # Always None\n",
    "        \"coordinates\",  # Always None\n",
    "        \"contributors\",  # Always None\n",
    "        \"quote_count\",  # Always 0\n",
    "        \"reply_count\",  # Always 0\n",
    "        \"retweet_count\",  # Always 0\n",
    "        \"favorite_count\",  # Always 0\n",
    "        \"favorited\",  # Always False\n",
    "        \"quoted_status_geo\",  # Always None\n",
    "        \"quoted_status_place\",  # Always None\n",
    "        \"quoted_status_coordinates\",  # Always None\n",
    "        \"quoted_status_retweeted\",  # Always False\n",
    "        \"quoted_status_filter_level\",  # Always \"low\"\n",
    "        \"quoted_status_contributors\",  # Always None\n",
    "        \"quoted_status_user_utc_offset\",  # Always None\n",
    "        \"quoted_status_user_lang\",  # Always None\n",
    "        \"quoted_status_user_time_zone\",  # Always None\n",
    "        \"quoted_status_user_follow_request_sent\",  # Always None\n",
    "        \"quoted_status_user_following\",  # Always None\n",
    "        \"quoted_status_user_notifications\",  # Always None\n",
    "        \"user_default_profile_image\",  # Always False\n",
    "        \"user_protected\",  # Always False\n",
    "        \"user_contributors_enabled\",  # Always False\n",
    "        \"user_lang\",  # Always None\n",
    "        \"user_notifications\",  # Always None\n",
    "        \"user_following\",  # Always None\n",
    "        \"user_utc_offset\",  # Always None\n",
    "        \"user_time_zone\",  # Always None\n",
    "        \"user_follow_request_sent\",  # Always None\n",
    "    ])\n",
    "    \n",
    "    df[\"full_text\"] = df.apply(lambda tweet: extract_full_text(tweet), axis=1)\n",
    "    \n",
    "    source_split = df[\"source\"].str.removeprefix(\"<a href=\\\"\").str.removesuffix(\"</a>\").str.split(\"\\\" rel=\\\"nofollow\\\">\").map(lambda x: x if len(x) == 2 else pd.NA)\n",
    "    df[\"source_url\"] = source_split.map(lambda x: x[0], na_action=\"ignore\")\n",
    "    df[\"source_name\"] = source_split.map(lambda x: x[1], na_action=\"ignore\")\n",
    "    \n",
    "    df[\"misc_text\"] = df.apply(\n",
    "        lambda x: \"via: {0}; reply: @{1}; quote: @{2} {3}\".format(x[\"source_name\"], x[\"in_reply_to_screen_name\"], x[\"quoted_status_user_screen_name\"], x[\"quoted_status_user_name\"]), axis=1,\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_full_text(tweet: pd.Series) -> str:\n",
    "    text: str = tweet[\"text\"]\n",
    "    \n",
    "    if not pd.isna(tweet[\"extended_tweet_full_text\"]):\n",
    "        text = tweet[\"extended_tweet_full_text\"]\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab1ee966",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(\"label\", axis=1)\n",
    "y_train = train_data[\"label\"]\n",
    "\n",
    "X_kaggle = kaggle_data\n",
    "\n",
    "X_train = preprocess(X_train)\n",
    "X_kaggle = preprocess(X_kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda51842",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5be1585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Made this a class to hold all the caches. It may resemble an nn.Module, but isn't one!\n",
    "class FeatureExtractor:\n",
    "    training: bool\n",
    "    device: torch.device\n",
    "    means: pd.Series | None\n",
    "    stds: pd.Series | None\n",
    "    afm_cache: dict[tuple[str, str], float]\n",
    "    text_encoder_name: str | None\n",
    "    text_tokenizer: nn.Module | None\n",
    "    text_encoder: nn.Module | None\n",
    "    text_config: transformers.configuration_utils.PretrainedConfig | None\n",
    "    text_enc_cache_path: pathlib.Path | None\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        text_encoder_name: str | None = None,\n",
    "        text_enc_cache_path: pathlib.Path | None = None,\n",
    "        # A shortcut to avoid downloading and loading into memory the heavy models\n",
    "        # if the encodings are already precomputed\n",
    "        allow_no_text_encoder: bool = True,\n",
    "        device: torch.device = device,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.means = None\n",
    "        self.stds = None\n",
    "        self.afm_cache = {}\n",
    "        self.text_enc_cache_path = text_enc_cache_path\n",
    "        \n",
    "        self.text_encoder_name = text_encoder_name\n",
    "        self.text_tokenizer = None\n",
    "        self.text_encoder = None\n",
    "        self.text_config = None\n",
    "        \n",
    "        if text_encoder_name is not None:\n",
    "            if (\n",
    "                allow_no_text_encoder and\n",
    "                text_enc_cache_path is not None and\n",
    "                (text_enc_cache_path / \"train.ckpt\").exists() and\n",
    "                (text_enc_cache_path / \"infer.ckpt\").exists()\n",
    "            ):\n",
    "                self.text_config = AutoConfig.from_pretrained(text_encoder_name)\n",
    "            else:\n",
    "                self.text_tokenizer = AutoTokenizer.from_pretrained(text_encoder_name)\n",
    "                if hasattr(self.text_tokenizer, \"to\"):  # Distilbert doesn't, apprently\n",
    "                    self.text_tokenizer = self.text_tokenizer.to(self.device)\n",
    "                self.text_encoder = AutoModel.from_pretrained(text_encoder_name).to(self.device)\n",
    "                self.text_config = self.text_encoder.config\n",
    "        \n",
    "        self.train()\n",
    "    \n",
    "    def train(self):\n",
    "        self.training = True\n",
    "    \n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            \"means\": self.means,\n",
    "            \"stds\": self.stds,\n",
    "            \"afm_cache\": self.afm_cache,\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.means = state_dict[\"means\"]\n",
    "        self.stds = state_dict[\"stds\"]\n",
    "        self.afm_cache = state_dict[\"afm_cache\"]\n",
    "    \n",
    "    def dims(self) -> dict[str, int]:\n",
    "        return {\n",
    "            \"md\": len(self.METADATA_FIELDS),\n",
    "        } | {\n",
    "            field: compress or self.embed_size\n",
    "            for field, compress in self.TEXT_FIELDS\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def embed_size(self) -> int:\n",
    "        return self.text_config.hidden_size\n",
    "    \n",
    "    def extract(self, df: pd.DataFrame, override_cache: bool = False) -> dict[str, torch.Tensor]:\n",
    "        result: dict[str, torch.Tensor] = {}\n",
    "        \n",
    "        if self.text_enc_cache_path is None:\n",
    "            self._extract(df, result)\n",
    "        else:\n",
    "            cf = self.text_enc_cache_path / (\"train.ckpt\" if self.training else \"infer.ckpt\")\n",
    "            cf.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if cf.exists() and not override_cache:\n",
    "                encodings: dict[str, torch.Tensor] = torch.load(cf)\n",
    "                for col_name, value in encodings.items():\n",
    "                    result[col_name] = value.to(self.device)\n",
    "            else:\n",
    "                self._extract(df, result)\n",
    "                \n",
    "                torch.save({\n",
    "                    field: embedding.cpu().detach()\n",
    "                    for field, embedding in result.items()\n",
    "                }, cf)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _extract(self, df: pd.DataFrame, result: dict[str, torch.Tensor]):\n",
    "        result[\"md\"] = self.extract_raw_metadata(df)\n",
    "        \n",
    "        for col_name, compress in self.TEXT_FIELDS:\n",
    "            emb = self.embed_texts(df[col_name])\n",
    "            \n",
    "            if compress is not None and compress < emb.shape[1]:\n",
    "                pca = PCA(n_components=compress)\n",
    "                emb = pca.fit_transform(emb.cpu().detach().numpy())\n",
    "                emb = torch.tensor(emb, dtype=torch.float32, device=self.device)\n",
    "            elif compress is not None:\n",
    "                print(f\"Warning: embedding for {col_name} zero-padded from {emb.shape[1]} to {compress}, consider reducing requested size\")\n",
    "                emb = torch.nn.functional.pad(emb, (0, compress - emb.shape[1]))\n",
    "            \n",
    "            result[col_name] = emb\n",
    "    \n",
    "    def extract_raw_metadata(self, df: pd.DataFrame) -> torch.Tensor:\n",
    "        md_cols: list[pd.Series] = []\n",
    "\n",
    "        for fn, col_name in tqdm(self.METADATA_FIELDS, desc=\"Extracting metadata\"):\n",
    "            md_cols.append(fn(self, df[col_name]))\n",
    "        \n",
    "        md: pd.DataFrame = pd.concat(md_cols, axis=1)\n",
    "        \n",
    "        # The second case shouldn't be triggered, but sometimes the preprocessor used during training is lost\n",
    "        if self.training:\n",
    "            self.means = md.mean().fillna(0)\n",
    "            self.stds = md.std().fillna(1)\n",
    "        \n",
    "        assert self.means is not None and self.stds is not None, \"You forgot to train/load the feature extractor\"\n",
    "\n",
    "        md = (md - self.means) / self.stds\n",
    "\n",
    "        return torch.from_numpy(md.to_numpy()).float().to(self.device)\n",
    "\n",
    "    def embed_texts(\n",
    "        self,\n",
    "        texts: pd.Series,\n",
    "        batch_size: int = 64,\n",
    "        progress: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        tokenizer = self.text_tokenizer\n",
    "        encoder = self.text_encoder\n",
    "        encoder.eval()\n",
    "\n",
    "        all_embeddings = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_offsets = range(0, len(texts), batch_size)\n",
    "            if progress:\n",
    "                batch_offsets = tqdm(batch_offsets, desc=f\"Embedding {texts.name or '<unnamed>'}\")\n",
    "            for i in batch_offsets:\n",
    "                batch_texts = texts.iloc[i:i + batch_size]\n",
    "                nonna = batch_texts.notna() & batch_texts.str.len().gt(0)\n",
    "\n",
    "                tokenized = tokenizer(\n",
    "                    batch_texts[nonna].tolist(),\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=self.text_config.max_position_embeddings\n",
    "                ).to(self.device)\n",
    "\n",
    "                outputs: transformers.modeling_outputs.BaseModelOutput = encoder(**tokenized)\n",
    "                last_hidden: torch.Tensor = outputs.last_hidden_state\n",
    "                mask: torch.Tensor = tokenized[\"attention_mask\"].unsqueeze(-1)\n",
    "                \n",
    "                masked_hidden = last_hidden * mask\n",
    "                summed = masked_hidden.sum(dim=1)\n",
    "                counts = mask.sum(dim=1)\n",
    "                embeddings = torch.zeros(len(batch_texts), last_hidden.shape[2], device=self.device)\n",
    "                nonna = nonna.reset_index(drop=True)\n",
    "                embeddings[nonna[nonna].index] = (summed / counts)\n",
    "\n",
    "                all_embeddings.append(embeddings)\n",
    "\n",
    "        return torch.cat(all_embeddings, dim=0)\n",
    "    \n",
    "    def apply_fill_mean(\n",
    "        self,\n",
    "        col: pd.Series,\n",
    "        func: typing.Callable[[typing.Any], typing.Any],\n",
    "    ) -> pd.Series:\n",
    "        col = col.map(func, na_action=\"ignore\")\n",
    "        \n",
    "        key = (col.name, func.__name__)\n",
    "        if self.training:\n",
    "            self.afm_cache[key] = col.mean()\n",
    "        assert key in self.afm_cache, \"You forgot to train/load the feature extractor\"\n",
    "        \n",
    "        return col.fillna(self.afm_cache[key])\n",
    "    \n",
    "    def md_bool(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(lambda x: (1 if x else -1), na_action=\"ignore\").fillna(0)\n",
    "\n",
    "    def md_len(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(len, na_action=\"ignore\").fillna(0)\n",
    "\n",
    "    def md_time(self, col: pd.Series) -> pd.Series:\n",
    "        return self.apply_fill_mean(col, lambda x: time.mktime(time.strptime(x, \"%a %b %d %H:%M:%S %z %Y\")))\n",
    "\n",
    "    def md_num(self, col: pd.Series) -> pd.Series:\n",
    "        return self.apply_fill_mean(col, pd.to_numeric)\n",
    "\n",
    "    def md_place(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(lambda x: int(x, 16), na_action=\"ignore\").fillna(0)\n",
    "    \n",
    "    METADATA_FIELDS: list[tuple[typing.Callable[[FeatureExtractor, pd.Series], pd.Series], str]] = [\n",
    "        (md_bool, \"is_quote_status\"),\n",
    "        (md_bool, \"is_reply\"),\n",
    "        (md_bool, \"possibly_sensitive\"),\n",
    "        (md_bool, \"quoted_status_user_verified\"),\n",
    "        (md_bool, \"user_is_translator\"),\n",
    "        (md_bool, \"user_geo_enabled\"),\n",
    "        (md_bool, \"user_profile_use_background_image\"),\n",
    "        (md_bool, \"user_default_profile\"),\n",
    "        \n",
    "        (md_len, \"full_text\"),\n",
    "        (md_len, \"source_name\"),\n",
    "        (md_len, \"in_reply_to_screen_name\"),\n",
    "        (md_len, \"quoted_status_extended_tweet_entities_urls\"),\n",
    "        (md_len, \"quoted_status_extended_tweet_entities_user_mentions\"),\n",
    "        (md_len, \"quoted_status_extended_tweet_full_text\"),\n",
    "        (md_len, \"quoted_status_entities_urls\"),\n",
    "        (md_len, \"quoted_status_user_profile_image_url_https\"),\n",
    "        (md_len, \"quoted_status_user_profile_background_image_url\"),\n",
    "        (md_len, \"quoted_status_user_profile_background_image_url_https\"),\n",
    "        (md_len, \"quoted_status_user_screen_name\"),\n",
    "        (md_len, \"quoted_status_user_name\"),\n",
    "        (md_len, \"entities_hashtags\"),\n",
    "        (md_len, \"entities_user_mentions\"),\n",
    "        (md_len, \"user_profile_image_url_https\"),\n",
    "        (md_len, \"user_profile_background_image_url\"),\n",
    "        (md_len, \"user_description\"),\n",
    "        (md_len, \"user_translator_type\"),\n",
    "        (md_len, \"user_url\"),\n",
    "        (md_len, \"user_profile_banner_url\"),\n",
    "        (md_len, \"user_location\"),\n",
    "        (md_len, \"display_text_range\"),\n",
    "        (md_len, \"extended_tweet_entities_urls\"),\n",
    "        (md_len, \"extended_tweet_entities_hashtags\"),\n",
    "        (md_len, \"extended_tweet_entities_user_mentions\"),\n",
    "        (md_len, \"quoted_status_permalink_expanded\"),\n",
    "        \n",
    "        (md_time, \"created_at\"),\n",
    "        (md_time, \"user_created_at\"),\n",
    "        (md_time, \"quoted_status_created_at\"),\n",
    "        (md_time, \"quoted_status_user_created_at\"),\n",
    "        \n",
    "        (md_num, \"user_statuses_count\"),\n",
    "        (md_num, \"user_listed_count\"),\n",
    "        (md_num, \"user_favourites_count\"),\n",
    "        (md_num, \"user_profile_background_tile\"),\n",
    "        (md_num, \"quoted_status_quote_count\"),\n",
    "        (md_num, \"quoted_status_user_followers_count\"),\n",
    "        (md_num, \"quoted_status_user_favourites_count\"),\n",
    "        (md_num, \"in_reply_to_status_id\"),\n",
    "        \n",
    "        (md_place, \"quoted_status_place_id\"),\n",
    "        (md_place, \"place_id\"),\n",
    "    ]\n",
    "\n",
    "    TEXT_FIELDS: list[tuple[str, int | None]] = [\n",
    "        (\"full_text\", None),\n",
    "        (\"user_description\", 64),\n",
    "        (\"misc_text\", None),\n",
    "        # (\"source_name\", None),\n",
    "        # (\"in_reply_to_screen_name\", None),\n",
    "        # (\"quoted_status_user_screen_name\", None),\n",
    "        # (\"quoted_status_user_name\", None),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1bc90d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    features: dict[str, torch.Tensor]\n",
    "    labels: torch.Tensor\n",
    "    device: torch.device\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_extractor: FeatureExtractor,\n",
    "        df: pd.DataFrame,\n",
    "        labels: pd.Series,\n",
    "        device: torch.device = device,\n",
    "    ):\n",
    "        self.features = feature_extractor.extract(df)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features[\"md\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"features\": {key: val[idx] for key, val in self.features.items()},\n",
    "            \"label\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    features = {\n",
    "        key: torch.stack([x[\"features\"][key] for x in batch])\n",
    "        for key in batch[0][\"features\"].keys()\n",
    "    }\n",
    "    labels = torch.stack([x[\"label\"] for x in batch])\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af4a722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "\n",
    "class TweetClassifier(nn.Module):\n",
    "    feature_sizes: dict[str, int]\n",
    "    \n",
    "    layer1: nn.ModuleDict\n",
    "    fc2: nn.Linear\n",
    "    fc3: nn.Linear\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_sizes: dict[str, int],\n",
    "        hidden_dim: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_sizes = feature_sizes\n",
    "        \n",
    "        self.layer1 = nn.ModuleDict()\n",
    "        \n",
    "        def _add(name, dropout: float):\n",
    "            self.layer1[name] = nn.Sequential(\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(feature_sizes[name], hidden_dim),\n",
    "            )\n",
    "        \n",
    "        _add(\"md\", 0.1)\n",
    "        _add(\"full_text\", 0.1)\n",
    "        _add(\"user_description\", 0.75)\n",
    "        _add(\"misc_text\", 0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, NUM_CLASSES)\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def forward(self, features: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n",
    "        batch_size = len(features[\"md\"])\n",
    "        \n",
    "        x = torch.zeros(batch_size, self.fc2.in_features, device=self.device)\n",
    "        \n",
    "        for name, module in self.layer1.items():\n",
    "            x += module(features[name])\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"probs\": probs,\n",
    "            \"log_probs\": log_probs,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c7a47ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: TweetClassifier,\n",
    "    train_ds: Dataset,\n",
    "    val_ds: Dataset,\n",
    "    epochs: int = 3,\n",
    "    lr: float = 2e-4,\n",
    "    weight_decay: float = 0.01,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    "    optimizer: torch.optim.Optimizer | None = None,\n",
    "    checkpoints_path: pathlib.Path | str | None = \".\",\n",
    "    return_best: bool = False,\n",
    ") -> TweetClassifier:\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    if optimizer is None:\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_file: pathlib.Path | None = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        status_bar = tqdm(train_loader, desc=\"Training\")\n",
    "\n",
    "        for features, labels in status_bar:\n",
    "            features: dict[str, torch.Tensor]\n",
    "            labels: torch.Tensor\n",
    "            features = {k: v.to(device) for k, v in features.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            out = model(features)\n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            status_bar.set_postfix({\"loss\": total_loss / (status_bar.n + 1)})\n",
    "        \n",
    "        print(f\"Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        val_metrics = evaluate_model(\n",
    "            model=model,\n",
    "            val_ds=val_ds,\n",
    "            device=device,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['acc']:.4f}\")\n",
    "\n",
    "        if checkpoints_path is not None:\n",
    "            ckpt = pathlib.Path(checkpoints_path) / f\"epoch_{epoch:02}.pt\"\n",
    "            torch.save(model.state_dict(), ckpt)\n",
    "            print(f\"Checkpoint saved to {ckpt}\")\n",
    "            \n",
    "            if val_metrics[\"loss\"] < best_val_loss:\n",
    "                best_val_loss = val_metrics[\"loss\"]\n",
    "                best_model_file = ckpt\n",
    "\n",
    "    if return_best and best_model_file is not None:\n",
    "        print(f\"Best model: {best_model_file}\")\n",
    "        model.load_state_dict(torch.load(best_model_file))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: TweetClassifier,\n",
    "    val_ds: Dataset,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> tuple[float, float]:\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        status_bar = tqdm(val_loader, desc=\"Evaluating\")\n",
    "        \n",
    "        for features, labels in status_bar:\n",
    "            features: dict[str, torch.Tensor]\n",
    "            labels: torch.Tensor\n",
    "            features = {k: v.to(device) for k, v in features.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = model(features)\n",
    "            logits: torch.Tensor = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            count += labels.size(0)\n",
    "            \n",
    "            status_bar.set_postfix({\"loss\": total_loss / (status_bar.n + 1), \"acc\": correct / count})\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / len(val_loader),\n",
    "        \"acc\": correct / count,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "558a176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_model(\n",
    "    model: TweetClassifier,\n",
    "    feature_extractor: FeatureExtractor,\n",
    "    df: pd.DataFrame,\n",
    "    out_file: pathlib.Path | str | None = None,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> pd.Series:\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        TweetDataset(feature_extractor, df, torch.zeros(len(df), dtype=torch.long, device=device), device=device),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = torch.zeros(len(df), dtype=torch.long)\n",
    "    cur_idx = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(data_loader, desc=\"Inferring\"):\n",
    "            features: dict[str, torch.Tensor]\n",
    "            features = {k: v.to(device) for k, v in features.items()}\n",
    "\n",
    "            out = model(features)\n",
    "            logits: torch.Tensor = out[\"logits\"].cpu()\n",
    "            \n",
    "            predictions[cur_idx:cur_idx+len(features[\"md\"])] = logits.argmax(dim=-1)\n",
    "            cur_idx += len(features[\"md\"])\n",
    "    \n",
    "    df = df.copy()\n",
    "    df[\"pred_label\"] = pd.Series(predictions).astype(int)\n",
    "\n",
    "    # Reconciliation between same users\n",
    "    # Note: the pandas implementation OOM-ed, but the pure python one seems a lot slower\n",
    "    same_user_key = [\"user_created_at\", \"user_profile_image_url\"]\n",
    "    per_user_stats: dict[tuple[str, str], list[int]] = dict()\n",
    "    for _, row in df.iterrows():\n",
    "        per_user_stats.setdefault(tuple(row[same_user_key].tolist()), [0, 0])[int(row[\"pred_label\"])] += 1\n",
    "    \n",
    "    per_user_correct: dict[tuple[str, str], int] = dict()\n",
    "    for key, stats in per_user_stats.items():\n",
    "        if stats[0] == 0 or stats[1] == 0:\n",
    "            continue\n",
    "        \n",
    "        per_user_correct[key] = np.select(\n",
    "            [stats[0] > stats[1], stats[1] > stats[0]],\n",
    "            [0, 1],\n",
    "            default=np.random.randint(0, 2),\n",
    "        )\n",
    "    \n",
    "    del per_user_stats\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        key = tuple(row[same_user_key].tolist())\n",
    "        if key in per_user_correct:\n",
    "            per_user_correct[key]\n",
    "            df.at[idx, \"pred_label\"] = per_user_correct[key]\n",
    "    \n",
    "    if out_file is not None:\n",
    "        output = df[[\"challenge_id\", \"pred_label\"]]\n",
    "        output.columns = [\"ID\", \"Prediction\"]\n",
    "        output.to_csv(out_file, index=False)\n",
    "    \n",
    "    return df[\"pred_label\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957dd8c",
   "metadata": {},
   "source": [
    "# Test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff6068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [ almanach/camembertav2-base ] =====\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [00:17<00:00, 125.40it/s, loss=0.365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 243/243 [00:01<00:00, 198.41it/s, loss=0.347, acc=0.857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3269, Acc: 0.8569\n",
      "Checkpoint saved to models\\v10\\camembertav2-base\\epoch_01.pt\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [00:17<00:00, 123.14it/s, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 243/243 [00:01<00:00, 189.82it/s, loss=0.35, acc=0.86]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3241, Acc: 0.8597\n",
      "Checkpoint saved to models\\v10\\camembertav2-base\\epoch_02.pt\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [00:18<00:00, 119.62it/s, loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 243/243 [00:01<00:00, 202.37it/s, loss=0.332, acc=0.859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3256, Acc: 0.8593\n",
      "Checkpoint saved to models\\v10\\camembertav2-base\\epoch_03.pt\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [00:18<00:00, 119.45it/s, loss=0.353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 243/243 [00:01<00:00, 189.65it/s, loss=0.354, acc=0.856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3303, Acc: 0.8560\n",
      "Checkpoint saved to models\\v10\\camembertav2-base\\epoch_04.pt\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [00:19<00:00, 109.08it/s, loss=0.352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 243/243 [00:01<00:00, 176.78it/s, loss=0.339, acc=0.861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3235, Acc: 0.8611\n",
      "Checkpoint saved to models\\v10\\camembertav2-base\\epoch_05.pt\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [00:19<00:00, 114.34it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 243/243 [00:01<00:00, 192.94it/s, loss=0.352, acc=0.859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3246, Acc: 0.8587\n",
      "Checkpoint saved to models\\v10\\camembertav2-base\\epoch_06.pt\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [00:18<00:00, 115.38it/s, loss=0.341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 243/243 [00:01<00:00, 186.23it/s, loss=0.346, acc=0.857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3263, Acc: 0.8572\n",
      "Checkpoint saved to models\\v10\\camembertav2-base\\epoch_07.pt\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [00:19<00:00, 114.50it/s, loss=0.337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 243/243 [00:01<00:00, 189.75it/s, loss=0.35, acc=0.857] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3245, Acc: 0.8569\n",
      "Checkpoint saved to models\\v10\\camembertav2-base\\epoch_08.pt\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [00:18<00:00, 114.76it/s, loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 243/243 [00:01<00:00, 184.83it/s, loss=0.34, acc=0.853] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3363, Acc: 0.8533\n",
      "Checkpoint saved to models\\v10\\camembertav2-base\\epoch_09.pt\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [00:19<00:00, 111.75it/s, loss=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 243/243 [00:01<00:00, 172.36it/s, loss=0.343, acc=0.858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3272, Acc: 0.8585\n",
      "Checkpoint saved to models\\v10\\camembertav2-base\\epoch_10.pt\n",
      "Best model: models\\v10\\camembertav2-base\\epoch_05.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abel\\AppData\\Local\\Temp\\ipykernel_8008\\3880069523.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
      "Inferring: 100%|██████████| 1616/1616 [00:05<00:00, 306.66it/s]\n"
     ]
    }
   ],
   "source": [
    "text_encoder_name = \"almanach/camembertav2-base\"\n",
    "print(f\"\\n===== [ {text_encoder_name} ] =====\\n\")\n",
    "\n",
    "model_folder = pathlib.Path(\"./models/v10/\") / text_encoder_name.split(\"/\")[-1]\n",
    "model_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "feature_extractor = FeatureExtractor(text_encoder_name=text_encoder_name, text_enc_cache_path=model_folder / \"text_enc_cache\", device=device)\n",
    "feature_extractor.train()\n",
    "\n",
    "f_ext_ckpt = model_folder / \"feature_extractor.ckpt\"\n",
    "if f_ext_ckpt.exists():\n",
    "    feature_extractor.load_state_dict(torch.load(f_ext_ckpt, weights_only=False))  # This has a pd.Series in it, so otherwise torch 2.6+ complains\n",
    "\n",
    "full_train_ds = TweetDataset(feature_extractor, X_train, y_train, device=device)\n",
    "\n",
    "torch.save(feature_extractor.state_dict(), f_ext_ckpt)\n",
    "\n",
    "train_ds, val_ds = random_split(full_train_ds, [0.9, 0.1])\n",
    "\n",
    "model = TweetClassifier(\n",
    "    feature_sizes=feature_extractor.dims(),\n",
    "    hidden_dim=512,\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(model_folder / \"saved_e20.pt\"))\n",
    "\n",
    "model = train_model(model, train_ds, val_ds, lr=2e-4, epochs=10, batch_size=64, device=device, checkpoints_path=model_folder, return_best=True)\n",
    "torch.save(model.state_dict(), model_folder / \"best_model.ckpt\")\n",
    "torch.cuda.empty_cache()\n",
    "infer_with_model(model, feature_extractor, X_kaggle, batch_size=64, device=device, out_file=model_folder / \"predictions-v10.csv\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad414f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abel\\AppData\\Local\\Temp\\ipykernel_8008\\3880069523.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
      "Inferring: 100%|██████████| 1616/1616 [00:04<00:00, 323.44it/s]\n"
     ]
    }
   ],
   "source": [
    "model_folder = pathlib.Path(\"./models/v10/camembertav2-base/\")\n",
    "feature_extractor = FeatureExtractor(text_encoder_name=\"almanach/camembertav2-base\", text_enc_cache_path=model_folder / \"text_enc_cache\", device=device)\n",
    "feature_extractor.load_state_dict(torch.load(model_folder / \"feature_extractor.ckpt\", weights_only=False))\n",
    "model = TweetClassifier(\n",
    "    feature_sizes=feature_extractor.dims(),\n",
    "    hidden_dim=512,\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(model_folder / \"epoch_05.pt\"))\n",
    "good_predictions = infer_with_model(model, feature_extractor, X_kaggle, batch_size=64, device=device, out_file=model_folder / \"predictions-v10-e09.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

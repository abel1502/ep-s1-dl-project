{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a05761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb900036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import typing\n",
    "import json\n",
    "import pathlib\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import hashlib\n",
    "import urllib.parse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import transformers\n",
    "import transformers.modeling_outputs\n",
    "import transformers.configuration_utils\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f873bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_KAGGLE = \"KAGGLE_DOCKER_IMAGE\" in os.environ\n",
    "\n",
    "DATASETS = pathlib.Path(\n",
    "    \".\"\n",
    "    if not IS_KAGGLE\n",
    "    else \"/kaggle/input/influencers-or-observers-predicting-social-roles/Kaggle2025\"\n",
    ")\n",
    "\n",
    "DATASET_TRAIN = DATASETS / \"train.jsonl\"\n",
    "DATASET_KAGGLE = DATASETS / \"kaggle_test.jsonl\"\n",
    "\n",
    "CACHE_DIR = pathlib.Path(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7fc921d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e20b6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f28f61",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e16f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: pathlib.Path, cache: bool = False) -> pd.DataFrame:\n",
    "    path_pq = (CACHE_DIR / path.name).with_stem(f\"{path.stem}_raw\").with_suffix(\".parquet\")\n",
    "    \n",
    "    if cache and path_pq.exists():\n",
    "        return pd.read_parquet(path_pq)\n",
    "    \n",
    "    # This leaves things to be desired, since there's no way to specify dtypes\n",
    "    # and it assumes float instead of int, causing a loss in precision...\n",
    "    # But I guess it only matters for ids, which we'll probably discard in preprocessing anyway\n",
    "    result = pd.json_normalize(list(map(json.loads, path.read_bytes().splitlines())))\n",
    "    \n",
    "    if cache:\n",
    "        result.to_parquet(path_pq)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80d1df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_json(DATASET_TRAIN, cache=True)\n",
    "kaggle_data = load_json(DATASET_KAGGLE, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bcf662",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ea7df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # For technical reasons, any text columns we want to use should have no dots in their names.\n",
    "    # The simplest way to achieve this is to replace all dots indiscriminately.\n",
    "    \n",
    "    df = df.rename(columns=lambda x: x.replace(\".\", \"_\"))\n",
    "    \n",
    "    df[\"is_reply\"] = df[\"in_reply_to_status_id\"].notna()\n",
    "    \n",
    "    user_id_key = [\"user_description\", \"user_created_at\", \"user_profile_image_url\"]\n",
    "    df[\"user_hash\"] = df[user_id_key].fillna(\"<NA>\").astype(str).agg(''.join, axis=1).where(~df[user_id_key].isna().all(axis=1), df[\"id_str\"]).map(fast_hash)\n",
    "    \n",
    "    df = df.drop(columns=[\n",
    "        \"in_reply_to_status_id_str\",\n",
    "        # \"in_reply_to_status_id\",\n",
    "        \"in_reply_to_user_id_str\",\n",
    "        \"in_reply_to_user_id\",\n",
    "        \"quoted_status_id_str\",\n",
    "        \"quoted_status_id\",\n",
    "        \"id_str\",\n",
    "        \"quoted_status_in_reply_to_status_id_str\",\n",
    "        \"quoted_status_in_reply_to_status_id\",\n",
    "        \"quoted_status_in_reply_to_user_id_str\",\n",
    "        \"quoted_status_in_reply_to_user_id\",\n",
    "        \"quoted_status_id_str\",\n",
    "        \"quoted_status_id\",\n",
    "        \"quoted_status_user_id_str\",\n",
    "        \"quoted_status_user_id\",\n",
    "        # \"quoted_status_permalink_expanded\",\n",
    "        \"quoted_status_permalink_display\",\n",
    "        \"quoted_status_permalink_url\",\n",
    "        \"quoted_status_quoted_status_id\",\n",
    "        \"quoted_status_quoted_status_id_str\",\n",
    "        # \"quoted_status_place_id\",\n",
    "        # \"place_id\",\n",
    "        \"lang\",  # Always \"fr\"\n",
    "        \"retweeted\",  # Always False\n",
    "        \"filter_level\",  # Always \"low\"\n",
    "        \"geo\",  # Always None\n",
    "        \"place\",  # Always None\n",
    "        \"coordinates\",  # Always None\n",
    "        \"contributors\",  # Always None\n",
    "        \"quote_count\",  # Always 0\n",
    "        \"reply_count\",  # Always 0\n",
    "        \"retweet_count\",  # Always 0\n",
    "        \"favorite_count\",  # Always 0\n",
    "        \"favorited\",  # Always False\n",
    "        \"quoted_status_geo\",  # Always None\n",
    "        \"quoted_status_place\",  # Always None\n",
    "        \"quoted_status_coordinates\",  # Always None\n",
    "        \"quoted_status_retweeted\",  # Always False\n",
    "        \"quoted_status_filter_level\",  # Always \"low\"\n",
    "        \"quoted_status_contributors\",  # Always None\n",
    "        \"quoted_status_user_utc_offset\",  # Always None\n",
    "        \"quoted_status_user_lang\",  # Always None\n",
    "        \"quoted_status_user_time_zone\",  # Always None\n",
    "        \"quoted_status_user_follow_request_sent\",  # Always None\n",
    "        \"quoted_status_user_following\",  # Always None\n",
    "        \"quoted_status_user_notifications\",  # Always None\n",
    "        \"user_default_profile_image\",  # Always False\n",
    "        \"user_protected\",  # Always False\n",
    "        \"user_contributors_enabled\",  # Always False\n",
    "        \"user_lang\",  # Always None\n",
    "        \"user_notifications\",  # Always None\n",
    "        \"user_following\",  # Always None\n",
    "        \"user_utc_offset\",  # Always None\n",
    "        \"user_time_zone\",  # Always None\n",
    "        \"user_follow_request_sent\",  # Always None\n",
    "    ])\n",
    "    \n",
    "    df[\"full_text\"] = df.apply(lambda tweet: extract_full_text(tweet), axis=1)\n",
    "    \n",
    "    source_split = df[\"source\"].str.removeprefix(\"<a href=\\\"\").str.removesuffix(\"</a>\").str.split(\"\\\" rel=\\\"nofollow\\\">\").map(lambda x: x if len(x) == 2 else pd.NA)\n",
    "    df[\"source_url\"] = source_split.map(lambda x: x[0], na_action=\"ignore\")\n",
    "    df[\"source_name\"] = source_split.map(lambda x: x[1], na_action=\"ignore\")\n",
    "    \n",
    "    df[\"misc_text\"] = df.apply(\n",
    "        lambda x: \"via: {0}; reply: @{1}; quote: @{2} {3}\".format(x[\"source_name\"], x[\"in_reply_to_screen_name\"], x[\"quoted_status_user_screen_name\"], x[\"quoted_status_user_name\"]), axis=1,\n",
    "    )\n",
    "    \n",
    "    df[\"source_domain\"] = df[\"source_url\"].map(extract_domain, na_action=\"ignore\")\n",
    "    df[\"user_domain\"] = df[\"user_url\"].str.extract(r\"https?://([^/]+)/\")\n",
    "    \n",
    "    for col in [\n",
    "        \"quoted_status_user_profile_link_color\",\n",
    "        \"quoted_status_user_profile_background_color\",\n",
    "        \"quoted_status_user_profile_sidebar_border_color\",\n",
    "        \"quoted_status_user_profile_text_color\",\n",
    "        \"user_profile_link_color\",\n",
    "        \"user_profile_background_color\",\n",
    "        \"user_profile_sidebar_border_color\",\n",
    "        \"user_profile_text_color\",\n",
    "        \"user_profile_sidebar_fill_color\",\n",
    "    ]:\n",
    "        df[f\"{col}_r\"], df[f\"{col}_g\"], df[f\"{col}_b\"] = extract_color(df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_full_text(tweet: pd.Series) -> str:\n",
    "    text: str = tweet[\"text\"]\n",
    "    \n",
    "    if not pd.isna(tweet[\"extended_tweet_full_text\"]):\n",
    "        text = tweet[\"extended_tweet_full_text\"]\n",
    "    \n",
    "    return text\n",
    "\n",
    "def fast_hash(content: str) -> str:\n",
    "    h = hashlib.blake2s(digest_size=16)\n",
    "    h.update(content.encode('utf-8'))\n",
    "    return h.hexdigest()\n",
    "\n",
    "def extract_domain(url: str) -> str:\n",
    "    return urllib.parse.urlparse(url).netloc\n",
    "\n",
    "def extract_color(color: pd.Series) -> tuple[pd.Series, pd.Series, pd.Series]:\n",
    "    return tuple(\n",
    "        color.str.slice(i, i + 2).map(lambda x: int(x, 16), na_action=\"ignore\")\n",
    "        for i in (0, 2, 4)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab1ee966",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(\"label\", axis=1)\n",
    "y_train = train_data[\"label\"]\n",
    "\n",
    "X_kaggle = kaggle_data\n",
    "\n",
    "X_train = preprocess(X_train)\n",
    "X_kaggle = preprocess(X_kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c8d7e",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "829f7ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03703379602187509"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.full_text.map(emoji.emoji_count, na_action=\"ignore\").corr(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e6d12ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1569"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_freq = X_train.full_text.map(lambda x: [y[\"emoji\"] for y in emoji.emoji_list(x)], na_action=\"ignore\").explode().value_counts()\n",
    "len(emoji_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ae2c0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":backhand_index_pointing_right: in full_text    0.072842\n",
       ":right_arrow: in full_text                      0.055572\n",
       ":right_arrow_curving_down: in full_text         0.033804\n",
       ":play_button: in full_text                      0.032676\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_emoji = list(emoji_freq[:512].index)\n",
    "most_relevant_emoji = pd.Series({\n",
    "    f\"{emoji.demojize(em)} in full_text\": X_train.full_text.map(lambda x: x.count(em), na_action=\"ignore\").corr(y_train)\n",
    "    for em in top_emoji\n",
    "}).abs().sort_values(ascending=False).pipe(lambda x: x[x >= 0.03])\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(most_relevant_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "898d8a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_profile_link_color_b              0.245343\n",
       "user_profile_background_color_r        0.196165\n",
       "user_profile_background_color_g        0.192955\n",
       "user_profile_link_color_g              0.189547\n",
       "user_profile_background_color_b        0.184779\n",
       "user_profile_link_color_r              0.164342\n",
       "user_profile_sidebar_border_color_b    0.136743\n",
       "user_profile_sidebar_fill_color_b      0.124482\n",
       "user_profile_sidebar_border_color_g    0.119252\n",
       "user_profile_sidebar_fill_color_g      0.115195\n",
       "user_profile_sidebar_fill_color_r      0.107217\n",
       "user_profile_sidebar_border_color_r    0.077069\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[[col for col in X_train.columns if col.endswith(\"_r\") or col.endswith(\"_g\") or col.endswith(\"_b\")]].corrwith(y_train).abs().sort_values(ascending=False).pipe(lambda x: x[x >= 0.05])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda51842",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5be1585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Made this a class to hold all the caches. It may resemble an nn.Module, but isn't one!\n",
    "class FeatureExtractor:\n",
    "    mode: typing.Literal[\"train\", \"eval\", \"infer\"]\n",
    "    device: torch.device\n",
    "    means: pd.Series | None\n",
    "    stds: pd.Series | None\n",
    "    afm_cache: dict[tuple[str, str], float]\n",
    "    text_encoder_name: str | None\n",
    "    text_tokenizer: nn.Module | None\n",
    "    text_encoder: nn.Module | None\n",
    "    text_config: transformers.configuration_utils.PretrainedConfig | None\n",
    "    text_enc_cache_path: pathlib.Path | None\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        text_encoder_name: str | None = None,\n",
    "        text_enc_cache_path: pathlib.Path | None = None,\n",
    "        device: torch.device = device,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.means = None\n",
    "        self.stds = None\n",
    "        self.afm_cache = {}\n",
    "        self.text_enc_cache_path = text_enc_cache_path\n",
    "        \n",
    "        self.text_encoder_name = text_encoder_name\n",
    "        self.text_tokenizer = None\n",
    "        self.text_encoder = None\n",
    "        self.text_config = None\n",
    "        \n",
    "        if text_encoder_name is not None:\n",
    "            self.text_tokenizer = AutoTokenizer.from_pretrained(text_encoder_name)\n",
    "            if hasattr(self.text_tokenizer, \"to\"):  # Distilbert doesn't, apprently\n",
    "                self.text_tokenizer = self.text_tokenizer.to(self.device)\n",
    "            self.text_encoder = AutoModel.from_pretrained(text_encoder_name).to(self.device)\n",
    "            self.text_config = self.text_encoder.config\n",
    "        \n",
    "        self.train()\n",
    "    \n",
    "    def train(self):\n",
    "        self.mode = \"train\"\n",
    "    \n",
    "    def eval(self):\n",
    "        self.mode = \"eval\"\n",
    "    \n",
    "    def infer(self):\n",
    "        self.mode = \"infer\"\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            \"means\": self.means,\n",
    "            \"stds\": self.stds,\n",
    "            \"afm_cache\": self.afm_cache,\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.means = state_dict[\"means\"]\n",
    "        self.stds = state_dict[\"stds\"]\n",
    "        self.afm_cache = state_dict[\"afm_cache\"]\n",
    "    \n",
    "    def dims(self) -> dict[str, int]:\n",
    "        return {\n",
    "            \"md\": len(self.METADATA_FIELDS),\n",
    "            \"substrings\": len(self.SUBSTRINGS),\n",
    "        } | {\n",
    "            field: compress or self.embed_size\n",
    "            for field, compress in self.TEXT_FIELDS\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def embed_size(self) -> int:\n",
    "        return self.text_config.hidden_size\n",
    "    \n",
    "    def extract(self, df: pd.DataFrame, override_cache: bool = False) -> dict[str, torch.Tensor]:\n",
    "        result: dict[str, torch.Tensor] = {}\n",
    "        \n",
    "        if self.text_enc_cache_path is None:\n",
    "            self._extract(df, result)\n",
    "            return result\n",
    "        \n",
    "        cf = self.text_enc_cache_path / f\"{self.mode}.ckpt\"\n",
    "        cf.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if cf.exists() and not override_cache:\n",
    "            encodings: dict[str, torch.Tensor] = torch.load(cf)\n",
    "            for col_name, value in encodings.items():\n",
    "                result[col_name] = value.to(self.device)\n",
    "        \n",
    "        keys_pre = len(result)\n",
    "        self._extract(df, result)\n",
    "        keys_post = len(result)\n",
    "        \n",
    "        if keys_post > keys_pre:\n",
    "            torch.save({\n",
    "                field: embedding.cpu().detach()\n",
    "                for field, embedding in result.items()\n",
    "            }, cf)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _extract(self, df: pd.DataFrame, result: dict[str, torch.Tensor]):\n",
    "        if \"md\" not in result:\n",
    "            result[\"md\"] = self.extract_raw_metadata(df)\n",
    "        \n",
    "        if \"substrings\" not in result:\n",
    "            result[\"substrings\"] = torch.tensor([df[\"full_text\"].str.count(x).fillna(0).to_numpy() for x in self.SUBSTRINGS], dtype=torch.float32, device=self.device).T\n",
    "        \n",
    "        for col_name, compress in self.TEXT_FIELDS:\n",
    "            if col_name in result:\n",
    "                continue\n",
    "            \n",
    "            emb = self.embed_texts(df[col_name])\n",
    "            \n",
    "            if compress is not None and compress < emb.shape[1]:\n",
    "                pca = PCA(n_components=compress)\n",
    "                emb = pca.fit_transform(emb.cpu().detach().numpy())\n",
    "                emb = torch.tensor(emb, dtype=torch.float32, device=self.device)\n",
    "            elif compress is not None:\n",
    "                print(f\"Warning: embedding for {col_name} zero-padded from {emb.shape[1]} to {compress}, consider reducing requested size\")\n",
    "                emb = torch.nn.functional.pad(emb, (0, compress - emb.shape[1]))\n",
    "            \n",
    "            result[col_name] = emb\n",
    "    \n",
    "    def extract_raw_metadata(self, df: pd.DataFrame) -> torch.Tensor:\n",
    "        md_cols: list[pd.Series] = []\n",
    "\n",
    "        for fn, col_name in tqdm(self.METADATA_FIELDS, desc=\"Extracting metadata\"):\n",
    "            md_cols.append(fn(self, df[col_name]))\n",
    "        \n",
    "        md: pd.DataFrame = pd.concat(md_cols, axis=1)\n",
    "        \n",
    "        # The second case shouldn't be triggered, but sometimes the preprocessor used during training is lost\n",
    "        if self.mode == \"train\":\n",
    "            self.means = md.mean().fillna(0)\n",
    "            self.stds = md.std().fillna(1)\n",
    "        \n",
    "        assert self.means is not None and self.stds is not None, \"You forgot to train/load the feature extractor\"\n",
    "\n",
    "        md = (md - self.means) / self.stds\n",
    "\n",
    "        return torch.from_numpy(md.to_numpy()).float().to(self.device)\n",
    "\n",
    "    def embed_texts(\n",
    "        self,\n",
    "        texts: pd.Series,\n",
    "        batch_size: int = 64,\n",
    "        progress: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        tokenizer = self.text_tokenizer\n",
    "        encoder = self.text_encoder\n",
    "        encoder.eval()\n",
    "\n",
    "        all_embeddings = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_offsets = range(0, len(texts), batch_size)\n",
    "            if progress:\n",
    "                batch_offsets = tqdm(batch_offsets, desc=f\"Embedding {texts.name or '<unnamed>'}\")\n",
    "            for i in batch_offsets:\n",
    "                batch_texts = texts.iloc[i:i + batch_size]\n",
    "                nonna = batch_texts.notna() & batch_texts.str.len().gt(0)\n",
    "\n",
    "                tokenized = tokenizer(\n",
    "                    batch_texts[nonna].tolist(),\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=self.text_config.max_position_embeddings\n",
    "                ).to(self.device)\n",
    "\n",
    "                outputs: transformers.modeling_outputs.BaseModelOutput = encoder(**tokenized)\n",
    "                last_hidden: torch.Tensor = outputs.last_hidden_state\n",
    "                mask: torch.Tensor = tokenized[\"attention_mask\"].unsqueeze(-1)\n",
    "                \n",
    "                masked_hidden = last_hidden * mask\n",
    "                summed = masked_hidden.sum(dim=1)\n",
    "                counts = mask.sum(dim=1)\n",
    "                embeddings = torch.zeros(len(batch_texts), last_hidden.shape[2], device=self.device)\n",
    "                nonna = nonna.reset_index(drop=True)\n",
    "                embeddings[nonna[nonna].index] = (summed / counts)\n",
    "\n",
    "                all_embeddings.append(embeddings)\n",
    "\n",
    "        return torch.cat(all_embeddings, dim=0)\n",
    "    \n",
    "    def apply_fill_mean(\n",
    "        self,\n",
    "        col: pd.Series,\n",
    "        func: typing.Callable[[typing.Any], typing.Any],\n",
    "    ) -> pd.Series:\n",
    "        col = col.map(func, na_action=\"ignore\")\n",
    "        \n",
    "        key = (col.name, func.__name__)\n",
    "        if self.mode == \"train\":\n",
    "            self.afm_cache[key] = col.mean()\n",
    "        assert key in self.afm_cache, \"You forgot to train/load the feature extractor\"\n",
    "        \n",
    "        return col.fillna(self.afm_cache[key])\n",
    "    \n",
    "    def md_bool(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(lambda x: (1 if x else -1), na_action=\"ignore\").fillna(0)\n",
    "\n",
    "    def md_len(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(len, na_action=\"ignore\").fillna(0)\n",
    "\n",
    "    def md_time(self, col: pd.Series) -> pd.Series:\n",
    "        return self.apply_fill_mean(col, lambda x: time.mktime(time.strptime(x, \"%a %b %d %H:%M:%S %z %Y\")))\n",
    "\n",
    "    def md_num(self, col: pd.Series) -> pd.Series:\n",
    "        return self.apply_fill_mean(col, pd.to_numeric)\n",
    "\n",
    "    def md_place(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(lambda x: int(x, 16), na_action=\"ignore\").fillna(0)\n",
    "    \n",
    "    def md_emoji_count(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(emoji.emoji_count, na_action=\"ignore\").fillna(0)\n",
    "    \n",
    "    METADATA_FIELDS: list[tuple[typing.Callable[[FeatureExtractor, pd.Series], pd.Series], str]] = [\n",
    "        (md_bool, \"is_quote_status\"),\n",
    "        (md_bool, \"is_reply\"),\n",
    "        (md_bool, \"possibly_sensitive\"),\n",
    "        (md_bool, \"quoted_status_user_verified\"),\n",
    "        (md_bool, \"user_is_translator\"),\n",
    "        (md_bool, \"user_geo_enabled\"),\n",
    "        (md_bool, \"user_profile_use_background_image\"),\n",
    "        (md_bool, \"user_default_profile\"),\n",
    "        \n",
    "        (md_len, \"full_text\"),\n",
    "        (md_len, \"source_name\"),\n",
    "        (md_len, \"in_reply_to_screen_name\"),\n",
    "        (md_len, \"quoted_status_extended_tweet_entities_urls\"),\n",
    "        (md_len, \"quoted_status_extended_tweet_entities_user_mentions\"),\n",
    "        (md_len, \"quoted_status_extended_tweet_full_text\"),\n",
    "        (md_len, \"quoted_status_entities_urls\"),\n",
    "        (md_len, \"quoted_status_user_profile_image_url_https\"),\n",
    "        (md_len, \"quoted_status_user_profile_background_image_url\"),\n",
    "        (md_len, \"quoted_status_user_profile_background_image_url_https\"),\n",
    "        (md_len, \"quoted_status_user_screen_name\"),\n",
    "        (md_len, \"quoted_status_user_name\"),\n",
    "        (md_len, \"entities_hashtags\"),\n",
    "        (md_len, \"entities_user_mentions\"),\n",
    "        (md_len, \"user_profile_image_url_https\"),\n",
    "        (md_len, \"user_profile_background_image_url\"),\n",
    "        (md_len, \"user_description\"),\n",
    "        (md_len, \"user_translator_type\"),\n",
    "        (md_len, \"user_url\"),\n",
    "        (md_len, \"user_profile_banner_url\"),\n",
    "        (md_len, \"user_location\"),\n",
    "        (md_len, \"display_text_range\"),\n",
    "        (md_len, \"extended_tweet_entities_urls\"),\n",
    "        (md_len, \"extended_tweet_entities_hashtags\"),\n",
    "        (md_len, \"extended_tweet_entities_user_mentions\"),\n",
    "        (md_len, \"quoted_status_permalink_expanded\"),\n",
    "        \n",
    "        (md_time, \"created_at\"),\n",
    "        (md_time, \"user_created_at\"),\n",
    "        (md_time, \"quoted_status_created_at\"),\n",
    "        (md_time, \"quoted_status_user_created_at\"),\n",
    "        \n",
    "        (md_num, \"user_statuses_count\"),\n",
    "        (md_num, \"user_listed_count\"),\n",
    "        (md_num, \"user_favourites_count\"),\n",
    "        (md_num, \"user_profile_background_tile\"),\n",
    "        (md_num, \"quoted_status_quote_count\"),\n",
    "        (md_num, \"quoted_status_user_followers_count\"),\n",
    "        (md_num, \"quoted_status_user_favourites_count\"),\n",
    "        (md_num, \"in_reply_to_status_id\"),\n",
    "        (md_num, \"user_profile_link_color_b\"),\n",
    "        (md_num, \"user_profile_background_color_r\"),\n",
    "        (md_num, \"user_profile_background_color_g\"),\n",
    "        (md_num, \"user_profile_link_color_g\"),\n",
    "        (md_num, \"user_profile_background_color_b\"),\n",
    "        (md_num, \"user_profile_link_color_r\"),\n",
    "        (md_num, \"user_profile_sidebar_border_color_b\"),\n",
    "        (md_num, \"user_profile_sidebar_fill_color_b\"),\n",
    "        (md_num, \"user_profile_sidebar_border_color_g\"),\n",
    "        (md_num, \"user_profile_sidebar_fill_color_g\"),\n",
    "        (md_num, \"user_profile_sidebar_fill_color_r\"),\n",
    "        (md_num, \"user_profile_sidebar_border_color_r\"),\n",
    "        \n",
    "        (md_place, \"quoted_status_place_id\"),\n",
    "        (md_place, \"place_id\"),\n",
    "        \n",
    "        (md_emoji_count, \"full_text\"),\n",
    "    ]\n",
    "\n",
    "    TEXT_FIELDS: list[tuple[str, int | None]] = [\n",
    "        (\"full_text\", None),\n",
    "        (\"user_description\", None),\n",
    "        (\"misc_text\", None),\n",
    "        # (\"source_name\", None),\n",
    "        # (\"in_reply_to_screen_name\", None),\n",
    "        # (\"quoted_status_user_screen_name\", None),\n",
    "        # (\"quoted_status_user_name\", None),\n",
    "    ]\n",
    "    \n",
    "    SUBSTRINGS: list[str] = [\n",
    "        emoji.demojize(em)\n",
    "        for em in [\n",
    "            \":backhand_index_pointing_right:\",\n",
    "            \":right_arrow:\",\n",
    "            \":right_arrow_curving_down:\",\n",
    "            \":play_button:\",\n",
    "        ]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1bc90d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    features: dict[str, torch.Tensor]\n",
    "    labels: torch.Tensor\n",
    "    device: torch.device\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_extractor: FeatureExtractor,\n",
    "        df: pd.DataFrame,\n",
    "        labels: pd.Series,\n",
    "        device: torch.device = device,\n",
    "    ):\n",
    "        self.features = feature_extractor.extract(df)\n",
    "        self.labels = torch.tensor(labels.to_numpy(), dtype=torch.long, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features[\"md\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"features\": {key: val[idx] for key, val in self.features.items()},\n",
    "            \"label\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    features = {\n",
    "        key: torch.stack([x[\"features\"][key] for x in batch])\n",
    "        for key in batch[0][\"features\"].keys()\n",
    "    }\n",
    "    labels = torch.stack([x[\"label\"] for x in batch])\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af4a722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "\n",
    "class TweetClassifier(nn.Module):\n",
    "    feature_sizes: dict[str, int]\n",
    "    \n",
    "    layer1: nn.ModuleDict\n",
    "    fc2: nn.Linear\n",
    "    fc3: nn.Linear\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_sizes: dict[str, int],\n",
    "        hidden_dim: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_sizes = feature_sizes\n",
    "        \n",
    "        self.layer1 = nn.ModuleDict()\n",
    "        \n",
    "        def _add(name, dropout: float):\n",
    "            self.layer1[name] = nn.Sequential(\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(feature_sizes[name], hidden_dim),\n",
    "            )\n",
    "        \n",
    "        _add(\"md\", 0.1)\n",
    "        _add(\"substrings\", 0.1)\n",
    "        _add(\"full_text\", 0.1)\n",
    "        _add(\"user_description\", 0.4)\n",
    "        _add(\"misc_text\", 0.1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, NUM_CLASSES)\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def forward(self, features: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n",
    "        batch_size = len(features[\"md\"])\n",
    "        \n",
    "        x = torch.zeros(batch_size, self.fc2.in_features, device=self.device)\n",
    "        # x = torch.zeros(batch_size, self.fc3.in_features, device=self.device)\n",
    "        \n",
    "        for name, module in self.layer1.items():\n",
    "            x += module(features[name])\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"probs\": probs,\n",
    "            \"log_probs\": log_probs,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7a47ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(\n",
    "    model: TweetClassifier,\n",
    "    train_ds: Dataset,\n",
    "    val_ds: Dataset,\n",
    "    epochs: int = 3,\n",
    "    lr: float = 2e-4,\n",
    "    weight_decay: float = 0.01,  # TODO: Lower?\n",
    "    max_grad_norm: float = 1.0,\n",
    "    freeze_components_after: dict[str, int] | None = None,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    "    optimizer: torch.optim.Optimizer | None = None,\n",
    "    checkpoints_path: pathlib.Path | str | None = \".\",\n",
    "    return_best: bool = False,\n",
    ") -> TweetClassifier:\n",
    "    if freeze_components_after is None:\n",
    "        freeze_components_after = {}\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    if optimizer is None:\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_file: pathlib.Path | None = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        status_bar = tqdm(train_loader, desc=\"Training\")\n",
    "\n",
    "        for features, labels in status_bar:\n",
    "            features: dict[str, torch.Tensor]\n",
    "            labels: torch.Tensor\n",
    "            features = {k: v.to(device) for k, v in features.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            out = model(features)\n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            status_bar.set_postfix({\"loss\": total_loss / (status_bar.n + 1)})\n",
    "        \n",
    "        print(f\"Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        val_metrics = evaluate_model(\n",
    "            model=model,\n",
    "            val_ds=val_ds,\n",
    "            device=device,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['acc']:.4f}\")\n",
    "\n",
    "        if checkpoints_path is not None:\n",
    "            ckpt = pathlib.Path(checkpoints_path) / f\"epoch_{epoch:02}.pt\"\n",
    "            torch.save(model.state_dict(), ckpt)\n",
    "            print(f\"Checkpoint saved to {ckpt}\")\n",
    "            \n",
    "            if val_metrics[\"loss\"] < best_val_loss:\n",
    "                best_val_loss = val_metrics[\"loss\"]\n",
    "                best_model_file = ckpt\n",
    "        \n",
    "        for comp, e in freeze_components_after.items():\n",
    "            if e != epoch:\n",
    "                continue\n",
    "            \n",
    "            print(f\"Freezing {comp} for the rest of training\")\n",
    "            for param in model.layer1[comp].parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    if return_best and best_model_file is not None:\n",
    "        print(f\"Best model: {best_model_file}\")\n",
    "        model.load_state_dict(torch.load(best_model_file))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: TweetClassifier,\n",
    "    val_ds: Dataset,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> tuple[float, float]:\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        status_bar = tqdm(val_loader, desc=\"Evaluating\")\n",
    "        \n",
    "        for features, labels in status_bar:\n",
    "            features: dict[str, torch.Tensor]\n",
    "            labels: torch.Tensor\n",
    "            features = {k: v.to(device) for k, v in features.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = model(features)\n",
    "            logits: torch.Tensor = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            count += labels.size(0)\n",
    "            \n",
    "            status_bar.set_postfix({\"loss\": total_loss / count, \"acc\": correct / count})\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / count,\n",
    "        \"acc\": correct / count,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "558a176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_model(\n",
    "    model: TweetClassifier,\n",
    "    feature_extractor: FeatureExtractor,\n",
    "    df: pd.DataFrame,\n",
    "    out_file: pathlib.Path | str | None = None,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> pd.Series:\n",
    "    feature_extractor.infer()\n",
    "    data_loader = DataLoader(\n",
    "        TweetDataset(feature_extractor, df, pd.Series(np.zeros(len(df))), device=device),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = torch.zeros(len(df), dtype=torch.long)\n",
    "    cur_idx = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(data_loader, desc=\"Inferring\"):\n",
    "            features: dict[str, torch.Tensor]\n",
    "            features = {k: v.to(device) for k, v in features.items()}\n",
    "\n",
    "            out = model(features)\n",
    "            logits: torch.Tensor = out[\"logits\"].cpu()\n",
    "            \n",
    "            predictions[cur_idx:cur_idx+len(features[\"md\"])] = logits.argmax(dim=-1)\n",
    "            cur_idx += len(features[\"md\"])\n",
    "    \n",
    "    df = df.copy()\n",
    "    df[\"pred_label\"] = pd.Series(predictions).astype(int)\n",
    "\n",
    "    # Reconciliation between same users\n",
    "    # Note: the pandas implementation OOM-ed, but the pure python one seems a lot slower\n",
    "    per_user_stats: dict[str, list[int]] = dict()\n",
    "    for _, row in df.iterrows():\n",
    "        per_user_stats.setdefault(row[\"user_hash\"], [0, 0])[int(row[\"pred_label\"])] += 1\n",
    "    \n",
    "    per_user_correct: dict[tuple[str, str], int] = dict()\n",
    "    for key, stats in per_user_stats.items():\n",
    "        if stats[0] == 0 or stats[1] == 0:\n",
    "            continue\n",
    "        \n",
    "        per_user_correct[key] = np.select(\n",
    "            [stats[0] > stats[1], stats[1] > stats[0]],\n",
    "            [0, 1],\n",
    "            default=np.random.randint(0, 2),\n",
    "        )\n",
    "    \n",
    "    del per_user_stats\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        key = row[\"user_hash\"]\n",
    "        if key in per_user_correct:\n",
    "            per_user_correct[key]\n",
    "            df.at[idx, \"pred_label\"] = per_user_correct[key]\n",
    "    \n",
    "    if out_file is not None:\n",
    "        output = df[[\"challenge_id\", \"pred_label\"]]\n",
    "        output.columns = [\"ID\", \"Prediction\"]\n",
    "        output.to_csv(out_file, index=False)\n",
    "    \n",
    "    return df[\"pred_label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0e62665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_group(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    group: pd.Series,\n",
    "    test_size: float = 0.15,\n",
    "    random_state: int = 42\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    gss = GroupShuffleSplit(\n",
    "        test_size=test_size,\n",
    "        n_splits=1,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    \n",
    "    train_idx, val_idx = next(gss.split(X, y, group))\n",
    "    \n",
    "    return X.iloc[train_idx], X.iloc[val_idx], y.iloc[train_idx], y.iloc[val_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957dd8c",
   "metadata": {},
   "source": [
    "# Training & inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff6068",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_name = \"camembert/camembert-large\"\n",
    "version = \"v21\"\n",
    "\n",
    "print(f\"Running: {text_encoder_name} {version}\")\n",
    "\n",
    "model_folder = pathlib.Path(\"./models/\") / version / text_encoder_name.split(\"/\")[-1]\n",
    "model_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# I guess a global cache does make more sense\n",
    "text_enc_cache_path = pathlib.Path(\"./text_enc_cache/\") / text_encoder_name.split(\"/\")[-1]\n",
    "text_enc_cache_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "feature_extractor = FeatureExtractor(text_encoder_name=text_encoder_name, text_enc_cache_path=text_enc_cache_path, device=device)\n",
    "\n",
    "f_ext_ckpt = text_enc_cache_path / \"feature_extractor.ckpt\"\n",
    "if f_ext_ckpt.exists():\n",
    "    feature_extractor.load_state_dict(torch.load(f_ext_ckpt, weights_only=False))  # This has a pd.Series in it, so otherwise torch 2.6+ complains\n",
    "\n",
    "# Note: changing the split invalidates the cache now!\n",
    "X_train_for_real, X_val, y_train_for_real, y_val = train_test_split_group(X_train, y_train, X_train[\"user_hash\"], test_size=0.1, random_state=42)\n",
    "\n",
    "feature_extractor.train()\n",
    "train_ds = TweetDataset(feature_extractor, X_train_for_real, y_train_for_real, device=device)\n",
    "\n",
    "torch.save(feature_extractor.state_dict(), f_ext_ckpt)\n",
    "\n",
    "feature_extractor.eval()\n",
    "val_ds = TweetDataset(feature_extractor, X_val, y_val, device=device)\n",
    "\n",
    "model = TweetClassifier(\n",
    "    feature_sizes=feature_extractor.dims(),\n",
    "    # hidden_dim=512,\n",
    "    # hidden_dim=384,\n",
    "    # hidden_dim=768,\n",
    "    hidden_dim=256,\n",
    ").to(device)\n",
    "\n",
    "model = train_model(\n",
    "    model,\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    lr=2e-4,\n",
    "    epochs=10,\n",
    "    freeze_components_after={\n",
    "        \"user_description\": 5,\n",
    "    },\n",
    "    batch_size=64,\n",
    "    device=device,\n",
    "    checkpoints_path=model_folder,\n",
    "    return_best=True,\n",
    ")\n",
    "# torch.save(model.state_dict(), model_folder / \"best_model.ckpt\")\n",
    "\n",
    "infer_with_model(\n",
    "    model,\n",
    "    feature_extractor,\n",
    "    X_kaggle,\n",
    "    batch_size=64,\n",
    "    device=device,\n",
    "    out_file=model_folder / f\"predictions-{version}.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ad414f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pretrained: camembert/camembert-large v21 e05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertModel were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Inferring: 100%|██████████| 1616/1616 [00:07<00:00, 204.29it/s]\n"
     ]
    }
   ],
   "source": [
    "text_encoder_name = \"camembert/camembert-large\"\n",
    "version = \"v21\"\n",
    "epoch = 5\n",
    "\n",
    "print(f\"Running pretrained: {text_encoder_name} {version} e{epoch:02}\")\n",
    "\n",
    "model_folder = pathlib.Path(\"./models/\") / version / text_encoder_name.split(\"/\")[-1]\n",
    "model_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "text_enc_cache_path = pathlib.Path(\"./text_enc_cache/\") / text_encoder_name.split(\"/\")[-1]\n",
    "text_enc_cache_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "feature_extractor = FeatureExtractor(text_encoder_name=text_encoder_name, text_enc_cache_path=text_enc_cache_path, device=device)\n",
    "feature_extractor.load_state_dict(torch.load(text_enc_cache_path / \"feature_extractor.ckpt\", weights_only=False))\n",
    "\n",
    "model = TweetClassifier(\n",
    "    feature_sizes=feature_extractor.dims(),\n",
    "    hidden_dim=256,\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(model_folder / f\"epoch_{epoch:02}.pt\"))\n",
    "\n",
    "good_predictions = infer_with_model(model, feature_extractor, X_kaggle, batch_size=64, device=device, out_file=model_folder / f\"predictions-{version}-e{epoch:02}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb900036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import typing\n",
    "import json\n",
    "import pathlib\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import transformers\n",
    "import transformers.modeling_outputs\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sklearn\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f873bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_KAGGLE = \"KAGGLE_DOCKER_IMAGE\" in os.environ\n",
    "\n",
    "DATASETS = pathlib.Path(\n",
    "    \".\"\n",
    "    if not IS_KAGGLE\n",
    "    else \"/kaggle/input/influencers-or-observers-predicting-social-roles/Kaggle2025\"\n",
    ")\n",
    "\n",
    "DATASET_TRAIN = DATASETS / \"train.jsonl\"\n",
    "DATASET_KAGGLE = DATASETS / \"kaggle_test.jsonl\"\n",
    "\n",
    "CACHE_DIR = pathlib.Path(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fc921d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Maybe make deterministic? A bit too late though, we already have trained a checkpoint with randomness\n",
    "# torch.random.manual_seed(42)\n",
    "# np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f28f61",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e16f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: pathlib.Path, cache: bool = False) -> pd.DataFrame:\n",
    "    path_pq = (CACHE_DIR / path.name).with_stem(f\"{path.stem}_raw\").with_suffix(\".parquet\")\n",
    "    \n",
    "    if cache and path_pq.exists():\n",
    "        return pd.read_parquet(path_pq)\n",
    "    \n",
    "    # This leaves things to be desired, since there's no way to specify dtypes\n",
    "    # and it assumes float instead of int, causing a loss in precision...\n",
    "    # But I guess it only matters for ids, which we'll probably discard in preprocessing anyway\n",
    "    result = pd.json_normalize(list(map(json.loads, path.read_bytes().splitlines())))\n",
    "    \n",
    "    if cache:\n",
    "        result.to_parquet(path_pq)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d1df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_json(DATASET_TRAIN, cache=True)\n",
    "kaggle_data = load_json(DATASET_KAGGLE, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bcf662",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ea7df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"is_reply\"] = df[\"in_reply_to_status_id\"].notna()\n",
    "    \n",
    "    # Remove various ID fields\n",
    "    # TODO: Could we actually use them for something?\n",
    "    # Note: challenge_id and label seem to be added for the kaggle challenge\n",
    "    df = df.drop(columns=[\n",
    "        \"in_reply_to_status_id_str\",\n",
    "        \"in_reply_to_status_id\",\n",
    "        \"in_reply_to_user_id_str\",\n",
    "        \"in_reply_to_user_id\",\n",
    "        \"quoted_status_id_str\",\n",
    "        \"quoted_status_id\",\n",
    "        \"id_str\",\n",
    "        \"quoted_status.in_reply_to_status_id_str\",\n",
    "        \"quoted_status.in_reply_to_status_id\",\n",
    "        \"quoted_status.in_reply_to_user_id_str\",\n",
    "        \"quoted_status.in_reply_to_user_id\",\n",
    "        \"quoted_status.id_str\",\n",
    "        \"quoted_status.id\",\n",
    "        \"quoted_status.user.id_str\",\n",
    "        \"quoted_status.user.id\",\n",
    "        \"quoted_status_permalink.expanded\",\n",
    "        \"quoted_status_permalink.display\",\n",
    "        \"quoted_status_permalink.url\",\n",
    "        \"quoted_status.quoted_status_id\",\n",
    "        \"quoted_status.quoted_status_id_str\",\n",
    "        \"quoted_status.place.id\",\n",
    "        \"place.id\",\n",
    "        \"lang\",  # Always \"fr\"\n",
    "        \"retweeted\",  # Always False\n",
    "        \"filter_level\",  # Always \"low\"\n",
    "        \"geo\",  # Always None\n",
    "        \"place\",  # Always None\n",
    "        \"coordinates\",  # Always None\n",
    "        \"contributors\",  # Always None\n",
    "        \"quote_count\",  # Always 0\n",
    "        \"reply_count\",  # Always 0\n",
    "        \"retweet_count\",  # Always 0\n",
    "        \"favorite_count\",  # Always 0\n",
    "        \"favorited\",  # Always False\n",
    "        \"quoted_status.geo\",  # Always None\n",
    "        \"quoted_status.place\",  # Always None\n",
    "        \"quoted_status.coordinates\",  # Always None\n",
    "        \"quoted_status.retweeted\",  # Always False\n",
    "        \"quoted_status.filter_level\",  # Always \"low\"\n",
    "        \"quoted_status.contributors\",  # Always None\n",
    "        \"quoted_status.user.utc_offset\",  # Always None\n",
    "        \"quoted_status.user.lang\",  # Always None\n",
    "        \"quoted_status.user.time_zone\",  # Always None\n",
    "        \"quoted_status.user.follow_request_sent\",  # Always None\n",
    "        \"quoted_status.user.following\",  # Always None\n",
    "        \"quoted_status.user.notifications\",  # Always None\n",
    "        \"user.default_profile_image\",  # Always False\n",
    "        \"user.protected\",  # Always False\n",
    "        \"user.contributors_enabled\",  # Always False\n",
    "        \"user.lang\",  # Always None\n",
    "        \"user.notifications\",  # Always None\n",
    "        \"user.following\",  # Always None\n",
    "        \"user.utc_offset\",  # Always None\n",
    "        \"user.time_zone\",  # Always None\n",
    "        \"user.follow_request_sent\",  # Always None\n",
    "    ])\n",
    "    \n",
    "    # TODO: Augment text with other string features?\n",
    "    df[\"full_text\"] = df.apply(lambda tweet: extract_full_text(tweet), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_full_text(tweet: pd.Series) -> str:\n",
    "    text: str = tweet[\"text\"]\n",
    "    \n",
    "    if not pd.isna(tweet[\"extended_tweet.full_text\"]):\n",
    "        text = tweet[\"extended_tweet.full_text\"]\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab1ee966",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(\"label\", axis=1)\n",
    "y_train = train_data[\"label\"]\n",
    "\n",
    "X_kaggle = kaggle_data\n",
    "\n",
    "X_train = preprocess(X_train)\n",
    "X_kaggle = preprocess(X_kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda51842",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b02e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: discard quoted_status.lang != \"fr\"?\n",
    "# TODO: some tweets are images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "class TweetClassifier(nn.Module):\n",
    "    tokenizer: nn.Module\n",
    "    encoder: nn.Module\n",
    "    metadata_dim: int\n",
    "    md_layernorm: nn.Module\n",
    "    fc1: nn.Module\n",
    "    fc2: nn.Module\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_encoder: str = \"distilbert-base-cased\", # \"camembert-base\", \"Geotrend/distilbert-base-en-fr-cased\", \"flaubert/flaubert_base_cased\", \"flaubert/flaubert_small_cased\"\n",
    "        metadata_dim: int = 16,\n",
    "        hidden_dim: int = 128,\n",
    "        max_length: int = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_encoder)\n",
    "        self.encoder = AutoModel.from_pretrained(pretrained_encoder)\n",
    "\n",
    "        # Don't finetune the encoder... yet?\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.encoder_dim = self.encoder.config.hidden_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.metadata_dim = metadata_dim\n",
    "        self.md_layernorm = nn.LayerNorm(metadata_dim)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.encoder_dim + metadata_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, NUM_CLASSES)\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def encode_text(self, texts: list[str]) -> torch.Tensor:\n",
    "        encoded: torch.Tensor = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        outputs: transformers.modeling_outputs.BaseModelOutput = self.encoder(**encoded)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        return cls_embeddings  # [batch, encoder_dim]\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        texts: list[str] | torch.Tensor,\n",
    "        metadata: torch.Tensor,\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns dict with:\n",
    "            \"logits\": tensor [batch_size, num_classes]\n",
    "            \"probs\": tensor [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "        batch_size = len(texts)\n",
    "        \n",
    "        if isinstance(texts, torch.Tensor):\n",
    "            text_vecs = texts\n",
    "        else:\n",
    "            text_vecs = self.encode_text(texts)  # [B, encoder_dim]\n",
    "\n",
    "        metadata = metadata.to(device)\n",
    "        assert metadata.shape == (batch_size, self.metadata_dim)\n",
    "        \n",
    "        metadata = self.md_layernorm(metadata)\n",
    "        \n",
    "        x = torch.cat([text_vecs, metadata], dim=1)\n",
    "\n",
    "        hidden = F.relu(self.fc1(x))\n",
    "        logits = self.fc2(hidden)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"probs\": probs,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5be1585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_DIM = 13  # Note: update when adding new fields!!\n",
    "\n",
    "def extract_metadata(df: pd.DataFrame) -> torch.Tensor:\n",
    "    md: list[pd.Series] = []\n",
    "\n",
    "    def md_add_bool(col: str):\n",
    "        md.append(df[col].apply(lambda x: (1 if x else -1) if pd.notnull(x) else 0))\n",
    "\n",
    "    md_add_bool(\"is_quote_status\")\n",
    "    md_add_bool(\"is_reply\")\n",
    "    md_add_bool(\"possibly_sensitive\")\n",
    "    md_add_bool(\"quoted_status.user.verified\")\n",
    "\n",
    "    def md_add_len(col: str):\n",
    "        # pd.notnull considered lists as not scalar\n",
    "        md.append(df[col].apply(lambda x: len(x) if x is not None and not (isinstance(x, float) and np.isnan(x)) else 0))\n",
    "\n",
    "    md_add_len(\"full_text\")\n",
    "    md_add_len(\"extended_tweet.entities.urls\")\n",
    "    md_add_len(\"extended_tweet.entities.hashtags\")\n",
    "    md_add_len(\"extended_tweet.entities.user_mentions\")\n",
    "    md_add_len(\"extended_tweet.entities.symbols\")\n",
    "\n",
    "    def md_add_time(col: str):\n",
    "        tmp = df[col].apply(lambda x: time.mktime(time.strptime(x, \"%a %b %d %H:%M:%S %z %Y\")) if pd.notnull(x) else pd.NA)\n",
    "        md.append(tmp.fillna(tmp.mean()))\n",
    "\n",
    "    md_add_time(\"created_at\")  # TODO: Same as timestamp_ms / 1000?\n",
    "    md_add_time(\"quoted_status.user.created_at\")\n",
    "\n",
    "    def md_add_num(col: str):\n",
    "        tmp = df[col].apply(pd.to_numeric)\n",
    "        md.append(tmp.fillna(tmp.mean()))\n",
    "\n",
    "    md_add_num(\"quoted_status.user.followers_count\")\n",
    "    md_add_num(\"timestamp_ms\")\n",
    "\n",
    "    return torch.from_numpy(np.array(md)).transpose(0, 1).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bc90d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    texts: list[str]\n",
    "    metadata: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "    device: torch.device\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, labels: pd.Series, device: torch.device = device):\n",
    "        self.texts = df[\"full_text\"].tolist()\n",
    "        self.metadata = extract_metadata(df).to(device)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"text\": self.texts[idx],\n",
    "            \"metadata\": self.metadata[idx],\n",
    "            \"label\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts = [x[\"text\"] for x in batch]\n",
    "    metadata = torch.stack([x[\"metadata\"] for x in batch])\n",
    "    labels = torch.stack([x[\"label\"] for x in batch])\n",
    "    return texts, metadata, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a47ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: TweetClassifier,\n",
    "    train_ds: Dataset,\n",
    "    epochs: int = 3,\n",
    "    lr: float = 2e-4,\n",
    "    weight_decay: float = 0.01,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> TweetClassifier:\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        status_bar = tqdm(train_loader, desc=\"Training\")\n",
    "\n",
    "        for texts, metadata, labels in status_bar:\n",
    "            texts: list[str]\n",
    "            metadata: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "            \n",
    "            metadata = metadata.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            out = model(\n",
    "                texts=texts,\n",
    "                metadata=metadata,\n",
    "            )\n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)  # TODO: ?\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            status_bar.set_postfix({\"loss\": total_loss / (status_bar.n + 1)})\n",
    "\n",
    "        print(f\"Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa5e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: TweetClassifier,\n",
    "    val_ds: Dataset,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> tuple[float, float]:\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        status_bar = tqdm(val_loader, desc=\"Evaluating\")\n",
    "        \n",
    "        for texts, metadata, labels in status_bar:\n",
    "            texts: list[str]\n",
    "            metadata: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "            metadata = metadata.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = model(\n",
    "                texts=texts,\n",
    "                metadata=metadata,\n",
    "            )\n",
    "            logits: torch.Tensor = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            count += labels.size(0)\n",
    "            \n",
    "            status_bar.set_postfix({\"loss\": total_loss / (status_bar.n + 1), \"acc\": correct / count})\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / len(val_loader),\n",
    "        \"acc\": correct / count,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "558a176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_model(\n",
    "    model: TweetClassifier,\n",
    "    df: pd.DataFrame,\n",
    "    out_file: pathlib.Path | str | None = None,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> torch.Tensor:\n",
    "    data_loader = DataLoader(\n",
    "        TweetDataset(df, torch.zeros(len(df), dtype=torch.long, device=device), device=device),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = torch.zeros(len(df), dtype=torch.long)\n",
    "    cur_idx = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, metadata, _ in tqdm(data_loader, desc=\"Inferring\"):\n",
    "            texts: list[str]\n",
    "            metadata: torch.Tensor\n",
    "            metadata = metadata.to(device)\n",
    "\n",
    "            out = model(\n",
    "                texts=texts,\n",
    "                metadata=metadata,\n",
    "            )\n",
    "            logits: torch.Tensor = out[\"logits\"].cpu()\n",
    "            \n",
    "            predictions[cur_idx:cur_idx+len(texts)] = logits.argmax(dim=-1)\n",
    "            cur_idx += len(texts)\n",
    "    \n",
    "    if out_file is not None:\n",
    "        output = pd.concat([df[\"challenge_id\"], pd.DataFrame(predictions)], axis=1, ignore_index=True)\n",
    "        output.columns = [\"ID\", \"Prediction\"]\n",
    "        output.to_csv(out_file, index=False)\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55cd1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TweetClassifier(\n",
    "    pretrained_encoder=\"camembert-base\",\n",
    "    metadata_dim=METADATA_DIM,\n",
    "    hidden_dim=128,\n",
    "    max_length=256\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f1b1bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abel\\AppData\\Local\\Temp\\ipykernel_26644\\54356709.py:26: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  md.append(tmp.fillna(tmp.mean()))\n"
     ]
    }
   ],
   "source": [
    "full_train_ds = TweetDataset(X_train, y_train, device=device)\n",
    "\n",
    "train_ds, val_ds = random_split(full_train_ds, [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(version: str) -> pathlib.Path:\n",
    "    return pathlib.Path(\n",
    "        f\"./models/{version}/model-{version}.pt\"\n",
    "        if not IS_KAGGLE\n",
    "        else f\"/kaggle/input/model-{version}-pt/model-{version}.pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = get_model_path(\"v1\")\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "# model = train_model(model, train_ds, epochs=3, batch_size=64, device=device)\n",
    "# torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1426f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python310\\lib\\site-packages\\transformers\\models\\camembert\\modeling_camembert.py:364: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.595617502128955, 0.6738105997030533)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Pre-encode all texts?\n",
    "evaluate_model(model, val_ds, batch_size=64, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abel\\AppData\\Local\\Temp\\ipykernel_26644\\54356709.py:26: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  md.append(tmp.fillna(tmp.mean()))\n",
      "C:\\Users\\Abel\\AppData\\Local\\Temp\\ipykernel_26644\\2988799321.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
      "Inferring: 100%|██████████| 1616/1616 [09:59<00:00,  2.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_with_model(model, X_kaggle, batch_size=64, out_file=\"predictions-v1.csv\", device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

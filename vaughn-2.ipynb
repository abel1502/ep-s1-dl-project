{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb900036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import typing\n",
    "import json\n",
    "import pathlib\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import transformers\n",
    "import transformers.modeling_outputs\n",
    "import transformers.configuration_utils\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f873bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_KAGGLE = \"KAGGLE_DOCKER_IMAGE\" in os.environ\n",
    "\n",
    "DATASETS = pathlib.Path(\n",
    "    \".\"\n",
    "    if not IS_KAGGLE\n",
    "    else \"/kaggle/input/influencers-or-observers-predicting-social-roles/Kaggle2025\"\n",
    ")\n",
    "\n",
    "DATASET_TRAIN = DATASETS / \"train.jsonl\"\n",
    "DATASET_KAGGLE = DATASETS / \"kaggle_test.jsonl\"\n",
    "\n",
    "CACHE_DIR = pathlib.Path(\".\")\n",
    "VERSION = \"v12-vaughn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fc921d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e20b6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f28f61",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e16f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: pathlib.Path, cache: bool = False) -> pd.DataFrame:\n",
    "    path_pq = (CACHE_DIR / path.name).with_stem(f\"{path.stem}_raw\").with_suffix(\".parquet\")\n",
    "    \n",
    "    if cache and path_pq.exists():\n",
    "        return pd.read_parquet(path_pq)\n",
    "    \n",
    "    # This leaves things to be desired, since there's no way to specify dtypes\n",
    "    # and it assumes float instead of int, causing a loss in precision...\n",
    "    # But I guess it only matters for ids, which we'll probably discard in preprocessing anyway\n",
    "    result = pd.json_normalize(list(map(json.loads, path.read_bytes().splitlines())))\n",
    "    \n",
    "    if cache:\n",
    "        result.to_parquet(path_pq)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d1df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_json(DATASET_TRAIN, cache=True)\n",
    "kaggle_data = load_json(DATASET_KAGGLE, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bcf662",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # For technical reasons, any text columns we want to use should have no dots in their names.\n",
    "    # The simplest way to achieve this is to replace all dots indiscriminately.\n",
    "    \n",
    "    df = df.rename(columns=lambda x: x.replace(\".\", \"_\"))\n",
    "    \n",
    "    df[\"is_reply\"] = df[\"in_reply_to_status_id\"].notna()\n",
    "    \n",
    "    df = df.drop(columns=[\n",
    "        \"in_reply_to_status_id_str\",\n",
    "        # \"in_reply_to_status_id\",\n",
    "        \"in_reply_to_user_id_str\",\n",
    "        \"in_reply_to_user_id\",\n",
    "        \"quoted_status_id_str\",\n",
    "        \"quoted_status_id\",\n",
    "        \"id_str\",\n",
    "        \"quoted_status_in_reply_to_status_id_str\",\n",
    "        \"quoted_status_in_reply_to_status_id\",\n",
    "        \"quoted_status_in_reply_to_user_id_str\",\n",
    "        \"quoted_status_in_reply_to_user_id\",\n",
    "        \"quoted_status_id_str\",\n",
    "        \"quoted_status_id\",\n",
    "        \"quoted_status_user_id_str\",\n",
    "        \"quoted_status_user_id\",\n",
    "        # \"quoted_status_permalink_expanded\",\n",
    "        \"quoted_status_permalink_display\",\n",
    "        \"quoted_status_permalink_url\",\n",
    "        \"quoted_status_quoted_status_id\",\n",
    "        \"quoted_status_quoted_status_id_str\",\n",
    "        # \"quoted_status_place_id\",\n",
    "        # \"place_id\",\n",
    "        \"lang\",  # Always \"fr\"\n",
    "        \"retweeted\",  # Always False\n",
    "        \"filter_level\",  # Always \"low\"\n",
    "        \"geo\",  # Always None\n",
    "        \"place\",  # Always None\n",
    "        \"coordinates\",  # Always None\n",
    "        \"contributors\",  # Always None\n",
    "        \"quote_count\",  # Always 0\n",
    "        \"reply_count\",  # Always 0\n",
    "        \"retweet_count\",  # Always 0\n",
    "        \"favorite_count\",  # Always 0\n",
    "        \"favorited\",  # Always False\n",
    "        \"quoted_status_geo\",  # Always None\n",
    "        \"quoted_status_place\",  # Always None\n",
    "        \"quoted_status_coordinates\",  # Always None\n",
    "        \"quoted_status_retweeted\",  # Always False\n",
    "        \"quoted_status_filter_level\",  # Always \"low\"\n",
    "        \"quoted_status_contributors\",  # Always None\n",
    "        \"quoted_status_user_utc_offset\",  # Always None\n",
    "        \"quoted_status_user_lang\",  # Always None\n",
    "        \"quoted_status_user_time_zone\",  # Always None\n",
    "        \"quoted_status_user_follow_request_sent\",  # Always None\n",
    "        \"quoted_status_user_following\",  # Always None\n",
    "        \"quoted_status_user_notifications\",  # Always None\n",
    "        \"user_default_profile_image\",  # Always False\n",
    "        \"user_protected\",  # Always False\n",
    "        \"user_contributors_enabled\",  # Always False\n",
    "        \"user_lang\",  # Always None\n",
    "        \"user_notifications\",  # Always None\n",
    "        \"user_following\",  # Always None\n",
    "        \"user_utc_offset\",  # Always None\n",
    "        \"user_time_zone\",  # Always None\n",
    "        \"user_follow_request_sent\",  # Always None\n",
    "    ])\n",
    "    \n",
    "    df[\"full_text\"] = df.apply(lambda tweet: extract_full_text(tweet), axis=1)\n",
    "    \n",
    "    source_split = df[\"source\"].str.removeprefix(\"<a href=\\\"\").str.removesuffix(\"</a>\").str.split(\"\\\" rel=\\\"nofollow\\\">\").map(lambda x: x if len(x) == 2 else pd.NA)\n",
    "    df[\"source_url\"] = source_split.map(lambda x: x[0], na_action=\"ignore\")\n",
    "    df[\"source_name\"] = source_split.map(lambda x: x[1], na_action=\"ignore\")\n",
    "    \n",
    "    df[\"misc_text\"] = df.apply(\n",
    "        lambda x: \"via: {0}; reply: @{1}; quote: @{2} {3}\".format(x[\"source_name\"], x[\"in_reply_to_screen_name\"], x[\"quoted_status_user_screen_name\"], x[\"quoted_status_user_name\"]), axis=1,\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_full_text(tweet: pd.Series) -> str:\n",
    "    text: str = tweet[\"text\"]\n",
    "    \n",
    "    if not pd.isna(tweet[\"extended_tweet_full_text\"]):\n",
    "        text = tweet[\"extended_tweet_full_text\"]\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1ee966",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(\"label\", axis=1)\n",
    "y_train = train_data[\"label\"]\n",
    "\n",
    "X_kaggle = kaggle_data\n",
    "\n",
    "X_train = preprocess(X_train)\n",
    "X_kaggle = preprocess(X_kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c8d7e",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda51842",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Made this a class to hold all the caches. It may resemble an nn.Module, but isn't one!\n",
    "class FeatureExtractor:\n",
    "    text_encoder_name: str | None\n",
    "    text_tokenizer: nn.Module | None\n",
    "    text_encoder: nn.Module | None\n",
    "    text_config: transformers.configuration_utils.PretrainedConfig | None\n",
    "    text_enc_cache_path: pathlib.Path | None\n",
    "    \n",
    "    # New attribute to hold pre-computed embeddings\n",
    "    text_encodings: dict[str, dict[str, torch.Tensor]]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        text_encoder_name: str | None = None,\n",
    "        text_enc_cache_path: pathlib.Path | None = None,\n",
    "        device: torch.device = device,\n",
    "    ):\n",
    "        # super().__init__() # Removed this line as FeatureExtractor is not an nn.Module\n",
    "        self.device = device\n",
    "        self.means = None\n",
    "        self.stds = None\n",
    "        self.afm_cache = {}\n",
    "        self.text_enc_cache_path = text_enc_cache_path\n",
    "        \n",
    "        self.text_encoder_name = text_encoder_name\n",
    "        self.text_tokenizer = None\n",
    "        self.text_encoder = None\n",
    "        self.text_config = None\n",
    "        self.text_encodings = {\"train\": {}, \"infer\": {}} # Initialize dict for train/infer cache\n",
    "        \n",
    "        # Load Text Encoder/Tokenizer only if text_encoder_name is provided\n",
    "        if text_encoder_name is not None:\n",
    "            self.text_tokenizer = AutoTokenizer.from_pretrained(text_encoder_name)\n",
    "            if hasattr(self.text_tokenizer, \"to\"):\n",
    "                self.text_tokenizer = self.text_tokenizer.to(self.device)\n",
    "            self.text_encoder = AutoModel.from_pretrained(text_encoder_name).to(self.device)\n",
    "            self.text_config = self.text_encoder.config\n",
    "            \n",
    "        self.train() # Default to training mode\n",
    "    \n",
    "    def freeze_encoder(self):\n",
    "        if self.text_encoder is not None:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.text_encoder.eval()\n",
    "            print(\"Text encoder frozen.\")\n",
    "\n",
    "    def unfreeze_encoder(self):\n",
    "        if self.text_encoder is not None:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"Text encoder unfrozen.\")\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        if self.text_encoder is not None:\n",
    "            self.text_encoder.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        if self.text_encoder is not None:\n",
    "            self.text_encoder.eval()\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            \"means\": self.means,\n",
    "            \"stds\": self.stds,\n",
    "            \"afm_cache\": self.afm_cache,\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.means = state_dict[\"means\"]\n",
    "        self.stds = state_dict[\"stds\"]\n",
    "        self.afm_cache = state_dict[\"afm_cache\"]\n",
    "    \n",
    "    def dims(self) -> dict[str, int]:\n",
    "        return {\n",
    "            \"md\": len(self.METADATA_FIELDS),\n",
    "        } | {\n",
    "            field: compress or self.embed_size\n",
    "            for field, compress in self.TEXT_FIELDS\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def embed_size(self) -> int:\n",
    "        return self.text_config.hidden_size\n",
    "    \n",
    "    def extract(self, df: pd.DataFrame, split_name: str, override_cache: bool = False) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extracts features, loading or computing text embeddings from cache.\n",
    "        split_name should be 'train' or 'infer'\n",
    "        \"\"\"\n",
    "        result: dict[str, torch.Tensor] = {}\n",
    "        cache_key = split_name\n",
    "        \n",
    "        # 1. Metadata extraction (always computed)\n",
    "        result[\"md\"] = self.extract_raw_metadata(df)\n",
    "        \n",
    "        # 2. Text embedding extraction (cached)\n",
    "        cf = self.text_enc_cache_path / f\"{cache_key}.ckpt\"\n",
    "        cf.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if not override_cache and cf.exists():\n",
    "            print(f\"Loading cached encodings for {split_name}...\")\n",
    "            # Load embeddings into the internal dictionary first\n",
    "            self.text_encodings[split_name] = torch.load(cf)\n",
    "            \n",
    "            # Transfer to the result dictionary\n",
    "            for col_name, value in self.text_encodings[split_name].items():\n",
    "                result[col_name] = value.to(self.device)\n",
    "                \n",
    "            # Perform PCA/padding only on loaded tensors if needed\n",
    "            for col_name, compress in self.TEXT_FIELDS:\n",
    "                if col_name in result:\n",
    "                     if compress is not None and compress != result[col_name].shape[1]:\n",
    "                         # This part is complex if PCA was applied, best to ensure PCA is part of the initial encoding if cached.\n",
    "                         # Since the original notebook applied PCA *after* encoding but *before* caching, \n",
    "                         # we assume the cached tensor is the final (potentially PCA'd/padded) result.\n",
    "                         # If you need to re-apply PCA after loading, you must store the original embeddings and PCA components.\n",
    "                         # For now, we assume the cached size is the intended size (either original or compressed).\n",
    "                         pass \n",
    "        else:\n",
    "            print(f\"Computing and caching embeddings for {split_name}...\")\n",
    "            self.text_encodings[split_name] = {}\n",
    "            for col_name, compress in self.TEXT_FIELDS:\n",
    "                emb = self.embed_texts(df[col_name])\n",
    "                \n",
    "                # Apply PCA/Padding\n",
    "                if compress is not None and compress < emb.shape[1]:\n",
    "                    pca = PCA(n_components=compress)\n",
    "                    # PCA requires NumPy/CPU, ensure the tensor is on CPU before converting to NumPy\n",
    "                    emb_np = emb.cpu().detach().numpy()\n",
    "                    emb_np_compressed = pca.fit_transform(emb_np)\n",
    "                    emb = torch.tensor(emb_np_compressed, dtype=torch.float32, device=self.device)\n",
    "                    print(f\"Applied PCA to {col_name} reducing size from {emb_np.shape[1]} to {compress}\")\n",
    "                elif compress is not None and compress > emb.shape[1]:\n",
    "                    print(f\"Warning: embedding for {col_name} zero-padded from {emb.shape[1]} to {compress}\")\n",
    "                    emb = torch.nn.functional.pad(emb, (0, compress - emb.shape[1]))\n",
    "                \n",
    "                result[col_name] = emb\n",
    "                self.text_encodings[split_name][col_name] = emb.cpu().detach()\n",
    "                \n",
    "            # Save the computed embeddings\n",
    "            torch.save(self.text_encodings[split_name], cf)\n",
    "            print(f\"Encodings saved to {cf}\")\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    # The _extract method is removed as its logic is now inside extract\n",
    "    \n",
    "    def extract_raw_metadata(self, df: pd.DataFrame) -> torch.Tensor:\n",
    "        # ... (Keep existing implementation of extract_raw_metadata)\n",
    "        md_cols: list[pd.Series] = []\n",
    "\n",
    "        for fn, col_name in tqdm(self.METADATA_FIELDS, desc=\"Extracting metadata\"):\\\n",
    "            md_cols.append(fn(self, df[col_name]))\n",
    "        \n",
    "        md: pd.DataFrame = pd.concat(md_cols, axis=1)\n",
    "        \n",
    "        if self.training:\n",
    "            self.means = md.mean().fillna(0)\n",
    "            self.stds = md.std().fillna(1)\n",
    "            self.stds = self.stds.replace(0, 1)\n",
    "        \n",
    "        assert self.means is not None and self.stds is not None, \"You forgot to train/load the feature extractor\"\n",
    "\n",
    "        md = (md - self.means) / self.stds\n",
    "\n",
    "        return torch.from_numpy(md.to_numpy()).float().to(self.device)\n",
    "\n",
    "    def embed_texts(\n",
    "        self,\n",
    "        texts: pd.Series,\n",
    "        batch_size: int = 64,\n",
    "        progress: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        # ... (Keep existing implementation of embed_texts)\n",
    "        # Ensure encoder is available before calling\n",
    "        if self.text_encoder is None or self.text_tokenizer is None:\n",
    "            raise ValueError(\"Text encoder and tokenizer must be loaded to embed texts.\")\n",
    "            \n",
    "        tokenizer = self.text_tokenizer\n",
    "        encoder = self.text_encoder\n",
    "        encoder.eval() # Always evaluate the encoder when embedding texts\n",
    "\n",
    "        all_embeddings = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_offsets = range(0, len(texts), batch_size)\n",
    "            if progress:\n",
    "                batch_offsets = tqdm(batch_offsets, desc=f\"Embedding {texts.name or '<unnamed>'}\")\n",
    "            for i in batch_offsets:\n",
    "                batch_texts = texts.iloc[i:i + batch_size]\n",
    "                nonna = batch_texts.notna() & batch_texts.str.len().gt(0)\n",
    "\n",
    "                tokenized = tokenizer(\n",
    "                    batch_texts[nonna].tolist(),\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=self.text_config.max_position_embeddings\n",
    "                ).to(self.device)\n",
    "\n",
    "                outputs: transformers.modeling_outputs.BaseModelOutput = encoder(**tokenized)\n",
    "                last_hidden: torch.Tensor = outputs.last_hidden_state\n",
    "                mask: torch.Tensor = tokenized[\"attention_mask\"].unsqueeze(-1)\n",
    "                \n",
    "                masked_hidden = last_hidden * mask\n",
    "                summed = masked_hidden.sum(dim=1)\n",
    "                counts = mask.sum(dim=1)\n",
    "                embeddings = torch.zeros(len(batch_texts), last_hidden.shape[2], device=self.device)\n",
    "                nonna = nonna.reset_index(drop=True)\n",
    "                embeddings[nonna[nonna].index] = (summed / counts)\n",
    "\n",
    "                all_embeddings.append(embeddings)\n",
    "\n",
    "        return torch.cat(all_embeddings, dim=0)\n",
    "    \n",
    "    def apply_fill_mean(\n",
    "        self,\n",
    "        col: pd.Series,\n",
    "        func: typing.Callable[[typing.Any], typing.Any],\n",
    "    ) -> pd.Series:\n",
    "        col = col.map(func, na_action=\"ignore\")\n",
    "        \n",
    "        key = (col.name, func.__name__)\n",
    "        if self.training:\n",
    "            self.afm_cache[key] = col.mean()\n",
    "        assert key in self.afm_cache, \"You forgot to train/load the feature extractor\"\n",
    "        \n",
    "        return col.fillna(self.afm_cache[key])\n",
    "    \n",
    "    def md_bool(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(lambda x: (1 if x else -1), na_action=\"ignore\").fillna(0)\n",
    "\n",
    "    def md_len(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(len, na_action=\"ignore\").fillna(0)\n",
    "\n",
    "    def md_time(self, col: pd.Series) -> pd.Series:\n",
    "        return self.apply_fill_mean(col, lambda x: time.mktime(time.strptime(x, \"%a %b %d %H:%M:%S %z %Y\")))\n",
    "\n",
    "    def md_num(self, col: pd.Series) -> pd.Series:\n",
    "        return self.apply_fill_mean(col, pd.to_numeric)\n",
    "\n",
    "    def md_place(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(lambda x: int(x, 16), na_action=\"ignore\").fillna(0)\n",
    "    \n",
    "    METADATA_FIELDS: list[tuple[typing.Callable[[FeatureExtractor, pd.Series], pd.Series], str]] = [\n",
    "        (md_bool, \"is_quote_status\"),\n",
    "        (md_bool, \"is_reply\"),\n",
    "        (md_bool, \"possibly_sensitive\"),\n",
    "        (md_bool, \"quoted_status_user_verified\"),\n",
    "        (md_bool, \"user_is_translator\"),\n",
    "        (md_bool, \"user_geo_enabled\"),\n",
    "        (md_bool, \"user_profile_use_background_image\"),\n",
    "        (md_bool, \"user_default_profile\"),\n",
    "        \n",
    "        (md_len, \"full_text\"),\n",
    "        (md_len, \"source_name\"),\n",
    "        (md_len, \"in_reply_to_screen_name\"),\n",
    "        (md_len, \"quoted_status_extended_tweet_entities_urls\"),\n",
    "        (md_len, \"quoted_status_extended_tweet_entities_user_mentions\"),\n",
    "        (md_len, \"quoted_status_extended_tweet_full_text\"),\n",
    "        (md_len, \"quoted_status_entities_urls\"),\n",
    "        (md_len, \"quoted_status_user_profile_image_url_https\"),\n",
    "        (md_len, \"quoted_status_user_profile_background_image_url\"),\n",
    "        (md_len, \"quoted_status_user_profile_background_image_url_https\"),\n",
    "        (md_len, \"quoted_status_user_screen_name\"),\n",
    "        (md_len, \"quoted_status_user_name\"),\n",
    "        (md_len, \"entities_hashtags\"),\n",
    "        (md_len, \"entities_user_mentions\"),\n",
    "        (md_len, \"user_profile_image_url_https\"),\n",
    "        (md_len, \"user_profile_background_image_url\"),\n",
    "        (md_len, \"user_description\"),\n",
    "        (md_len, \"user_translator_type\"),\n",
    "        (md_len, \"user_url\"),\n",
    "        (md_len, \"user_profile_banner_url\"),\n",
    "        (md_len, \"user_location\"),\n",
    "        (md_len, \"display_text_range\"),\n",
    "        (md_len, \"extended_tweet_entities_urls\"),\n",
    "        (md_len, \"extended_tweet_entities_hashtags\"),\n",
    "        (md_len, \"extended_tweet_entities_user_mentions\"),\n",
    "        (md_len, \"quoted_status_permalink_expanded\"),\n",
    "        \n",
    "        (md_time, \"created_at\"),\n",
    "        (md_time, \"user_created_at\"),\n",
    "        (md_time, \"quoted_status_created_at\"),\n",
    "        (md_time, \"quoted_status_user_created_at\"),\n",
    "        \n",
    "        (md_num, \"user_statuses_count\"),\n",
    "        (md_num, \"user_listed_count\"),\n",
    "        (md_num, \"user_favourites_count\"),\n",
    "        (md_num, \"user_profile_background_tile\"),\n",
    "        (md_num, \"quoted_status_quote_count\"),\n",
    "        (md_num, \"quoted_status_user_followers_count\"),\n",
    "        (md_num, \"quoted_status_user_favourites_count\"),\n",
    "        (md_num, \"in_reply_to_status_id\"),\n",
    "        \n",
    "        (md_place, \"quoted_status_place_id\"),\n",
    "        (md_place, \"place_id\"),\n",
    "    ]\n",
    "\n",
    "    TEXT_FIELDS: list[tuple[str, int | None]] = [\n",
    "        (\"full_text\", None),\n",
    "        (\"user_description\", 64),\n",
    "        (\"misc_text\", None),\n",
    "        (\"source_name\", None),\n",
    "        # (\"in_reply_to_screen_name\", None),\n",
    "        # (\"quoted_status_user_screen_name\", None),\n",
    "        # (\"quoted_status_user_name\", None),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bc90d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    features: dict[str, torch.Tensor]\n",
    "    labels: torch.Tensor\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        features: dict[str, torch.Tensor],\n",
    "        labels: pd.Series,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        self.features = features\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features[\"md\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"features\": {key: val[idx] for key, val in self.features.items()},\n",
    "            \"label\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    features = {\n",
    "        key: torch.stack([x[\"features\"][key] for x in batch])\n",
    "        for key in batch[0][\"features\"].keys()\n",
    "    }\n",
    "    labels = torch.stack([x[\"label\"] for x in batch])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af4a722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "\n",
    "class TweetClassifier(nn.Module):\n",
    "    feature_sizes: dict[str, int]\n",
    "    \n",
    "    layer1: nn.ModuleDict\n",
    "    fc2: nn.Linear\n",
    "    fc3: nn.Linear\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_sizes: dict[str, int],\n",
    "        hidden_dim: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_sizes = feature_sizes\n",
    "        \n",
    "        self.layer1 = nn.ModuleDict()\n",
    "        \n",
    "        def _add(name, dropout: float):\n",
    "            self.layer1[name] = nn.Sequential(\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(feature_sizes[name], hidden_dim),\n",
    "            )\n",
    "        \n",
    "        _add(\"md\", 0.1)\n",
    "        _add(\"full_text\", 0.1)\n",
    "        _add(\"user_description\", 0.37)\n",
    "        _add(\"misc_text\", 0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, NUM_CLASSES)\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def forward(self, features: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n",
    "        batch_size = len(features[\"md\"])\n",
    "        \n",
    "        x = torch.zeros(batch_size, self.fc2.in_features, device=self.device)\n",
    "        \n",
    "        for name, module in self.layer1.items():\n",
    "            x += module(features[name])\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"probs\": probs,\n",
    "            \"log_probs\": log_probs,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7a47ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: TweetClassifier,\n",
    "    train_ds: Dataset,\n",
    "    val_ds: Dataset,\n",
    "    epochs: int = 3,\n",
    "    lr: float = 2e-4,\n",
    "    weight_decay: float = 0.01,  # TODO: Lower?\n",
    "    max_grad_norm: float = 1.0,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    "    optimizer: torch.optim.Optimizer | None = None,\n",
    "    checkpoints_path: pathlib.Path | str | None = \".\",\n",
    "    return_best: bool = False,\n",
    ") -> TweetClassifier:\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    if optimizer is None:\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_file: pathlib.Path | None = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        status_bar = tqdm(train_loader, desc=\"Training\")\n",
    "\n",
    "        for features, labels in status_bar:\n",
    "            features: dict[str, torch.Tensor]\n",
    "            labels: torch.Tensor\n",
    "            features = {k: v.to(device) for k, v in features.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            out = model(features)\n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            status_bar.set_postfix({\"loss\": total_loss / (status_bar.n + 1)})\n",
    "        \n",
    "        print(f\"Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        val_metrics = evaluate_model(\n",
    "            model=model,\n",
    "            val_ds=val_ds,\n",
    "            device=device,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['acc']:.4f}\")\n",
    "\n",
    "        if checkpoints_path is not None:\n",
    "            ckpt = pathlib.Path(checkpoints_path) / f\"epoch_{epoch:02}.pt\"\n",
    "            torch.save(model.state_dict(), ckpt)\n",
    "            print(f\"Checkpoint saved to {ckpt}\")\n",
    "            \n",
    "            if val_metrics[\"loss\"] < best_val_loss:\n",
    "                best_val_loss = val_metrics[\"loss\"]\n",
    "                best_model_file = ckpt\n",
    "\n",
    "    if return_best and best_model_file is not None:\n",
    "        print(f\"Best model: {best_model_file}\")\n",
    "        model.load_state_dict(torch.load(best_model_file))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: TweetClassifier,\n",
    "    val_ds: Dataset,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> tuple[float, float]:\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        status_bar = tqdm(val_loader, desc=\"Evaluating\")\n",
    "        \n",
    "        for features, labels in status_bar:\n",
    "            features: dict[str, torch.Tensor]\n",
    "            labels: torch.Tensor\n",
    "            features = {k: v.to(device) for k, v in features.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = model(features)\n",
    "            logits: torch.Tensor = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            count += labels.size(0)\n",
    "            \n",
    "            status_bar.set_postfix({\"loss\": total_loss / (status_bar.n + 1), \"acc\": correct / count})\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / len(val_loader),\n",
    "        \"acc\": correct / count,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "558a176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_model(\n",
    "    model: TweetClassifier,\n",
    "    feature_extractor: FeatureExtractor,\n",
    "    df: pd.DataFrame,\n",
    "    out_file: pathlib.Path,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> pd.Series:\n",
    "    \n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    # 1. Setup Data Loader with Lazy Extracted Features\n",
    "    # The features are extracted/loaded from cache here:\n",
    "    infer_features = feature_extractor.extract(df, 'infer')\n",
    "    infer_ds = TweetDataset(\n",
    "        infer_features, \n",
    "        pd.Series(torch.zeros(len(df), dtype=torch.long)), # Placeholder labels\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        infer_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = torch.zeros(len(df), dtype=torch.long)\n",
    "    offset = 0\n",
    "    \n",
    "    # 2. Run Model Inference\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(data_loader, desc=\"Inferring\"):\n",
    "            \n",
    "            out = model(features)\n",
    "            logits = out[\"logits\"]\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            \n",
    "            predictions[offset: offset + len(preds)] = preds.cpu()\n",
    "            offset += len(preds)\n",
    "            \n",
    "    # --- USER-LEVEL RECONCILIATION ---\n",
    "    \n",
    "    # Copy the input dataframe and attach the single-tweet predictions\n",
    "    df = df.copy()\n",
    "    df[\"pred_label\"] = pd.Series(predictions).astype(int)\n",
    "\n",
    "    # Reconciliation between same users\n",
    "    same_user_key = [\"user_created_at\", \"user_profile_image_url\"]\n",
    "    \n",
    "    # Step A: Count predicted labels (0 or 1) for each unique user key\n",
    "    per_user_stats: dict[tuple[str, str], list[int]] = dict()\n",
    "    for _, row in df.iterrows():\n",
    "        # .setdefault returns [count_label_0, count_label_1]\n",
    "        per_user_stats.setdefault(tuple(row[same_user_key].tolist()), [0, 0])[int(row[\"pred_label\"])] += 1\n",
    "    \n",
    "    # Step B: Determine the reconciled label for users with conflicting predictions\n",
    "    per_user_correct: dict[tuple[str, str], int] = dict()\n",
    "    for key, stats in per_user_stats.items():\n",
    "        # The original code only calculates the majority/tie-breaker if both labels were seen (conflict)\n",
    "        if stats[0] == 0 or stats[1] == 0:\n",
    "            continue # Skip users with unanimous predictions\n",
    "        \n",
    "        # Calculate majority vote (0 or 1), or randomly pick on a tie\n",
    "        per_user_correct[key] = np.select(\n",
    "            [stats[0] > stats[1], stats[1] > stats[0]],\n",
    "            [0, 1],\n",
    "            default=np.random.randint(0, 2),\n",
    "        )\n",
    "    \n",
    "    del per_user_stats\n",
    "    \n",
    "    # Step C: Apply the reconciled prediction back to the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        key = tuple(row[same_user_key].tolist())\n",
    "        if key in per_user_correct:\n",
    "            # Overwrite the prediction with the reconciled label\n",
    "            df.at[idx, \"pred_label\"] = per_user_correct[key]\n",
    "    \n",
    "    # 3. Save to Output File\n",
    "    if out_file is not None:\n",
    "        output = df[[\"challenge_id\", \"pred_label\"]]\n",
    "        output.columns = [\"ID\", \"Prediction\"]\n",
    "        output.to_csv(out_file, index=False)\n",
    "\n",
    "    return df[\"pred_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957dd8c",
   "metadata": {},
   "source": [
    "# Test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9ff6068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [ almanach/camembertav2-base ] =====\n",
      "\n",
      "Text encoder frozen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting metadata:  20%|██        | 10/49 [00:00<00:01, 33.27it/s]/tmp/ipykernel_287071/953507279.py:237: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return col.map(len, na_action=\"ignore\").fillna(0)\n",
      "Extracting metadata: 100%|██████████| 49/49 [00:05<00:00,  8.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached encodings for train...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2215/2215 [00:08<00:00, 247.03it/s, loss=0.469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 206/206 [00:00<00:00, 394.28it/s, loss=0.459, acc=0.785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4525, Acc: 0.7851\n",
      "Checkpoint saved to models/v12-vaughn/camembertav2-base/epoch_01.pt\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2215/2215 [00:08<00:00, 258.85it/s, loss=0.435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 206/206 [00:00<00:00, 409.71it/s, loss=0.503, acc=0.797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4301, Acc: 0.7970\n",
      "Checkpoint saved to models/v12-vaughn/camembertav2-base/epoch_02.pt\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2215/2215 [00:08<00:00, 260.91it/s, loss=0.419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 206/206 [00:00<00:00, 418.52it/s, loss=0.496, acc=0.799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4288, Acc: 0.7995\n",
      "Checkpoint saved to models/v12-vaughn/camembertav2-base/epoch_03.pt\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2215/2215 [00:08<00:00, 254.40it/s, loss=0.41] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 206/206 [00:00<00:00, 394.52it/s, loss=0.537, acc=0.799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4250, Acc: 0.7993\n",
      "Checkpoint saved to models/v12-vaughn/camembertav2-base/epoch_04.pt\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2215/2215 [00:08<00:00, 253.00it/s, loss=0.405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 206/206 [00:00<00:00, 408.45it/s, loss=0.55, acc=0.797] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4405, Acc: 0.7974\n",
      "Checkpoint saved to models/v12-vaughn/camembertav2-base/epoch_05.pt\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2215/2215 [00:09<00:00, 244.21it/s, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 206/206 [00:00<00:00, 414.55it/s, loss=0.52, acc=0.801] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4320, Acc: 0.8010\n",
      "Checkpoint saved to models/v12-vaughn/camembertav2-base/epoch_06.pt\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2215/2215 [00:08<00:00, 251.87it/s, loss=0.392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 206/206 [00:00<00:00, 399.54it/s, loss=0.525, acc=0.802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4230, Acc: 0.8024\n",
      "Checkpoint saved to models/v12-vaughn/camembertav2-base/epoch_07.pt\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2215/2215 [00:08<00:00, 257.87it/s, loss=0.386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 206/206 [00:00<00:00, 404.32it/s, loss=0.521, acc=0.807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4197, Acc: 0.8072\n",
      "Checkpoint saved to models/v12-vaughn/camembertav2-base/epoch_08.pt\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2215/2215 [00:08<00:00, 256.05it/s, loss=0.38] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 206/206 [00:00<00:00, 410.66it/s, loss=0.505, acc=0.81] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4167, Acc: 0.8101\n",
      "Checkpoint saved to models/v12-vaughn/camembertav2-base/epoch_09.pt\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2215/2215 [00:08<00:00, 255.59it/s, loss=0.373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 206/206 [00:00<00:00, 406.18it/s, loss=0.506, acc=0.804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4301, Acc: 0.8041\n",
      "Checkpoint saved to models/v12-vaughn/camembertav2-base/epoch_10.pt\n",
      "Best model: models/v12-vaughn/camembertav2-base/epoch_09.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting metadata:  14%|█▍        | 7/49 [00:00<00:00, 60.31it/s]/tmp/ipykernel_287071/953507279.py:237: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return col.map(len, na_action=\"ignore\").fillna(0)\n",
      "Extracting metadata: 100%|██████████| 49/49 [00:03<00:00, 13.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached encodings for infer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting metadata: 100%|██████████| 49/49 [00:03<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached encodings for infer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferring: 100%|██████████| 1616/1616 [00:02<00:00, 774.20it/s]\n"
     ]
    }
   ],
   "source": [
    "text_encoder_name = \"almanach/camembertav2-base\"\n",
    "print(f\"\\n===== [ {text_encoder_name} ] =====\\n\")\n",
    "\n",
    "model_folder = pathlib.Path(f\"./models/{VERSION}/\") / text_encoder_name.split(\"/\")[-1]\n",
    "model_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# 1. Initialize FeatureExtractor\n",
    "feature_extractor = FeatureExtractor(\n",
    "    text_encoder_name=text_encoder_name, \n",
    "    text_enc_cache_path=pathlib.Path(f\"./text_enc_cache\"), \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load metadata normalization stats (means/stds) if available\n",
    "f_ext_ckpt = model_folder / \"feature_extractor.ckpt\"\n",
    "if f_ext_ckpt.exists():\n",
    "    # Load state_dict, which includes means, stds, and afm_cache\n",
    "    feature_extractor.load_state_dict(torch.load(f_ext_ckpt, weights_only=False))\n",
    "\n",
    "# 2. Freeze the encoder\n",
    "feature_extractor.freeze_encoder()\n",
    "feature_extractor.train() # Set to train mode for proper metadata normalization (if not loaded)\n",
    "\n",
    "# 3. Extract/Load the full training set features (text and metadata)\n",
    "# 'train' split name ensures the cache is named 'train.ckpt'\n",
    "full_train_features = feature_extractor.extract(X_train, 'train')\n",
    "\n",
    "# Save metadata normalization stats (means/stds) after extraction/computation\n",
    "torch.save(feature_extractor.state_dict(), f_ext_ckpt)\n",
    "\n",
    "# 4. Create full dataset\n",
    "# Pass the pre-extracted features and the original labels\n",
    "full_train_ds = TweetDataset(full_train_features, y_train, device=device)\n",
    "\n",
    "# 5. Split into train and validation sets\n",
    "user_descs = pd.Series(X_train['user_description']).fillna('__MISSING__').factorize()[0]\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "train_idx, val_idx = next(splitter.split(X_train, y_train, groups=user_descs))\n",
    "\n",
    "train_ds = torch.utils.data.Subset(full_train_ds, train_idx)\n",
    "val_ds   = torch.utils.data.Subset(full_train_ds, val_idx)\n",
    "\n",
    "# Instantiate and train the classifier as before\n",
    "model = TweetClassifier(\n",
    "    feature_sizes=feature_extractor.dims(),\n",
    "    hidden_dim=512,\n",
    ").to(device)\n",
    "\n",
    "# Uncomment the following lines to run training/inference\n",
    "model = train_model(model, train_ds, val_ds, lr=2e-4, epochs=10, batch_size=64, device=device, checkpoints_path=model_folder, return_best=True)\n",
    "torch.save(model.state_dict(), model_folder / \"best_model.ckpt\")\n",
    "torch.cuda.empty_cache()\n",
    "# Inference requires extracting features for X_kaggle with split_name='infer'\n",
    "feature_extractor.eval()\n",
    "infer_features = feature_extractor.extract(X_kaggle, 'infer')\n",
    "infer_ds = TweetDataset(infer_features, pd.Series(torch.zeros(len(X_kaggle), dtype=torch.long)), device=device)\n",
    "infer_with_model(model, feature_extractor, X_kaggle, batch_size=64, device=device, out_file=model_folder / f\"predictions-{VERSION}.csv\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ad414f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_folder = pathlib.Path(f\"./models/{VERSION}/camembertav2-base/\")\n",
    "# feature_extractor = FeatureExtractor(text_encoder_name=\"almanach/camembertav2-base\", text_enc_cache_path=model_folder / \"text_enc_cache\", device=device)\n",
    "# feature_extractor.load_state_dict(torch.load(model_folder / \"feature_extractor.ckpt\", weights_only=False))\n",
    "# model = TweetClassifier(\n",
    "#     feature_sizes=feature_extractor.dims(),\n",
    "#     hidden_dim=512,\n",
    "# ).to(device)\n",
    "# model.load_state_dict(torch.load(model_folder / \"epoch_05.pt\"))\n",
    "# good_predictions = infer_with_model(model, feature_extractor, X_kaggle, batch_size=64, device=device, out_file=model_folder / \"predictions-v10-e09.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e9e31a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train           # None user IDs :    28273\n",
      "                  # valid user IDs:   126641\n",
      "                  # unique IDs    :    25461\n",
      "                  % with banner :   81.75%\n",
      "\n",
      "Kaggle_test       # None user IDs :    18986\n",
      "                  # valid user IDs:    84394\n",
      "                  # unique IDs    :    16954\n",
      "                  % with banner :   81.63%\n",
      "\n",
      "Overlap           user IDs        :        0\n",
      "\n",
      "X_train           # None descs :    24430\n",
      "                  # valid descs:   130484\n",
      "                  # unique descs    :    41234\n",
      "                  % with desc :   84.23%\n",
      "\n",
      "Kaggle_test       # None descs :    18986\n",
      "                  # valid descs:    84394\n",
      "                  # unique descs    :    16954\n",
      "                  % with desc :   81.63%\n",
      "\n",
      "X_train           # None user_created_at :        0\n",
      "                  # valid user_created_at:   154914\n",
      "                  % with user_created_at :  100.00%\n",
      "\n",
      "                  # unique user_created_at:    30696\n",
      "\n",
      "                  # duplicate user_created_at:   124218\n",
      "\n",
      "X_train           # no desc & no banner:    11955\n",
      "                  % no desc & no banner:    7.72%\n",
      "\n",
      "                  # unique user_created_at (no desc & no banner):     2685\n",
      "\n",
      "Label 1           # source contains 'TweetDeck':     2231\n",
      "                  % source contains 'TweetDeck':    3.09%\n",
      "\n",
      "Label 0           # source contains 'TweetDeck':      327\n",
      "                  % source contains 'TweetDeck':    0.40%\n",
      "\n",
      "TweetDeck Users   % with label 1:   87.22%\n",
      "\n",
      "TweetDeck Users   % of all users:    1.65%\n",
      "\n",
      "Source Name       # unique values:      407\n",
      "\n",
      "Source Name Values Unique Values and Counts:\n",
      "Source Name          | Total Count  | % with Label = 1\n",
      "-------------------------------------------------------\n",
      "'Twitter for iPhone' | 49536        |      53.7     %\n",
      "'Twitter Web App   ' | 45072        |      45.8     %\n",
      "'Twitter for Android' | 44903        |      32.2     %\n",
      "'Twitter for iPad  ' | 3520         |      29.1     %\n",
      "'TweetDeck         ' | 2558         |      87.2     %\n",
      "'Hootsuite Inc.    ' | 1352         |      91.0     %\n",
      "'dlvr.it           ' | 1011         |      90.1     %\n",
      "'IFTTT             ' | 748          |      69.8     %\n",
      "'WordPress.com     ' | 717          |      76.7     %\n",
      "'Buffer            ' | 451          |      87.6     %\n",
      "'Echobox           ' | 385          |     100.0     %\n",
      "'AgoraPulse Manager' | 243          |      92.2     %\n",
      "'Twitter Media Studio' | 222          |     100.0     %\n",
      "'Paper.li          ' | 212          |      36.8     %\n",
      "'Twitter for Mac   ' | 147          |      41.5     %\n",
      "'Tweetbot for iΟS  ' | 142          |      66.2     %\n",
      "'Sprout Social     ' | 107          |     100.0     %\n",
      "'Zapier.com        ' | 102          |      78.4     %\n",
      "'Nonli             ' | 90           |     100.0     %\n",
      "'Sociallymap       ' | 84           |      77.4     %\n",
      "'Swello            ' | 77           |     100.0     %\n",
      "'Blog2Social APP   ' | 74           |      74.3     %\n",
      "'The Tweeted Times ' | 73           |      58.9     %\n",
      "'Scoop.it          ' | 70           |      80.0     %\n",
      "'Wildmoka          ' | 67           |     100.0     %\n",
      "'Mashup Web        ' | 64           |      92.2     %\n",
      "'LinkedIn          ' | 62           |      32.3     %\n",
      "'OverBlog Kiwi     ' | 62           |      66.1     %\n",
      "'Twitter for Advertisers (legacy)' | 56           |      98.2     %\n",
      "'PimpMySocial Workshop' | 48           |     100.0     %\n",
      "'FS Poster         ' | 47           |      68.1     %\n",
      "'Revive Social App ' | 42           |      38.1     %\n",
      "'Limber.app        ' | 38           |      63.2     %\n",
      "'HubSpot           ' | 37           |      78.4     %\n",
      "'Tweetbot for Mac  ' | 35           |      80.0     %\n",
      "'La Manche Libre New App' | 34           |     100.0     %\n",
      "'Link account with KUKU.io' | 31           |     100.0     %\n",
      "'Instagram         ' | 29           |      44.8     %\n",
      "'SociabbleApp      ' | 28           |      0.0      %\n",
      "'Twitter for Advertisers' | 27           |     100.0     %\n",
      "'Twibble.io        ' | 26           |      76.9     %\n",
      "'Twidere for Android' | 25           |      16.0     %\n",
      "'Twitter pour iPhone ' | 23           |      0.0      %\n",
      "'TweetCaster for Android' | 21           |      47.6     %\n",
      "'Tumblr            ' | 21           |      52.4     %\n",
      "'Facelift-Cloud    ' | 21           |     100.0     %\n",
      "'Echofon           ' | 20           |      70.0     %\n",
      "'TW Blue           ' | 20           |      50.0     %\n",
      "'Radio King LiveTweet' | 19           |      52.6     %\n",
      "'Plume for Android ' | 18           |      77.8     %\n",
      "'Twitterrific for iOS' | 17           |      29.4     %\n",
      "'FeedPress Publisher' | 17           |     100.0     %\n",
      "'Falcon Social Media Management ' | 16           |     100.0     %\n",
      "'Fenix 2           ' | 16           |      31.2     %\n",
      "'Twitter for ZTE   ' | 15           |      0.0      %\n",
      "'Twitter Media Studio - LiveCut' | 15           |     100.0     %\n",
      "'SocialFlow        ' | 15           |     100.0     %\n",
      "'Mobile Web (M2)   ' | 15           |      0.0      %\n",
      "'Wordtwitt for ROI ' | 15           |     100.0     %\n",
      "'Mastodon-Twitter Crossposter' | 15           |      6.7      %\n",
      "'Salesforce - Social Studio' | 15           |     100.0     %\n",
      "'SEMrush Social Media Tool' | 14           |      64.3     %\n",
      "'Zoho Social       ' | 14           |     100.0     %\n",
      "'Sendible          ' | 13           |      76.9     %\n",
      "'Periscope         ' | 13           |      53.8     %\n",
      "'SocialPilot.co    ' | 13           |     100.0     %\n",
      "'Socialbakers      ' | 12           |     100.0     %\n",
      "'ContentStudio.io  ' | 12           |     100.0     %\n",
      "'Publer            ' | 11           |      18.2     %\n",
      "'Loomly            ' | 11           |     100.0     %\n",
      "'Fastwittr         ' | 10           |      50.0     %\n",
      "'Mailchimp         ' | 10           |      40.0     %\n",
      "'Twitter for Weenect' | 10           |      0.0      %\n",
      "'Linky for iOS     ' | 10           |      50.0     %\n",
      "'Oktopost          ' | 10           |      50.0     %\n",
      "'Radio.co now playing' | 10           |      0.0      %\n",
      "'EveryoneSocial    ' | 10           |      0.0      %\n",
      "'Hearsay Social    ' | 10           |      0.0      %\n",
      "'True Anthem       ' | 10           |     100.0     %\n",
      "'ContentCal Studio ' | 10           |     100.0     %\n",
      "'Crowdfire App     ' | 9            |      33.3     %\n",
      "'Mac4Ever CMS      ' | 9            |     100.0     %\n",
      "'Yuzz.it Pro       ' | 9            |     100.0     %\n",
      "'RC Engage Digital EU' | 9            |     100.0     %\n",
      "'CoSchedule        ' | 9            |     100.0     %\n",
      "'Sprinklr          ' | 8            |     100.0     %\n",
      "'Sud Info          ' | 8            |     100.0     %\n",
      "'Freshdesk         ' | 8            |     100.0     %\n",
      "'Twitterrific for Mac' | 8            |     100.0     %\n",
      "'CentralCharts     ' | 8            |      62.5     %\n",
      "'Microsoft Power Platform' | 8            |     100.0     %\n",
      "'MacroDroid App    ' | 7            |     100.0     %\n",
      "'Talon Android     ' | 7            |      28.6     %\n",
      "'Meltwater Social  ' | 7            |     100.0     %\n",
      "'Twitter Ads       ' | 7            |     100.0     %\n",
      "'Alcmeon Account Management' | 6            |     100.0     %\n",
      "'OBI4wan           ' | 6            |     100.0     %\n",
      "'Podcast - Radio Addict' | 6            |      0.0      %\n",
      "'Yuzzit Pro        ' | 6            |     100.0     %\n",
      "'Luffy Twitter     ' | 5            |      0.0      %\n",
      "'orthodoxie_com_app_auto_publish' | 5            |     100.0     %\n",
      "'BotNagew          ' | 5            |      0.0      %\n",
      "'Sorel-Tracy Express' | 5            |     100.0     %\n",
      "'Nouveau DiploMix  ' | 5            |     100.0     %\n",
      "'Auto Publish WP Twitter 2' | 5            |     100.0     %\n",
      "'publication bons plans' | 5            |     100.0     %\n",
      "'LaterMedia        ' | 5            |      80.0     %\n",
      "'Le Nouveau Monde  ' | 5            |     100.0     %\n",
      "'Miguel_le_slip    ' | 5            |      0.0      %\n",
      "'Dici - Radio      ' | 5            |     100.0     %\n",
      "'ActuSuisse        ' | 5            |     100.0     %\n",
      "'BuzzInsolite      ' | 5            |     100.0     %\n",
      "'Freie-Welt        ' | 5            |      0.0      %\n",
      "'ecnouvs           ' | 5            |      0.0      %\n",
      "'ChtiSebCC         ' | 5            |      0.0      %\n",
      "'WP to Twitter Pro ' | 5            |     100.0     %\n",
      "'TubeBuddy         ' | 5            |      0.0      %\n",
      "'20sEmy            ' | 5            |      0.0      %\n",
      "'Point du Lac Saint-Jean' | 5            |     100.0     %\n",
      "'Picta Presse      ' | 5            |     100.0     %\n",
      "'coronavirused     ' | 5            |     100.0     %\n",
      "'WeLoveTennis      ' | 5            |     100.0     %\n",
      "'Dynamic Signal    ' | 5            |      0.0      %\n",
      "'PulpNews          ' | 5            |     100.0     %\n",
      "'FxBookLTTG        ' | 5            |     100.0     %\n",
      "'connectionivoirienne' | 5            |     100.0     %\n",
      "'https://media27.info' | 5            |     100.0     %\n",
      "'AutoTweetBAQ      ' | 5            |     100.0     %\n",
      "'Alert Infos       ' | 5            |      0.0      %\n",
      "'le1               ' | 5            |     100.0     %\n",
      "'actualitesdudroit ' | 5            |     100.0     %\n",
      "'RTL info pour Tweeter' | 5            |     100.0     %\n",
      "'Marsad Tunisia    ' | 5            |     100.0     %\n",
      "'Article19ma       ' | 5            |     100.0     %\n",
      "'SocialAutoPosterMagCentre' | 5            |     100.0     %\n",
      "'MyBotMyzen2       ' | 5            |      0.0      %\n",
      "'COVID19-Updates   ' | 5            |     100.0     %\n",
      "'StockTwits Web    ' | 5            |     100.0     %\n",
      "'App pour FS Poster' | 5            |     100.0     %\n",
      "'Flamingo for Android' | 5            |      60.0     %\n",
      "'La Chronique auto tweet' | 5            |     100.0     %\n",
      "'Canva             ' | 5            |      60.0     %\n",
      "'ISTFecamp         ' | 5            |     100.0     %\n",
      "'France Covid      ' | 5            |     100.0     %\n",
      "'TanjiroTwitter    ' | 5            |      0.0      %\n",
      "'L'Echo de la Rive-Nord' | 5            |     100.0     %\n",
      "'Puremédias (publi articles)' | 5            |     100.0     %\n",
      "'Moutons Enragés   ' | 5            |     100.0     %\n",
      "'PJL_PPL_app       ' | 5            |     100.0     %\n",
      "'Africa News Hub   ' | 5            |     100.0     %\n",
      "'Ostadi            ' | 5            |      0.0      %\n",
      "'BlogLesCrises     ' | 5            |     100.0     %\n",
      "'avmtest           ' | 5            |     100.0     %\n",
      "'innovscovid19     ' | 5            |      0.0      %\n",
      "'SocialRabbit Plugin' | 5            |      0.0      %\n",
      "'SNK367            ' | 5            |      0.0      %\n",
      "'AmazingContent    ' | 5            |      0.0      %\n",
      "'Hellsing Twitter  ' | 5            |      0.0      %\n",
      "'Le Doutotron      ' | 5            |     100.0     %\n",
      "'Cheap Bots, Done Quick!' | 5            |     100.0     %\n",
      "'divergence_app    ' | 5            |     100.0     %\n",
      "'Figaro International Méthode App' | 5            |     100.0     %\n",
      "'Desinfos          ' | 5            |     100.0     %\n",
      "'covid19_bot_tx    ' | 5            |     100.0     %\n",
      "'Story Chief       ' | 5            |     100.0     %\n",
      "'Mdq Canada        ' | 5            |     100.0     %\n",
      "'feed-bountou      ' | 5            |     100.0     %\n",
      "'Raprnbmag         ' | 5            |     100.0     %\n",
      "'Nouvelles sur RIMQ' | 5            |     100.0     %\n",
      "'Planate School    ' | 5            |      0.0      %\n",
      "'Netvibes Widget   ' | 5            |     100.0     %\n",
      "'Interkinois       ' | 5            |      0.0      %\n",
      "'FavOLT-Tweets     ' | 5            |     100.0     %\n",
      "'FashionMag        ' | 5            |     100.0     %\n",
      "'Houssen Mohsinaly ' | 5            |     100.0     %\n",
      "'TopHashtags       ' | 5            |     100.0     %\n",
      "'Gamepush2         ' | 5            |     100.0     %\n",
      "'happyhomeinc      ' | 5            |     100.0     %\n",
      "'Le Journal de Joliette' | 5            |     100.0     %\n",
      "'Journal de l'agence' | 5            |     100.0     %\n",
      "'LesObservateurs   ' | 5            |     100.0     %\n",
      "'OxfordBlue-Twitter' | 5            |     100.0     %\n",
      "'soccerGames       ' | 5            |      0.0      %\n",
      "'fdesouche.com     ' | 5            |     100.0     %\n",
      "'Senejournal Tweets' | 5            |     100.0     %\n",
      "'covid_coulsim     ' | 5            |     100.0     %\n",
      "'Sprinklr Publishing' | 5            |     100.0     %\n",
      "'OLPlusApp         ' | 5            |     100.0     %\n",
      "'ZinfosTwitter     ' | 5            |     100.0     %\n",
      "'ThreadReaderApp   ' | 5            |     100.0     %\n",
      "'AutoTweetFCV      ' | 5            |     100.0     %\n",
      "'Real Madrid Hoy   ' | 5            |     100.0     %\n",
      "'Chambly Express   ' | 5            |     100.0     %\n",
      "'Neomedia-VS       ' | 5            |     100.0     %\n",
      "'SartheInfosBot 1.0' | 5            |     100.0     %\n",
      "'MoneyVox          ' | 5            |     100.0     %\n",
      "'NewsBKFrance      ' | 5            |     100.0     %\n",
      "'Devdiscourse News Desk' | 5            |     100.0     %\n",
      "'HealthBuzz Tweets ' | 5            |      0.0      %\n",
      "'RTS Multimédia MédiaBus (Sports)' | 5            |     100.0     %\n",
      "'Alertes SNCF Transilien' | 5            |     100.0     %\n",
      "'Tunis Tribune status updater' | 5            |     100.0     %\n",
      "'Bitly             ' | 5            |      0.0      %\n",
      "'Pepsnews          ' | 5            |     100.0     %\n",
      "'InfoDimanche.com  ' | 5            |     100.0     %\n",
      "'TVMag Méthode App ' | 5            |     100.0     %\n",
      "'TAG.FR PRODUCTION ' | 5            |     100.0     %\n",
      "'tryrequest        ' | 5            |      0.0      %\n",
      "'autotweet scheduler' | 5            |     100.0     %\n",
      "'Wordpress TW Macon News FR' | 5            |     100.0     %\n",
      "'potins.net        ' | 5            |     100.0     %\n",
      "'R analyses for twitter' | 5            |     100.0     %\n",
      "'Marseille Football' | 5            |     100.0     %\n",
      "'Next_Alert        ' | 5            |     100.0     %\n",
      "'game2_lazbt       ' | 5            |      0.0      %\n",
      "'BasketEurope      ' | 5            |     100.0     %\n",
      "'FranceNetInfos    ' | 5            |     100.0     %\n",
      "'L'important       ' | 5            |     100.0     %\n",
      "'orsonebooks       ' | 5            |     100.0     %\n",
      "'dzonline application' | 5            |     100.0     %\n",
      "'http://www.mysweetimmo.com' | 5            |     100.0     %\n",
      "'RTL sport         ' | 5            |     100.0     %\n",
      "'UN Talent         ' | 5            |     100.0     %\n",
      "'Afropages         ' | 5            |     100.0     %\n",
      "'MyHeadlinez       ' | 5            |     100.0     %\n",
      "'autourdebordeaux  ' | 5            |     100.0     %\n",
      "'AutoPublishPNCContact' | 5            |     100.0     %\n",
      "'Sunubuzz          ' | 5            |     100.0     %\n",
      "'MichMich75000     ' | 5            |      0.0      %\n",
      "'Purepeople (publi articles)' | 5            |     100.0     %\n",
      "'twittbot.net      ' | 5            |     100.0     %\n",
      "'IIIPRS            ' | 5            |     100.0     %\n",
      "'theinfomaker.com  ' | 5            |     100.0     %\n",
      "'Postify1          ' | 5            |     100.0     %\n",
      "'News Releases Auto Tweeter' | 5            |     100.0     %\n",
      "'Javascript Newss  ' | 5            |     100.0     %\n",
      "'Lelision.com      ' | 5            |      0.0      %\n",
      "'Pause Fun         ' | 5            |     100.0     %\n",
      "'emploisjob        ' | 5            |      0.0      %\n",
      "'Nessma TV         ' | 5            |     100.0     %\n",
      "'Seinemaritime76   ' | 5            |     100.0     %\n",
      "'Montceau News     ' | 5            |     100.0     %\n",
      "'Buzzagain         ' | 5            |     100.0     %\n",
      "'Info-Flash-Write-Local-Pages' | 5            |     100.0     %\n",
      "'FlashNewsBot      ' | 5            |     100.0     %\n",
      "'Basketusa WP      ' | 5            |     100.0     %\n",
      "'Covid 19 France   ' | 5            |     100.0     %\n",
      "'Agenparl          ' | 5            |     100.0     %\n",
      "'quarantineOpportunity' | 5            |     100.0     %\n",
      "'Teledakar-officiel' | 5            |     100.0     %\n",
      "'KultureGeek.fr    ' | 5            |     100.0     %\n",
      "'Bot Vichy         ' | 5            |     100.0     %\n",
      "'KoolSaina.com     ' | 5            |     100.0     %\n",
      "'Telebruxelles_web ' | 5            |     100.0     %\n",
      "'Boursier.com      ' | 5            |     100.0     %\n",
      "'app-twitter-novelle' | 5            |     100.0     %\n",
      "'AkroRadio         ' | 5            |      0.0      %\n",
      "'Neutron Jimm      ' | 5            |      0.0      %\n",
      "'CFL_RssToTwitter  ' | 5            |     100.0     %\n",
      "'Dici TV           ' | 5            |     100.0     %\n",
      "'Accelerate Twitter Demo1' | 5            |      0.0      %\n",
      "'FFr Tweet this    ' | 5            |      0.0      %\n",
      "'Malivox.net       ' | 5            |     100.0     %\n",
      "'WhiteBeard CMS    ' | 5            |     100.0     %\n",
      "'The New Cameroon  ' | 5            |      0.0      %\n",
      "'newsnet-app       ' | 5            |     100.0     %\n",
      "'COVID Articles on SP Journals' | 5            |     100.0     %\n",
      "'Application @morandiniblog' | 5            |     100.0     %\n",
      "'Alertes RERB      ' | 5            |     100.0     %\n",
      "'T7.com            ' | 5            |     100.0     %\n",
      "'L'Observateur V5  ' | 5            |     100.0     %\n",
      "'La Commère 43     ' | 5            |     100.0     %\n",
      "'Walfoot           ' | 5            |     100.0     %\n",
      "'Cawbird           ' | 5            |      0.0      %\n",
      "'Titi Tweet        ' | 5            |      0.0      %\n",
      "'Poteaux carrés    ' | 5            |     100.0     %\n",
      "'LOTTA INTERNATIONALE' | 5            |     100.0     %\n",
      "' Accelerate Twitter Demo 4' | 5            |      0.0      %\n",
      "'Schtroumpsifier   ' | 5            |     100.0     %\n",
      "'URLs_fr           ' | 5            |     100.0     %\n",
      "'twitterpsfr       ' | 5            |     100.0     %\n",
      "'overall-twitter   ' | 5            |     100.0     %\n",
      "'TOPGYN CAPA       ' | 5            |      0.0      %\n",
      "'EnBeauce.com REST API v 1.1' | 5            |     100.0     %\n",
      "'Figaro Economie Méthode App' | 5            |     100.0     %\n",
      "'TwinyBots         ' | 5            |      0.0      %\n",
      "'Eauto Check       ' | 5            |     100.0     %\n",
      "'journal tunisie   ' | 5            |     100.0     %\n",
      "'Geek-tech         ' | 5            |     100.0     %\n",
      "'TweetPony         ' | 5            |      0.0      %\n",
      "'Atlas Magazine    ' | 5            |     100.0     %\n",
      "'Hospimedia        ' | 5            |     100.0     %\n",
      "'site reve86       ' | 5            |     100.0     %\n",
      "'LuckyTheo64       ' | 5            |      0.0      %\n",
      "'Qui a TT?         ' | 5            |     100.0     %\n",
      "'ahlam ahlam       ' | 5            |      0.0      %\n",
      "'yoyoyo_v3         ' | 5            |      0.0      %\n",
      "'Fenix for Android ' | 4            |     100.0     %\n",
      "'Stats Covid France' | 4            |     100.0     %\n",
      "'Basket Infos Twitter' | 4            |     100.0     %\n",
      "'Docteur imago     ' | 4            |     100.0     %\n",
      "'TweetGram.me      ' | 4            |     100.0     %\n",
      "'OLPlusFem         ' | 4            |     100.0     %\n",
      "'Scopalto          ' | 4            |     100.0     %\n",
      "'fr.allAfrica.com  ' | 4            |     100.0     %\n",
      "'AujourduiPress    ' | 4            |      0.0      %\n",
      "'jimwebfeed        ' | 4            |     100.0     %\n",
      "'MI Connect        ' | 4            |     100.0     %\n",
      "'94 Citoyens       ' | 4            |     100.0     %\n",
      "'Integromat        ' | 4            |     100.0     %\n",
      "'wiw Social Streams' | 4            |     100.0     %\n",
      "'TwidereX-Android  ' | 4            |      0.0      %\n",
      "'OS X              ' | 4            |      0.0      %\n",
      "'Figaro Vox Media App' | 4            |     100.0     %\n",
      "'recurpost.com     ' | 4            |     100.0     %\n",
      "'Melody HCR        ' | 4            |     100.0     %\n",
      "'Melody_CorseMatin ' | 4            |     100.0     %\n",
      "'twitter lba       ' | 4            |     100.0     %\n",
      "'BourseTrading     ' | 4            |     100.0     %\n",
      "'lacryptomonnaie   ' | 4            |     100.0     %\n",
      "'Revive pour duperrin.com' | 4            |     100.0     %\n",
      "'Neatly For BlackBerry 10' | 4            |     100.0     %\n",
      "'vivafrik          ' | 4            |     100.0     %\n",
      "'AlwihdApp         ' | 3            |     100.0     %\n",
      "'TF Tweets Auto    ' | 3            |     100.0     %\n",
      "'Instapaper        ' | 3            |      0.0      %\n",
      "'Gazette du Palais ' | 3            |     100.0     %\n",
      "'wtbass            ' | 3            |     100.0     %\n",
      "'Djib_s            ' | 3            |     100.0     %\n",
      "'Sharee Advocacy   ' | 3            |     100.0     %\n",
      "'thread.center     ' | 3            |      0.0      %\n",
      "'Carte de circulation interactive' | 3            |     100.0     %\n",
      "'Clarabridge Engage' | 3            |     100.0     %\n",
      "'eClincher         ' | 3            |     100.0     %\n",
      "'iOS               ' | 3            |     100.0     %\n",
      "'Iconosquare.com   ' | 3            |     100.0     %\n",
      "'tweetinfluence123 ' | 3            |      0.0      %\n",
      "'senedirect app    ' | 3            |     100.0     %\n",
      "'Blogs Ouest France' | 3            |     100.0     %\n",
      "'Odoo Social       ' | 3            |      0.0      %\n",
      "'Mafrique          ' | 3            |     100.0     %\n",
      "'Post Planner Inc. ' | 3            |     100.0     %\n",
      "'Figaro Etudiant Media App' | 3            |     100.0     %\n",
      "'TwitPane for Android' | 3            |      0.0      %\n",
      "'PreProd app senti ' | 2            |      0.0      %\n",
      "'Planable          ' | 2            |     100.0     %\n",
      "'Amelya Bot        ' | 2            |     100.0     %\n",
      "'MTV English News  ' | 2            |     100.0     %\n",
      "'Zlappo.com        ' | 2            |     100.0     %\n",
      "'Atlantico         ' | 2            |     100.0     %\n",
      "'Twitter Web Client' | 2            |      0.0      %\n",
      "'Khoros            ' | 2            |     100.0     %\n",
      "'MaLigue2 Auto publication' | 2            |     100.0     %\n",
      "'RT News Sharing   ' | 2            |      0.0      %\n",
      "'LCI Twitter Kit   ' | 2            |     100.0     %\n",
      "'TwitPanePlus      ' | 2            |      0.0      %\n",
      "'blogSpirit        ' | 2            |     100.0     %\n",
      "'SocialHub by maloon' | 2            |     100.0     %\n",
      "'http://www.gerontonews.com' | 2            |     100.0     %\n",
      "'Twitter pour le Web ' | 2            |      0.0      %\n",
      "'Zonebourse App    ' | 2            |      0.0      %\n",
      "'TestLimitDM       ' | 2            |     100.0     %\n",
      "'theglobe          ' | 2            |     100.0     %\n",
      "'Nintendo Switch Share' | 2            |     100.0     %\n",
      "'Schweizer Parlament ParlCH' | 2            |     100.0     %\n",
      "'UberSocial© PRO   ' | 2            |     100.0     %\n",
      "'ReviveOldPost2    ' | 2            |     100.0     %\n",
      "'Diaspora* at Framasphere' | 2            |     100.0     %\n",
      "'rtweet_token_sc   ' | 2            |     100.0     %\n",
      "'KultureGeek | Djibs' | 2            |     100.0     %\n",
      "'Nuzzel            ' | 2            |     100.0     %\n",
      "'Sud éducation 75  ' | 1            |     100.0     %\n",
      "'Publer.io         ' | 1            |      0.0      %\n",
      "'Twidere X Android ' | 1            |      0.0      %\n",
      "'TIC Mag           ' | 1            |     100.0     %\n",
      "'MyTwt_mycron      ' | 1            |     100.0     %\n",
      "'Les-Crises.fr0    ' | 1            |     100.0     %\n",
      "'Polyalert-Live    ' | 1            |     100.0     %\n",
      "'Twitlonger        ' | 1            |     100.0     %\n",
      "'Salon Beige       ' | 1            |     100.0     %\n",
      "'The Tweeted Times Mobile' | 1            |     100.0     %\n",
      "'Aviary (for iOS and macOS)' | 1            |     100.0     %\n",
      "'CanadaLIve        ' | 1            |      0.0      %\n",
      "'Alerte_Huit_lemonde' | 1            |     100.0     %\n",
      "'BirdSite          ' | 1            |     100.0     %\n",
      "'Vimeo             ' | 1            |     100.0     %\n",
      "'Reddit Official   ' | 1            |      0.0      %\n",
      "'Khoros Publishing ' | 1            |     100.0     %\n",
      "'Postoplan         ' | 1            |      0.0      %\n",
      "'PlayStation®Network' | 1            |     100.0     %\n",
      "'dirico            ' | 1            |     100.0     %\n",
      "'Future CAC40      ' | 1            |     100.0     %\n",
      "'WP Auto T         ' | 1            |     100.0     %\n",
      "'Skyrock           ' | 1            |      0.0      %\n",
      "'Tweepsmap         ' | 1            |     100.0     %\n",
      "'post auto heidi news' | 1            |     100.0     %\n",
      "'TelphoneFR        ' | 1            |      0.0      %\n",
      "'HashCut           ' | 1            |     100.0     %\n",
      "'Marocafrik.com    ' | 1            |     100.0     %\n",
      "'Auto publication quotidienne ' | 1            |     100.0     %\n",
      "'Streamlabs Twitter' | 1            |     100.0     %\n",
      "'Hypefury          ' | 1            |     100.0     %\n",
      "'chirr.app         ' | 1            |      0.0      %\n",
      "'redouad_twitter   ' | 1            |     100.0     %\n",
      "'Info241_tweet     ' | 1            |     100.0     %\n",
      "'Alerte Nouvelles  ' | 1            |     100.0     %\n",
      "'FCM Live tweet    ' | 1            |     100.0     %\n"
     ]
    }
   ],
   "source": [
    "# Extract user_ids\n",
    "def get_user_id(url):\n",
    "    return url.split(\"/\")[4] if pd.notna(url) else None\n",
    "\n",
    "# Training set\n",
    "train_ids = X_train[\"user_profile_banner_url\"].map(get_user_id)\n",
    "print(f\"{'X_train':<17} # None user IDs : {train_ids.isna().sum():>8}\")\n",
    "print(f\"{'':<17} # valid user IDs: {train_ids.notna().sum():>8}\")\n",
    "print(f\"{'':<17} # unique IDs    : {train_ids.nunique():>8}\")\n",
    "# Print percent of users that have a banner\n",
    "percent_with_banner = (train_ids.notna().sum() / len(train_ids)) * 100\n",
    "print(f\"{'':<17} % with banner : {percent_with_banner:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# Kaggle test set\n",
    "kaggle_ids = kaggle_data[\"user.profile_banner_url\"].map(get_user_id)\n",
    "print(f\"{'Kaggle_test':<17} # None user IDs : {kaggle_ids.isna().sum():>8}\")\n",
    "print(f\"{'':<17} # valid user IDs: {kaggle_ids.notna().sum():>8}\")\n",
    "print(f\"{'':<17} # unique IDs    : {kaggle_ids.nunique():>8}\")\n",
    "# Print percent of users that have a banner\n",
    "percent_with_banner = (kaggle_ids.notna().sum() / len(kaggle_ids)) * 100\n",
    "print(f\"{'':<17} % with banner : {percent_with_banner:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# Overlap\n",
    "train_set = set(train_ids.dropna())\n",
    "kaggle_set = set(kaggle_ids.dropna())\n",
    "overlap = train_set.intersection(kaggle_set)\n",
    "print(f\"{'Overlap':<17} user IDs        : {len(overlap):>8}\")\n",
    "print()\n",
    "\n",
    "# Training set\n",
    "train_ids = X_train[\"user_description\"]\n",
    "print(f\"{'X_train':<17} # None descs : {train_ids.isna().sum():>8}\")\n",
    "print(f\"{'':<17} # valid descs: {train_ids.notna().sum():>8}\")\n",
    "print(f\"{'':<17} # unique descs    : {train_ids.nunique():>8}\")\n",
    "# Print percent of users that have a description\n",
    "percent_with_desc = (train_ids.notna().sum() / len(train_ids)) * 100\n",
    "print(f\"{'':<17} % with desc : {percent_with_desc:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# Kaggle test set\n",
    "kaggle_ids = kaggle_data[\"user.profile_banner_url\"].map(get_user_id)\n",
    "print(f\"{'Kaggle_test':<17} # None descs : {kaggle_ids.isna().sum():>8}\")\n",
    "print(f\"{'':<17} # valid descs: {kaggle_ids.notna().sum():>8}\")\n",
    "print(f\"{'':<17} # unique descs    : {kaggle_ids.nunique():>8}\")\n",
    "# Print percent of users that have a description\n",
    "percent_with_desc = (kaggle_ids.notna().sum() / len(kaggle_ids)) * 100\n",
    "print(f\"{'':<17} % with desc : {percent_with_desc:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# What percent of users have a user_created_at value?\n",
    "\n",
    "train_user_created_at = X_train[\"user_created_at\"]\n",
    "print(f\"{'X_train':<17} # None user_created_at : {train_user_created_at.isna().sum():>8}\")\n",
    "print(f\"{'':<17} # valid user_created_at: {train_user_created_at.notna().sum():>8}\")\n",
    "percent_with_user_created_at = (train_user_created_at.notna().sum() / len(train_user_created_at)) * 100\n",
    "print(f\"{'':<17} % with user_created_at : {percent_with_user_created_at:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# How many user_created_at are unique values?\n",
    "print(f\"{'':<17} # unique user_created_at: {train_user_created_at.nunique():>8}\")\n",
    "print()\n",
    "\n",
    "# How many user_created_at are not unique values?\n",
    "duplicate_user_created_at = len(train_user_created_at) - train_user_created_at.nunique()\n",
    "print(f\"{'':<17} # duplicate user_created_at: {duplicate_user_created_at:>8}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# What percent of tweets have no description and no banner?\n",
    "no_desc_and_no_banner = X_train[\"user_description\"].isna() & X_train[\"user_profile_banner_url\"].isna()\n",
    "count_no_desc_and_no_banner = no_desc_and_no_banner.sum()\n",
    "percent_no_desc_and_no_banner = (count_no_desc_and_no_banner / len(X_train)) * 100\n",
    "print(f\"{'X_train':<17} # no desc & no banner: {count_no_desc_and_no_banner:>8}\")\n",
    "print(f\"{'':<17} % no desc & no banner: {percent_no_desc_and_no_banner:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# Among users that have no description and no banner, how many unique user_created_at values are there?\n",
    "unique_user_created_at_no_desc_and_no_banner = X_train.loc[no_desc_and_no_banner, \"user_created_at\"].nunique()\n",
    "print(f\"{'':<17} # unique user_created_at (no desc & no banner): {unique_user_created_at_no_desc_and_no_banner:>8}\")\n",
    "print()\n",
    "\n",
    "# For users whose label is 1, what percent have a \"source\" field containing \"TweetDeck\"?\n",
    "label_1_source_tweetdeck = X_train.loc[y_train == 1, \"source\"].str.contains(\"TweetDeck\", na=False)\n",
    "count_label_1_source_tweetdeck = label_1_source_tweetdeck.sum()\n",
    "total_label_1 = (y_train == 1).sum()\n",
    "percent_label_1_source_tweetdeck = (count_label_1_source_tweetdeck / total_label_1) * 100\n",
    "print(f\"{'Label 1':<17} # source contains 'TweetDeck': {count_label_1_source_tweetdeck:>8}\")\n",
    "print(f\"{'':<17} % source contains 'TweetDeck': {percent_label_1_source_tweetdeck:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# For users whose label is 0, what percent have a \"source\" field containing \"TweetDeck\"?\n",
    "label_0_source_tweetdeck = X_train.loc[y_train == 0, \"source\"].str.contains(\"TweetDeck\", na=False)\n",
    "count_label_0_source_tweetdeck = label_0_source_tweetdeck.sum()\n",
    "total_label_0 = (y_train == 0).sum()\n",
    "percent_label_0_source_tweetdeck = (count_label_0_source_tweetdeck / total_label_0) * 100\n",
    "print(f\"{'Label 0':<17} # source contains 'TweetDeck': {count_label_0_source_tweetdeck:>8}\")\n",
    "print(f\"{'':<17} % source contains 'TweetDeck': {percent_label_0_source_tweetdeck:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# For uses who have \"TweetDeck\" in their source, what percent have label 1?\n",
    "total_tweetdeck = label_0_source_tweetdeck.sum() + label_1_source_tweetdeck.sum()\n",
    "percent_tweetdeck_label_1 = (count_label_1_source_tweetdeck / total_tweetdeck) * 100\n",
    "print(f\"{'TweetDeck Users':<17} % with label 1: {percent_tweetdeck_label_1:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# What percent of users use TweetDeck as their source?\n",
    "total_tweetdeck = label_0_source_tweetdeck.sum() + label_1_source_tweetdeck.sum()\n",
    "percent_tweetdeck_users = (total_tweetdeck / len(X_train)) * 100\n",
    "print(f\"{'TweetDeck Users':<17} % of all users: {percent_tweetdeck_users:>7.2f}%\")\n",
    "print()\n",
    "\n",
    "# How many unique values are there in \"source_name\" field?\n",
    "source_name_unique = X_train[\"source_name\"].nunique()\n",
    "print(f\"{'Source Name':<17} # unique values: {source_name_unique:>8}\")\n",
    "print()\n",
    "\n",
    "# List all unique values of \"source_name\" field along with their counts, sorted by count descending\n",
    "source_name_counts = X_train[\"source_name\"].value_counts()\n",
    "print(f\"{'Source Name Values':<17} Unique Values and Counts:\")\n",
    "\n",
    "# 1. Combine the source names and labels into a single DataFrame\n",
    "# Ensure X_train and y_train have the same index for correct alignment\n",
    "df_combined = pd.DataFrame({\n",
    "    \"source_name\": X_train[\"source_name\"],\n",
    "    \"label\": y_train\n",
    "})\n",
    "\n",
    "# 2. Calculate the total count for each source_name\n",
    "source_name_counts = df_combined[\"source_name\"].value_counts()\n",
    "\n",
    "# 3. Calculate the count where 'label' == 1 for each source_name\n",
    "# Group by \"source_name\" and sum the \"label\" (summing 1s gives the count of 1s)\n",
    "label_1_counts = df_combined.groupby(\"source_name\")[\"label\"].sum()\n",
    "\n",
    "# 4. Calculate the percentage of label == 1 for each source_name\n",
    "label_1_percentages = ((label_1_counts / source_name_counts) * 100).round(1)\n",
    "\n",
    "# 5. Print the results, sorted by total count descending\n",
    "print(f\"{'Source Name':<20} | {'Total Count':<12} | % with Label = 1\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for source_name, total_count in source_name_counts.items():\n",
    "    # Retrieve the calculated percentage for the current source_name\n",
    "    percentage = label_1_percentages.get(source_name, 0.0)\n",
    "    print(f\"'{source_name:<18}' | {total_count:<12} | {percentage:^14.1f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e500dab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

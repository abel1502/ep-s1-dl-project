{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb900036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import typing\n",
    "import json\n",
    "import pathlib\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import transformers\n",
    "import transformers.modeling_outputs\n",
    "import transformers.configuration_utils\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f873bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_KAGGLE = \"KAGGLE_DOCKER_IMAGE\" in os.environ\n",
    "\n",
    "DATASETS = pathlib.Path(\n",
    "    \".\"\n",
    "    if not IS_KAGGLE\n",
    "    else \"/kaggle/input/influencers-or-observers-predicting-social-roles/Kaggle2025\"\n",
    ")\n",
    "\n",
    "DATASET_TRAIN = DATASETS / \"train.jsonl\"\n",
    "DATASET_KAGGLE = DATASETS / \"kaggle_test.jsonl\"\n",
    "\n",
    "CACHE_DIR = pathlib.Path(\".\")\n",
    "VERSION = \"v11-vaughn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7fc921d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e20b6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f28f61",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e16f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: pathlib.Path, cache: bool = False) -> pd.DataFrame:\n",
    "    path_pq = (CACHE_DIR / path.name).with_stem(f\"{path.stem}_raw\").with_suffix(\".parquet\")\n",
    "    \n",
    "    if cache and path_pq.exists():\n",
    "        return pd.read_parquet(path_pq)\n",
    "    \n",
    "    # This leaves things to be desired, since there's no way to specify dtypes\n",
    "    # and it assumes float instead of int, causing a loss in precision...\n",
    "    # But I guess it only matters for ids, which we'll probably discard in preprocessing anyway\n",
    "    result = pd.json_normalize(list(map(json.loads, path.read_bytes().splitlines())))\n",
    "    \n",
    "    if cache:\n",
    "        result.to_parquet(path_pq)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80d1df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_json(DATASET_TRAIN, cache=True)\n",
    "kaggle_data = load_json(DATASET_KAGGLE, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bcf662",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ea7df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # For technical reasons, any text columns we want to use should have no dots in their names.\n",
    "    # The simplest way to achieve this is to replace all dots indiscriminately.\n",
    "    \n",
    "    df = df.rename(columns=lambda x: x.replace(\".\", \"_\"))\n",
    "    \n",
    "    df[\"is_reply\"] = df[\"in_reply_to_status_id\"].notna()\n",
    "    \n",
    "    df = df.drop(columns=[\n",
    "        \"in_reply_to_status_id_str\",\n",
    "        # \"in_reply_to_status_id\",\n",
    "        \"in_reply_to_user_id_str\",\n",
    "        \"in_reply_to_user_id\",\n",
    "        \"quoted_status_id_str\",\n",
    "        \"quoted_status_id\",\n",
    "        \"id_str\",\n",
    "        \"quoted_status_in_reply_to_status_id_str\",\n",
    "        \"quoted_status_in_reply_to_status_id\",\n",
    "        \"quoted_status_in_reply_to_user_id_str\",\n",
    "        \"quoted_status_in_reply_to_user_id\",\n",
    "        \"quoted_status_id_str\",\n",
    "        \"quoted_status_id\",\n",
    "        \"quoted_status_user_id_str\",\n",
    "        \"quoted_status_user_id\",\n",
    "        # \"quoted_status_permalink_expanded\",\n",
    "        \"quoted_status_permalink_display\",\n",
    "        \"quoted_status_permalink_url\",\n",
    "        \"quoted_status_quoted_status_id\",\n",
    "        \"quoted_status_quoted_status_id_str\",\n",
    "        # \"quoted_status_place_id\",\n",
    "        # \"place_id\",\n",
    "        \"lang\",  # Always \"fr\"\n",
    "        \"retweeted\",  # Always False\n",
    "        \"filter_level\",  # Always \"low\"\n",
    "        \"geo\",  # Always None\n",
    "        \"place\",  # Always None\n",
    "        \"coordinates\",  # Always None\n",
    "        \"contributors\",  # Always None\n",
    "        \"quote_count\",  # Always 0\n",
    "        \"reply_count\",  # Always 0\n",
    "        \"retweet_count\",  # Always 0\n",
    "        \"favorite_count\",  # Always 0\n",
    "        \"favorited\",  # Always False\n",
    "        \"quoted_status_geo\",  # Always None\n",
    "        \"quoted_status_place\",  # Always None\n",
    "        \"quoted_status_coordinates\",  # Always None\n",
    "        \"quoted_status_retweeted\",  # Always False\n",
    "        \"quoted_status_filter_level\",  # Always \"low\"\n",
    "        \"quoted_status_contributors\",  # Always None\n",
    "        \"quoted_status_user_utc_offset\",  # Always None\n",
    "        \"quoted_status_user_lang\",  # Always None\n",
    "        \"quoted_status_user_time_zone\",  # Always None\n",
    "        \"quoted_status_user_follow_request_sent\",  # Always None\n",
    "        \"quoted_status_user_following\",  # Always None\n",
    "        \"quoted_status_user_notifications\",  # Always None\n",
    "        \"user_default_profile_image\",  # Always False\n",
    "        \"user_protected\",  # Always False\n",
    "        \"user_contributors_enabled\",  # Always False\n",
    "        \"user_lang\",  # Always None\n",
    "        \"user_notifications\",  # Always None\n",
    "        \"user_following\",  # Always None\n",
    "        \"user_utc_offset\",  # Always None\n",
    "        \"user_time_zone\",  # Always None\n",
    "        \"user_follow_request_sent\",  # Always None\n",
    "    ])\n",
    "    \n",
    "    df[\"full_text\"] = df.apply(lambda tweet: extract_full_text(tweet), axis=1)\n",
    "    \n",
    "    source_split = df[\"source\"].str.removeprefix(\"<a href=\\\"\").str.removesuffix(\"</a>\").str.split(\"\\\" rel=\\\"nofollow\\\">\").map(lambda x: x if len(x) == 2 else pd.NA)\n",
    "    df[\"source_url\"] = source_split.map(lambda x: x[0], na_action=\"ignore\")\n",
    "    df[\"source_name\"] = source_split.map(lambda x: x[1], na_action=\"ignore\")\n",
    "    \n",
    "    df[\"misc_text\"] = df.apply(\n",
    "        lambda x: \"via: {0}; reply: @{1}; quote: @{2} {3}\".format(x[\"source_name\"], x[\"in_reply_to_screen_name\"], x[\"quoted_status_user_screen_name\"], x[\"quoted_status_user_name\"]), axis=1,\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_full_text(tweet: pd.Series) -> str:\n",
    "    text: str = tweet[\"text\"]\n",
    "    \n",
    "    if not pd.isna(tweet[\"extended_tweet_full_text\"]):\n",
    "        text = tweet[\"extended_tweet_full_text\"]\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab1ee966",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(\"label\", axis=1)\n",
    "y_train = train_data[\"label\"]\n",
    "\n",
    "X_kaggle = kaggle_data\n",
    "\n",
    "X_train = preprocess(X_train)\n",
    "X_kaggle = preprocess(X_kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c8d7e",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda51842",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5be1585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Made this a class to hold all the caches. It may resemble an nn.Module, but isn't one!\n",
    "class FeatureExtractor:\n",
    "    text_encoder_name: str | None\n",
    "    text_tokenizer: nn.Module | None\n",
    "    text_encoder: nn.Module | None\n",
    "    text_config: transformers.configuration_utils.PretrainedConfig | None\n",
    "    text_enc_cache_path: pathlib.Path | None\n",
    "    \n",
    "    # New attribute to hold pre-computed embeddings\n",
    "    text_encodings: dict[str, dict[str, torch.Tensor]]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        text_encoder_name: str | None = None,\n",
    "        text_enc_cache_path: pathlib.Path | None = None,\n",
    "        device: torch.device = device,\n",
    "    ):\n",
    "        # super().__init__() # Removed this line as FeatureExtractor is not an nn.Module\n",
    "        self.device = device\n",
    "        self.means = None\n",
    "        self.stds = None\n",
    "        self.afm_cache = {}\n",
    "        self.text_enc_cache_path = text_enc_cache_path\n",
    "        \n",
    "        self.text_encoder_name = text_encoder_name\n",
    "        self.text_tokenizer = None\n",
    "        self.text_encoder = None\n",
    "        self.text_config = None\n",
    "        self.text_encodings = {\"train\": {}, \"infer\": {}} # Initialize dict for train/infer cache\n",
    "        \n",
    "        # Load Text Encoder/Tokenizer only if text_encoder_name is provided\n",
    "        if text_encoder_name is not None:\n",
    "            self.text_tokenizer = AutoTokenizer.from_pretrained(text_encoder_name)\n",
    "            if hasattr(self.text_tokenizer, \"to\"):\n",
    "                self.text_tokenizer = self.text_tokenizer.to(self.device)\n",
    "            self.text_encoder = AutoModel.from_pretrained(text_encoder_name).to(self.device)\n",
    "            self.text_config = self.text_encoder.config\n",
    "            \n",
    "        self.train() # Default to training mode\n",
    "    \n",
    "    def freeze_encoder(self):\n",
    "        if self.text_encoder is not None:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.text_encoder.eval()\n",
    "            print(\"Text encoder frozen.\")\n",
    "\n",
    "    def unfreeze_encoder(self):\n",
    "        if self.text_encoder is not None:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"Text encoder unfrozen.\")\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        if self.text_encoder is not None:\n",
    "            self.text_encoder.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        if self.text_encoder is not None:\n",
    "            self.text_encoder.eval()\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            \"means\": self.means,\n",
    "            \"stds\": self.stds,\n",
    "            \"afm_cache\": self.afm_cache,\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.means = state_dict[\"means\"]\n",
    "        self.stds = state_dict[\"stds\"]\n",
    "        self.afm_cache = state_dict[\"afm_cache\"]\n",
    "    \n",
    "    def dims(self) -> dict[str, int]:\n",
    "        return {\n",
    "            \"md\": len(self.METADATA_FIELDS),\n",
    "        } | {\n",
    "            field: compress or self.embed_size\n",
    "            for field, compress in self.TEXT_FIELDS\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def embed_size(self) -> int:\n",
    "        return self.text_config.hidden_size\n",
    "    \n",
    "    def extract(self, df: pd.DataFrame, split_name: str, override_cache: bool = False) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extracts features, loading or computing text embeddings from cache.\n",
    "        split_name should be 'train' or 'infer'\n",
    "        \"\"\"\n",
    "        result: dict[str, torch.Tensor] = {}\n",
    "        cache_key = split_name\n",
    "        \n",
    "        # 1. Metadata extraction (always computed)\n",
    "        result[\"md\"] = self.extract_raw_metadata(df)\n",
    "        \n",
    "        # 2. Text embedding extraction (cached)\n",
    "        cf = self.text_enc_cache_path / f\"{cache_key}.ckpt\"\n",
    "        cf.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if not override_cache and cf.exists():\n",
    "            print(f\"Loading cached encodings for {split_name}...\")\n",
    "            # Load embeddings into the internal dictionary first\n",
    "            self.text_encodings[split_name] = torch.load(cf)\n",
    "            \n",
    "            # Transfer to the result dictionary\n",
    "            for col_name, value in self.text_encodings[split_name].items():\n",
    "                result[col_name] = value.to(self.device)\n",
    "                \n",
    "            # Perform PCA/padding only on loaded tensors if needed\n",
    "            for col_name, compress in self.TEXT_FIELDS:\n",
    "                if col_name in result:\n",
    "                     if compress is not None and compress != result[col_name].shape[1]:\n",
    "                         # This part is complex if PCA was applied, best to ensure PCA is part of the initial encoding if cached.\n",
    "                         # Since the original notebook applied PCA *after* encoding but *before* caching, \n",
    "                         # we assume the cached tensor is the final (potentially PCA'd/padded) result.\n",
    "                         # If you need to re-apply PCA after loading, you must store the original embeddings and PCA components.\n",
    "                         # For now, we assume the cached size is the intended size (either original or compressed).\n",
    "                         pass \n",
    "        else:\n",
    "            print(f\"Computing and caching embeddings for {split_name}...\")\n",
    "            self.text_encodings[split_name] = {}\n",
    "            for col_name, compress in self.TEXT_FIELDS:\n",
    "                emb = self.embed_texts(df[col_name])\n",
    "                \n",
    "                # Apply PCA/Padding\n",
    "                if compress is not None and compress < emb.shape[1]:\n",
    "                    pca = PCA(n_components=compress)\n",
    "                    # PCA requires NumPy/CPU, ensure the tensor is on CPU before converting to NumPy\n",
    "                    emb_np = emb.cpu().detach().numpy()\n",
    "                    emb_np_compressed = pca.fit_transform(emb_np)\n",
    "                    emb = torch.tensor(emb_np_compressed, dtype=torch.float32, device=self.device)\n",
    "                    print(f\"Applied PCA to {col_name} reducing size from {emb_np.shape[1]} to {compress}\")\n",
    "                elif compress is not None and compress > emb.shape[1]:\n",
    "                    print(f\"Warning: embedding for {col_name} zero-padded from {emb.shape[1]} to {compress}\")\n",
    "                    emb = torch.nn.functional.pad(emb, (0, compress - emb.shape[1]))\n",
    "                \n",
    "                result[col_name] = emb\n",
    "                self.text_encodings[split_name][col_name] = emb.cpu().detach()\n",
    "                \n",
    "            # Save the computed embeddings\n",
    "            torch.save(self.text_encodings[split_name], cf)\n",
    "            print(f\"Encodings saved to {cf}\")\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    # The _extract method is removed as its logic is now inside extract\n",
    "    \n",
    "    def extract_raw_metadata(self, df: pd.DataFrame) -> torch.Tensor:\n",
    "        # ... (Keep existing implementation of extract_raw_metadata)\n",
    "        md_cols: list[pd.Series] = []\n",
    "\n",
    "        for fn, col_name in tqdm(self.METADATA_FIELDS, desc=\"Extracting metadata\"):\\\n",
    "            md_cols.append(fn(self, df[col_name]))\n",
    "        \n",
    "        md: pd.DataFrame = pd.concat(md_cols, axis=1)\n",
    "        \n",
    "        if self.training:\n",
    "            self.means = md.mean().fillna(0)\n",
    "            self.stds = md.std().fillna(1)\n",
    "        \n",
    "        assert self.means is not None and self.stds is not None, \"You forgot to train/load the feature extractor\"\n",
    "\n",
    "        md = (md - self.means) / self.stds\n",
    "\n",
    "        return torch.from_numpy(md.to_numpy()).float().to(self.device)\n",
    "\n",
    "    def embed_texts(\n",
    "        self,\n",
    "        texts: pd.Series,\n",
    "        batch_size: int = 64,\n",
    "        progress: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        # ... (Keep existing implementation of embed_texts)\n",
    "        # Ensure encoder is available before calling\n",
    "        if self.text_encoder is None or self.text_tokenizer is None:\n",
    "            raise ValueError(\"Text encoder and tokenizer must be loaded to embed texts.\")\n",
    "            \n",
    "        tokenizer = self.text_tokenizer\n",
    "        encoder = self.text_encoder\n",
    "        encoder.eval() # Always evaluate the encoder when embedding texts\n",
    "\n",
    "        all_embeddings = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_offsets = range(0, len(texts), batch_size)\n",
    "            if progress:\n",
    "                batch_offsets = tqdm(batch_offsets, desc=f\"Embedding {texts.name or '<unnamed>'}\")\n",
    "            for i in batch_offsets:\n",
    "                batch_texts = texts.iloc[i:i + batch_size]\n",
    "                nonna = batch_texts.notna() & batch_texts.str.len().gt(0)\n",
    "\n",
    "                tokenized = tokenizer(\n",
    "                    batch_texts[nonna].tolist(),\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=self.text_config.max_position_embeddings\n",
    "                ).to(self.device)\n",
    "\n",
    "                outputs: transformers.modeling_outputs.BaseModelOutput = encoder(**tokenized)\n",
    "                last_hidden: torch.Tensor = outputs.last_hidden_state\n",
    "                mask: torch.Tensor = tokenized[\"attention_mask\"].unsqueeze(-1)\n",
    "                \n",
    "                masked_hidden = last_hidden * mask\n",
    "                summed = masked_hidden.sum(dim=1)\n",
    "                counts = mask.sum(dim=1)\n",
    "                embeddings = torch.zeros(len(batch_texts), last_hidden.shape[2], device=self.device)\n",
    "                nonna = nonna.reset_index(drop=True)\n",
    "                embeddings[nonna[nonna].index] = (summed / counts)\n",
    "\n",
    "                all_embeddings.append(embeddings)\n",
    "\n",
    "        return torch.cat(all_embeddings, dim=0)\n",
    "    \n",
    "    def apply_fill_mean(\n",
    "        self,\n",
    "        col: pd.Series,\n",
    "        func: typing.Callable[[typing.Any], typing.Any],\n",
    "    ) -> pd.Series:\n",
    "        col = col.map(func, na_action=\"ignore\")\n",
    "        \n",
    "        key = (col.name, func.__name__)\n",
    "        if self.training:\n",
    "            self.afm_cache[key] = col.mean()\n",
    "        assert key in self.afm_cache, \"You forgot to train/load the feature extractor\"\n",
    "        \n",
    "        return col.fillna(self.afm_cache[key])\n",
    "    \n",
    "    def md_bool(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(lambda x: (1 if x else -1), na_action=\"ignore\").fillna(0)\n",
    "\n",
    "    def md_len(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(len, na_action=\"ignore\").fillna(0)\n",
    "\n",
    "    def md_time(self, col: pd.Series) -> pd.Series:\n",
    "        return self.apply_fill_mean(col, lambda x: time.mktime(time.strptime(x, \"%a %b %d %H:%M:%S %z %Y\")))\n",
    "\n",
    "    def md_num(self, col: pd.Series) -> pd.Series:\n",
    "        return self.apply_fill_mean(col, pd.to_numeric)\n",
    "\n",
    "    def md_place(self, col: pd.Series) -> pd.Series:\n",
    "        return col.map(lambda x: int(x, 16), na_action=\"ignore\").fillna(0)\n",
    "    \n",
    "    METADATA_FIELDS: list[tuple[typing.Callable[[FeatureExtractor, pd.Series], pd.Series], str]] = [\n",
    "        (md_bool, \"is_quote_status\"),\n",
    "        (md_bool, \"is_reply\"),\n",
    "        (md_bool, \"possibly_sensitive\"),\n",
    "        (md_bool, \"quoted_status_user_verified\"),\n",
    "        (md_bool, \"user_is_translator\"),\n",
    "        (md_bool, \"user_geo_enabled\"),\n",
    "        (md_bool, \"user_profile_use_background_image\"),\n",
    "        (md_bool, \"user_default_profile\"),\n",
    "        \n",
    "        (md_len, \"full_text\"),\n",
    "        (md_len, \"source_name\"),\n",
    "        (md_len, \"in_reply_to_screen_name\"),\n",
    "        (md_len, \"quoted_status_extended_tweet_entities_urls\"),\n",
    "        (md_len, \"quoted_status_extended_tweet_entities_user_mentions\"),\n",
    "        (md_len, \"quoted_status_extended_tweet_full_text\"),\n",
    "        (md_len, \"quoted_status_entities_urls\"),\n",
    "        (md_len, \"quoted_status_user_profile_image_url_https\"),\n",
    "        (md_len, \"quoted_status_user_profile_background_image_url\"),\n",
    "        (md_len, \"quoted_status_user_profile_background_image_url_https\"),\n",
    "        (md_len, \"quoted_status_user_screen_name\"),\n",
    "        (md_len, \"quoted_status_user_name\"),\n",
    "        (md_len, \"entities_hashtags\"),\n",
    "        (md_len, \"entities_user_mentions\"),\n",
    "        (md_len, \"user_profile_image_url_https\"),\n",
    "        (md_len, \"user_profile_background_image_url\"),\n",
    "        (md_len, \"user_description\"),\n",
    "        (md_len, \"user_translator_type\"),\n",
    "        (md_len, \"user_url\"),\n",
    "        (md_len, \"user_profile_banner_url\"),\n",
    "        (md_len, \"user_location\"),\n",
    "        (md_len, \"display_text_range\"),\n",
    "        (md_len, \"extended_tweet_entities_urls\"),\n",
    "        (md_len, \"extended_tweet_entities_hashtags\"),\n",
    "        (md_len, \"extended_tweet_entities_user_mentions\"),\n",
    "        (md_len, \"quoted_status_permalink_expanded\"),\n",
    "        \n",
    "        (md_time, \"created_at\"),\n",
    "        (md_time, \"user_created_at\"),\n",
    "        (md_time, \"quoted_status_created_at\"),\n",
    "        (md_time, \"quoted_status_user_created_at\"),\n",
    "        \n",
    "        (md_num, \"user_statuses_count\"),\n",
    "        (md_num, \"user_listed_count\"),\n",
    "        (md_num, \"user_favourites_count\"),\n",
    "        (md_num, \"user_profile_background_tile\"),\n",
    "        (md_num, \"quoted_status_quote_count\"),\n",
    "        (md_num, \"quoted_status_user_followers_count\"),\n",
    "        (md_num, \"quoted_status_user_favourites_count\"),\n",
    "        (md_num, \"in_reply_to_status_id\"),\n",
    "        \n",
    "        (md_place, \"quoted_status_place_id\"),\n",
    "        (md_place, \"place_id\"),\n",
    "    ]\n",
    "\n",
    "    TEXT_FIELDS: list[tuple[str, int | None]] = [\n",
    "        (\"full_text\", None),\n",
    "        (\"user_description\", 64),\n",
    "        (\"misc_text\", None),\n",
    "        # (\"source_name\", None),\n",
    "        # (\"in_reply_to_screen_name\", None),\n",
    "        # (\"quoted_status_user_screen_name\", None),\n",
    "        # (\"quoted_status_user_name\", None),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1bc90d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    features: dict[str, torch.Tensor]\n",
    "    labels: torch.Tensor\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        features: dict[str, torch.Tensor],\n",
    "        labels: pd.Series,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        self.features = features\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features[\"md\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"features\": {key: val[idx] for key, val in self.features.items()},\n",
    "            \"label\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    features = {\n",
    "        key: torch.stack([x[\"features\"][key] for x in batch])\n",
    "        for key in batch[0][\"features\"].keys()\n",
    "    }\n",
    "    labels = torch.stack([x[\"label\"] for x in batch])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af4a722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "\n",
    "class TweetClassifier(nn.Module):\n",
    "    feature_sizes: dict[str, int]\n",
    "    \n",
    "    layer1: nn.ModuleDict\n",
    "    fc2: nn.Linear\n",
    "    fc3: nn.Linear\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_sizes: dict[str, int],\n",
    "        hidden_dim: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_sizes = feature_sizes\n",
    "        \n",
    "        self.layer1 = nn.ModuleDict()\n",
    "        \n",
    "        def _add(name, dropout: float):\n",
    "            self.layer1[name] = nn.Sequential(\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(feature_sizes[name], hidden_dim),\n",
    "            )\n",
    "        \n",
    "        _add(\"md\", 0.1)\n",
    "        _add(\"full_text\", 0.1)\n",
    "        _add(\"user_description\", 0.75)\n",
    "        _add(\"misc_text\", 0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, NUM_CLASSES)\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def forward(self, features: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n",
    "        batch_size = len(features[\"md\"])\n",
    "        \n",
    "        x = torch.zeros(batch_size, self.fc2.in_features, device=self.device)\n",
    "        \n",
    "        for name, module in self.layer1.items():\n",
    "            x += module(features[name])\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"probs\": probs,\n",
    "            \"log_probs\": log_probs,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c7a47ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: TweetClassifier,\n",
    "    train_ds: Dataset,\n",
    "    val_ds: Dataset,\n",
    "    epochs: int = 3,\n",
    "    lr: float = 2e-4,\n",
    "    weight_decay: float = 0.01,  # TODO: Lower?\n",
    "    max_grad_norm: float = 1.0,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    "    optimizer: torch.optim.Optimizer | None = None,\n",
    "    checkpoints_path: pathlib.Path | str | None = \".\",\n",
    "    return_best: bool = False,\n",
    ") -> TweetClassifier:\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    if optimizer is None:\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_file: pathlib.Path | None = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        status_bar = tqdm(train_loader, desc=\"Training\")\n",
    "\n",
    "        for features, labels in status_bar:\n",
    "            features: dict[str, torch.Tensor]\n",
    "            labels: torch.Tensor\n",
    "            features = {k: v.to(device) for k, v in features.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            out = model(features)\n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            status_bar.set_postfix({\"loss\": total_loss / (status_bar.n + 1)})\n",
    "        \n",
    "        print(f\"Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        val_metrics = evaluate_model(\n",
    "            model=model,\n",
    "            val_ds=val_ds,\n",
    "            device=device,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['acc']:.4f}\")\n",
    "\n",
    "        if checkpoints_path is not None:\n",
    "            ckpt = pathlib.Path(checkpoints_path) / f\"epoch_{epoch:02}.pt\"\n",
    "            torch.save(model.state_dict(), ckpt)\n",
    "            print(f\"Checkpoint saved to {ckpt}\")\n",
    "            \n",
    "            if val_metrics[\"loss\"] < best_val_loss:\n",
    "                best_val_loss = val_metrics[\"loss\"]\n",
    "                best_model_file = ckpt\n",
    "\n",
    "    if return_best and best_model_file is not None:\n",
    "        print(f\"Best model: {best_model_file}\")\n",
    "        model.load_state_dict(torch.load(best_model_file))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: TweetClassifier,\n",
    "    val_ds: Dataset,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> tuple[float, float]:\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        status_bar = tqdm(val_loader, desc=\"Evaluating\")\n",
    "        \n",
    "        for features, labels in status_bar:\n",
    "            features: dict[str, torch.Tensor]\n",
    "            labels: torch.Tensor\n",
    "            features = {k: v.to(device) for k, v in features.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = model(features)\n",
    "            logits: torch.Tensor = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            count += labels.size(0)\n",
    "            \n",
    "            status_bar.set_postfix({\"loss\": total_loss / (status_bar.n + 1), \"acc\": correct / count})\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / len(val_loader),\n",
    "        \"acc\": correct / count,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "558a176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_model(\n",
    "    model: TweetClassifier,\n",
    "    feature_extractor: FeatureExtractor,\n",
    "    df: pd.DataFrame,\n",
    "    out_file: pathlib.Path,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> pd.Series:\n",
    "    \n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    # 1. Setup Data Loader with Lazy Extracted Features\n",
    "    # The features are extracted/loaded from cache here:\n",
    "    infer_features = feature_extractor.extract(df, 'infer')\n",
    "    infer_ds = TweetDataset(\n",
    "        infer_features, \n",
    "        pd.Series(torch.zeros(len(df), dtype=torch.long)), # Placeholder labels\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        infer_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = torch.zeros(len(df), dtype=torch.long)\n",
    "    offset = 0\n",
    "    \n",
    "    # 2. Run Model Inference\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(data_loader, desc=\"Inferring\"):\n",
    "            \n",
    "            out = model(features)\n",
    "            logits = out[\"logits\"]\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            \n",
    "            predictions[offset: offset + len(preds)] = preds.cpu()\n",
    "            offset += len(preds)\n",
    "            \n",
    "    # --- USER-LEVEL RECONCILIATION ---\n",
    "    \n",
    "    # Copy the input dataframe and attach the single-tweet predictions\n",
    "    df = df.copy()\n",
    "    df[\"pred_label\"] = pd.Series(predictions).astype(int)\n",
    "\n",
    "    # Reconciliation between same users\n",
    "    same_user_key = [\"user_created_at\", \"user_profile_image_url\"]\n",
    "    \n",
    "    # Step A: Count predicted labels (0 or 1) for each unique user key\n",
    "    per_user_stats: dict[tuple[str, str], list[int]] = dict()\n",
    "    for _, row in df.iterrows():\n",
    "        # .setdefault returns [count_label_0, count_label_1]\n",
    "        per_user_stats.setdefault(tuple(row[same_user_key].tolist()), [0, 0])[int(row[\"pred_label\"])] += 1\n",
    "    \n",
    "    # Step B: Determine the reconciled label for users with conflicting predictions\n",
    "    per_user_correct: dict[tuple[str, str], int] = dict()\n",
    "    for key, stats in per_user_stats.items():\n",
    "        # The original code only calculates the majority/tie-breaker if both labels were seen (conflict)\n",
    "        if stats[0] == 0 or stats[1] == 0:\n",
    "            continue # Skip users with unanimous predictions\n",
    "        \n",
    "        # Calculate majority vote (0 or 1), or randomly pick on a tie\n",
    "        per_user_correct[key] = np.select(\n",
    "            [stats[0] > stats[1], stats[1] > stats[0]],\n",
    "            [0, 1],\n",
    "            default=np.random.randint(0, 2),\n",
    "        )\n",
    "    \n",
    "    del per_user_stats\n",
    "    \n",
    "    # Step C: Apply the reconciled prediction back to the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        key = tuple(row[same_user_key].tolist())\n",
    "        if key in per_user_correct:\n",
    "            # Overwrite the prediction with the reconciled label\n",
    "            df.at[idx, \"pred_label\"] = per_user_correct[key]\n",
    "    \n",
    "    # 3. Save to Output File\n",
    "    if out_file is not None:\n",
    "        output = df[[\"challenge_id\", \"pred_label\"]]\n",
    "        output.columns = [\"ID\", \"Prediction\"]\n",
    "        output.to_csv(out_file, index=False)\n",
    "\n",
    "    return df[\"pred_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957dd8c",
   "metadata": {},
   "source": [
    "# Test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff6068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [ almanach/camembertav2-base ] =====\n",
      "\n",
      "Text encoder frozen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting metadata:  10%|█         | 5/48 [00:00<00:00, 43.86it/s]/tmp/ipykernel_140004/2301781320.py:236: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return col.map(len, na_action=\"ignore\").fillna(0)\n",
      "Extracting metadata: 100%|██████████| 48/48 [00:05<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached encodings for train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting metadata:  15%|█▍        | 7/48 [00:00<00:00, 64.98it/s]/tmp/ipykernel_140004/2301781320.py:236: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return col.map(len, na_action=\"ignore\").fillna(0)\n",
      "Extracting metadata: 100%|██████████| 48/48 [00:03<00:00, 13.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached encodings for infer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting metadata: 100%|██████████| 48/48 [00:03<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached encodings for infer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferring: 100%|██████████| 1616/1616 [00:02<00:00, 787.08it/s]\n"
     ]
    }
   ],
   "source": [
    "text_encoder_name = \"almanach/camembertav2-base\"\n",
    "print(f\"\\n===== [ {text_encoder_name} ] =====\\n\")\n",
    "\n",
    "model_folder = pathlib.Path(f\"./models/{VERSION}/\") / text_encoder_name.split(\"/\")[-1]\n",
    "model_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# 1. Initialize FeatureExtractor\n",
    "feature_extractor = FeatureExtractor(\n",
    "    text_encoder_name=text_encoder_name, \n",
    "    text_enc_cache_path=model_folder / \"text_enc_cache\", \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load metadata normalization stats (means/stds) if available\n",
    "f_ext_ckpt = model_folder / \"feature_extractor.ckpt\"\n",
    "if f_ext_ckpt.exists():\n",
    "    # Load state_dict, which includes means, stds, and afm_cache\n",
    "    feature_extractor.load_state_dict(torch.load(f_ext_ckpt, weights_only=False))\n",
    "\n",
    "# 2. Freeze the encoder\n",
    "feature_extractor.freeze_encoder()\n",
    "feature_extractor.train() # Set to train mode for proper metadata normalization (if not loaded)\n",
    "\n",
    "# 3. Extract/Load the full training set features (text and metadata)\n",
    "# 'train' split name ensures the cache is named 'train.ckpt'\n",
    "full_train_features = feature_extractor.extract(X_train, 'train')\n",
    "\n",
    "# Save metadata normalization stats (means/stds) after extraction/computation\n",
    "torch.save(feature_extractor.state_dict(), f_ext_ckpt)\n",
    "\n",
    "# 4. Create full dataset\n",
    "# Pass the pre-extracted features and the original labels\n",
    "full_train_ds = TweetDataset(full_train_features, y_train, device=device)\n",
    "\n",
    "# 5. Split into train and validation sets\n",
    "user_descs = pd.Series(X_train['user_description']).fillna('__MISSING__').factorize()[0]\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "train_idx, val_idx = next(splitter.split(X_train, y_train, groups=user_descs))\n",
    "\n",
    "train_ds = torch.utils.data.Subset(full_train_ds, train_idx)\n",
    "val_ds   = torch.utils.data.Subset(full_train_ds, val_idx)\n",
    "\n",
    "# Instantiate and train the classifier as before\n",
    "model = TweetClassifier(\n",
    "    feature_sizes=feature_extractor.dims(),\n",
    "    hidden_dim=512,\n",
    ").to(device)\n",
    "\n",
    "# Uncomment the following lines to run training/inference\n",
    "# model = train_model(model, train_ds, val_ds, lr=2e-4, epochs=10, batch_size=64, device=device, checkpoints_path=model_folder, return_best=True)\n",
    "# torch.save(model.state_dict(), model_folder / \"best_model.ckpt\")\n",
    "# torch.cuda.empty_cache()\n",
    "# Inference requires extracting features for X_kaggle with split_name='infer'\n",
    "feature_extractor.eval()\n",
    "infer_features = feature_extractor.extract(X_kaggle, 'infer')\n",
    "infer_ds = TweetDataset(infer_features, pd.Series(torch.zeros(len(X_kaggle), dtype=torch.long)), device=device)\n",
    "infer_with_model(model, feature_extractor, X_kaggle, batch_size=64, device=device, out_file=model_folder / f\"predictions-{VERSION}.csv\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad414f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting metadata:  12%|█▎        | 6/48 [00:00<00:00, 57.09it/s]/tmp/ipykernel_140004/2301781320.py:236: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return col.map(len, na_action=\"ignore\").fillna(0)\n",
      "Extracting metadata: 100%|██████████| 48/48 [00:03<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached encodings for infer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferring: 100%|██████████| 1616/1616 [00:02<00:00, 719.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# model_folder = pathlib.Path(f\"./models/{VERSION}/camembertav2-base/\")\n",
    "# feature_extractor = FeatureExtractor(text_encoder_name=\"almanach/camembertav2-base\", text_enc_cache_path=model_folder / \"text_enc_cache\", device=device)\n",
    "# feature_extractor.load_state_dict(torch.load(model_folder / \"feature_extractor.ckpt\", weights_only=False))\n",
    "# model = TweetClassifier(\n",
    "#     feature_sizes=feature_extractor.dims(),\n",
    "#     hidden_dim=512,\n",
    "# ).to(device)\n",
    "# model.load_state_dict(torch.load(model_folder / \"epoch_05.pt\"))\n",
    "# good_predictions = infer_with_model(model, feature_extractor, X_kaggle, batch_size=64, device=device, out_file=model_folder / \"predictions-v10-e09.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156e4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

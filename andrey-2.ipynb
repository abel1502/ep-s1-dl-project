{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fb900036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import typing\n",
    "import json\n",
    "import pathlib\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import transformers\n",
    "import transformers.modeling_outputs\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sklearn\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5f873bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_KAGGLE = \"KAGGLE_DOCKER_IMAGE\" in os.environ\n",
    "\n",
    "DATASETS = pathlib.Path(\n",
    "    \".\"\n",
    "    if not IS_KAGGLE\n",
    "    else \"/kaggle/input/influencers-or-observers-predicting-social-roles/Kaggle2025\"\n",
    ")\n",
    "\n",
    "DATASET_TRAIN = DATASETS / \"train.jsonl\"\n",
    "DATASET_KAGGLE = DATASETS / \"kaggle_test.jsonl\"\n",
    "\n",
    "CACHE_DIR = pathlib.Path(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7fc921d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e20b6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f28f61",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5e16f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: pathlib.Path, cache: bool = False) -> pd.DataFrame:\n",
    "    path_pq = (CACHE_DIR / path.name).with_stem(f\"{path.stem}_raw\").with_suffix(\".parquet\")\n",
    "    \n",
    "    if cache and path_pq.exists():\n",
    "        return pd.read_parquet(path_pq)\n",
    "    \n",
    "    # This leaves things to be desired, since there's no way to specify dtypes\n",
    "    # and it assumes float instead of int, causing a loss in precision...\n",
    "    # But I guess it only matters for ids, which we'll probably discard in preprocessing anyway\n",
    "    result = pd.json_normalize(list(map(json.loads, path.read_bytes().splitlines())))\n",
    "    \n",
    "    if cache:\n",
    "        result.to_parquet(path_pq)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "80d1df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_json(DATASET_TRAIN, cache=True)\n",
    "kaggle_data = load_json(DATASET_KAGGLE, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bcf662",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4ea7df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"is_reply\"] = df[\"in_reply_to_status_id\"].notna()\n",
    "    \n",
    "    # Remove various ID fields\n",
    "    # TODO: Could we actually use them for something?\n",
    "    # Note: challenge_id and label seem to be added for the kaggle challenge\n",
    "    df = df.drop(columns=[\n",
    "        \"in_reply_to_status_id_str\",\n",
    "        \"in_reply_to_status_id\",\n",
    "        \"in_reply_to_user_id_str\",\n",
    "        \"in_reply_to_user_id\",\n",
    "        \"quoted_status_id_str\",\n",
    "        \"quoted_status_id\",\n",
    "        \"id_str\",\n",
    "        \"quoted_status.in_reply_to_status_id_str\",\n",
    "        \"quoted_status.in_reply_to_status_id\",\n",
    "        \"quoted_status.in_reply_to_user_id_str\",\n",
    "        \"quoted_status.in_reply_to_user_id\",\n",
    "        \"quoted_status.id_str\",\n",
    "        \"quoted_status.id\",\n",
    "        \"quoted_status.user.id_str\",\n",
    "        \"quoted_status.user.id\",\n",
    "        \"quoted_status_permalink.expanded\",\n",
    "        \"quoted_status_permalink.display\",\n",
    "        \"quoted_status_permalink.url\",\n",
    "        \"quoted_status.quoted_status_id\",\n",
    "        \"quoted_status.quoted_status_id_str\",\n",
    "        \"quoted_status.place.id\",\n",
    "        \"place.id\",\n",
    "        \"lang\",  # Always \"fr\"\n",
    "        \"retweeted\",  # Always False\n",
    "        \"filter_level\",  # Always \"low\"\n",
    "        \"geo\",  # Always None\n",
    "        \"place\",  # Always None\n",
    "        \"coordinates\",  # Always None\n",
    "        \"contributors\",  # Always None\n",
    "        \"quote_count\",  # Always 0\n",
    "        \"reply_count\",  # Always 0\n",
    "        \"retweet_count\",  # Always 0\n",
    "        \"favorite_count\",  # Always 0\n",
    "        \"favorited\",  # Always False\n",
    "        \"quoted_status.geo\",  # Always None\n",
    "        \"quoted_status.place\",  # Always None\n",
    "        \"quoted_status.coordinates\",  # Always None\n",
    "        \"quoted_status.retweeted\",  # Always False\n",
    "        \"quoted_status.filter_level\",  # Always \"low\"\n",
    "        \"quoted_status.contributors\",  # Always None\n",
    "        \"quoted_status.user.utc_offset\",  # Always None\n",
    "        \"quoted_status.user.lang\",  # Always None\n",
    "        \"quoted_status.user.time_zone\",  # Always None\n",
    "        \"quoted_status.user.follow_request_sent\",  # Always None\n",
    "        \"quoted_status.user.following\",  # Always None\n",
    "        \"quoted_status.user.notifications\",  # Always None\n",
    "        \"user.default_profile_image\",  # Always False\n",
    "        \"user.protected\",  # Always False\n",
    "        \"user.contributors_enabled\",  # Always False\n",
    "        \"user.lang\",  # Always None\n",
    "        \"user.notifications\",  # Always None\n",
    "        \"user.following\",  # Always None\n",
    "        \"user.utc_offset\",  # Always None\n",
    "        \"user.time_zone\",  # Always None\n",
    "        \"user.follow_request_sent\",  # Always None\n",
    "    ])\n",
    "    \n",
    "    # TODO: Augment text with other string features?\n",
    "    df[\"full_text\"] = df.apply(lambda tweet: extract_full_text(tweet), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_full_text(tweet: pd.Series) -> str:\n",
    "    text: str = tweet[\"text\"]\n",
    "    \n",
    "    if not pd.isna(tweet[\"extended_tweet.full_text\"]):\n",
    "        text = tweet[\"extended_tweet.full_text\"]\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ab1ee966",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(\"label\", axis=1)\n",
    "y_train = train_data[\"label\"]\n",
    "\n",
    "X_kaggle = kaggle_data\n",
    "\n",
    "X_train = preprocess(X_train)\n",
    "X_kaggle = preprocess(X_kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eaec88",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6396560b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_quote_status                       -0.018314\n",
       "truncated                             -0.009665\n",
       "challenge_id                           0.001228\n",
       "quoted_status.retweet_count            0.017500\n",
       "quoted_status.favorite_count           0.017766\n",
       "quoted_status.quote_count              0.046755\n",
       "quoted_status.reply_count              0.007355\n",
       "quoted_status.user.friends_count      -0.019404\n",
       "quoted_status.user.listed_count        0.001474\n",
       "quoted_status.user.favourites_count   -0.050869\n",
       "quoted_status.user.statuses_count     -0.000199\n",
       "quoted_status.user.followers_count     0.007178\n",
       "user.listed_count                      0.078584\n",
       "user.favourites_count                  0.146453\n",
       "user.is_translator                     0.013591\n",
       "user.geo_enabled                       0.296986\n",
       "user.profile_background_tile           0.180543\n",
       "user.statuses_count                    0.281050\n",
       "user.profile_use_background_image     -0.129781\n",
       "user.default_profile                  -0.324203\n",
       "is_reply                              -0.216044\n",
       "dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.corrwith(y_train, numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fbea4f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at                      -0.026027\n",
       "quoted_status.created_at        -0.018908\n",
       "quoted_status.user.created_at    0.021092\n",
       "user.created_at                 -0.291983\n",
       "dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_cols = X_train[:10].apply(lambda col: pd.to_datetime(col, format=\"%a %b %d %H:%M:%S %z %Y\", errors=\"coerce\"))\n",
    "dt_cols = dt_cols.columns[dt_cols.notna().any()]\n",
    "\n",
    "pd.Series({\n",
    "    col: X_train[col].apply(lambda x: time.mktime(time.strptime(x, \"%a %b %d %H:%M:%S %z %Y\")) if pd.notnull(x) else pd.NA).corr(y_train)\n",
    "    for col in dt_cols\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "35844e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Program Files\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "C:\\Users\\Abel\\AppData\\Local\\Temp\\ipykernel_27800\\3544080676.py:12: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  f\"len({col})\": X_train[col].apply(safe_len).fillna(0).corr(y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "len(created_at)                                                    NaN\n",
       "len(source)                                                  -0.084287\n",
       "len(in_reply_to_screen_name)                                 -0.217397\n",
       "len(text)                                                    -0.011660\n",
       "len(timestamp_ms)                                                  NaN\n",
       "len(quoted_status.extended_tweet.entities.urls)              -0.031582\n",
       "len(quoted_status.extended_tweet.entities.hashtags)          -0.000267\n",
       "len(quoted_status.extended_tweet.entities.user_mentions)     -0.015762\n",
       "len(quoted_status.extended_tweet.entities.symbols)            0.004725\n",
       "len(quoted_status.extended_tweet.full_text)                  -0.031543\n",
       "len(quoted_status.extended_tweet.display_text_range)         -0.027365\n",
       "len(quoted_status.created_at)                                -0.018283\n",
       "len(quoted_status.source)                                    -0.014854\n",
       "len(quoted_status.text)                                      -0.023169\n",
       "len(quoted_status.lang)                                      -0.018569\n",
       "len(quoted_status.entities.urls)                             -0.035785\n",
       "len(quoted_status.entities.hashtags)                          0.004730\n",
       "len(quoted_status.entities.user_mentions)                    -0.014256\n",
       "len(quoted_status.entities.symbols)                          -0.000859\n",
       "len(quoted_status.user.profile_image_url_https)              -0.018149\n",
       "len(quoted_status.user.profile_background_image_url)         -0.022667\n",
       "len(quoted_status.user.description)                          -0.019241\n",
       "len(quoted_status.user.created_at)                           -0.018283\n",
       "len(quoted_status.user.profile_background_image_url_https)   -0.022669\n",
       "len(quoted_status.user.screen_name)                          -0.024267\n",
       "len(quoted_status.user.profile_link_color)                   -0.018283\n",
       "len(quoted_status.user.translator_type)                      -0.012573\n",
       "len(quoted_status.user.profile_background_color)             -0.018283\n",
       "len(quoted_status.user.profile_sidebar_border_color)         -0.018283\n",
       "len(quoted_status.user.profile_text_color)                   -0.018283\n",
       "len(quoted_status.user.profile_image_url)                    -0.018147\n",
       "len(quoted_status.user.url)                                   0.001281\n",
       "len(quoted_status.user.profile_banner_url)                   -0.009256\n",
       "len(quoted_status.user.name)                                 -0.027843\n",
       "len(quoted_status.user.location)                             -0.007102\n",
       "len(quoted_status.user.profile_sidebar_fill_color)           -0.018283\n",
       "len(entities.urls)                                            0.011930\n",
       "len(entities.hashtags)                                        0.073421\n",
       "len(entities.user_mentions)                                  -0.201323\n",
       "len(entities.symbols)                                         0.013301\n",
       "len(user.profile_image_url_https)                            -0.036512\n",
       "len(user.profile_background_image_url)                        0.275473\n",
       "len(user.description)                                         0.209465\n",
       "len(user.created_at)                                               NaN\n",
       "len(user.profile_background_image_url_https)                  0.275453\n",
       "len(user.profile_link_color)                                       NaN\n",
       "len(user.translator_type)                                     0.147421\n",
       "len(user.profile_background_color)                                 NaN\n",
       "len(user.profile_sidebar_border_color)                             NaN\n",
       "len(user.profile_text_color)                                       NaN\n",
       "len(user.profile_image_url)                                  -0.036512\n",
       "len(user.url)                                                 0.359260\n",
       "len(user.profile_banner_url)                                  0.206593\n",
       "len(user.location)                                            0.121540\n",
       "len(user.profile_sidebar_fill_color)                               NaN\n",
       "len(display_text_range)                                      -0.092617\n",
       "len(extended_tweet.entities.urls)                             0.124327\n",
       "len(extended_tweet.entities.hashtags)                         0.070111\n",
       "len(extended_tweet.entities.user_mentions)                   -0.111226\n",
       "len(extended_tweet.entities.symbols)                          0.009571\n",
       "len(extended_tweet.full_text)                                -0.007871\n",
       "len(extended_tweet.display_text_range)                       -0.009665\n",
       "len(quoted_status.extended_entities.media)                    0.013987\n",
       "len(quoted_status.entities.media)                             0.012334\n",
       "len(quoted_status.display_text_range)                        -0.009546\n",
       "len(full_text)                                               -0.007709\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def safe_len(x: typing.Any) -> int | float:\n",
    "    try:\n",
    "        return len(x)\n",
    "    except TypeError:\n",
    "        return pd.NA\n",
    "\n",
    "len_cols = X_train[:10].apply(lambda col: col.map(safe_len))\n",
    "len_cols = len_cols.columns[len_cols.notna().any()]\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(pd.Series({\n",
    "        f\"len({col})\": X_train[col].apply(safe_len).fillna(0).corr(y_train)\n",
    "        for col in len_cols\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda51842",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "30b02e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: discard quoted_status.lang != \"fr\"?\n",
    "# TODO: some tweets are images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "af4a722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "class TweetClassifier(nn.Module):\n",
    "    # tokenizer: nn.Module\n",
    "    # encoder: nn.Module\n",
    "    metadata_dim: int\n",
    "    md_batchnorm: nn.Module\n",
    "    fc1: nn.Module\n",
    "    fc2: nn.Module\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        # pretrained_encoder: str = \"distilbert-base-cased\", # \"camembert-base\", \"Geotrend/distilbert-base-en-fr-cased\", \"flaubert/flaubert_base_cased\", \"flaubert/flaubert_small_cased\"\n",
    "        metadata_dim: int = 16,\n",
    "        hidden_dim: int = 128,\n",
    "        # max_length: int = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(pretrained_encoder)\n",
    "        # self.encoder = AutoModel.from_pretrained(pretrained_encoder)\n",
    "\n",
    "        # # Don't finetune the encoder... yet?\n",
    "        # for param in self.encoder.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        # self.encoder_dim = self.encoder.config.hidden_size\n",
    "        # self.max_length = max_length\n",
    "\n",
    "        self.metadata_dim = metadata_dim\n",
    "        self.md_batchnorm = nn.BatchNorm1d(metadata_dim)\n",
    "        \n",
    "        # self.fc1 = nn.Linear(self.encoder_dim + metadata_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(metadata_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, NUM_CLASSES)\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    # def encode_text(self, texts: list[str]) -> torch.Tensor:\n",
    "    #     encoded: torch.Tensor = self.tokenizer(\n",
    "    #         texts,\n",
    "    #         padding=True,\n",
    "    #         truncation=True,\n",
    "    #         max_length=self.max_length,\n",
    "    #         return_tensors=\"pt\",\n",
    "    #     ).to(self.device)\n",
    "\n",
    "    #     outputs: transformers.modeling_outputs.BaseModelOutput = self.encoder(**encoded)\n",
    "    #     cls_embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "    #     return cls_embeddings  # [batch, encoder_dim]\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        texts: list[str] | torch.Tensor,\n",
    "        metadata: torch.Tensor,\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns dict with:\n",
    "            \"logits\": tensor [batch_size, num_classes]\n",
    "            \"probs\": tensor [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "        batch_size = len(texts)\n",
    "        \n",
    "        # if isinstance(texts, torch.Tensor):\n",
    "        #     text_vecs = texts\n",
    "        # else:\n",
    "        #     text_vecs = self.encode_text(texts)  # [B, encoder_dim]\n",
    "\n",
    "        metadata = metadata.to(device)\n",
    "        assert metadata.shape == (batch_size, self.metadata_dim)\n",
    "        \n",
    "        metadata = self.md_batchnorm(metadata)\n",
    "        \n",
    "        # x = torch.cat([text_vecs, metadata], dim=1)\n",
    "        x = metadata\n",
    "\n",
    "        hidden = F.relu(self.fc1(x))\n",
    "        logits = self.fc2(hidden)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"probs\": probs,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5be1585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_bool(col: pd.Series):\n",
    "    return col.apply(lambda x: (1 if x else -1) if pd.notnull(x) else 0)\n",
    "\n",
    "def md_len(col: pd.Series):\n",
    "    return col.apply(safe_len).fillna(0)\n",
    "\n",
    "def md_time(col: pd.Series):\n",
    "    tmp = col.apply(lambda x: time.mktime(time.strptime(x, \"%a %b %d %H:%M:%S %z %Y\")) if pd.notnull(x) else pd.NA)\n",
    "    return tmp.fillna(tmp.mean())\n",
    "\n",
    "def md_num(col: pd.Series):\n",
    "    tmp = col.apply(pd.to_numeric)\n",
    "    return tmp.fillna(tmp.mean())\n",
    "\n",
    "\n",
    "METADATA = [\n",
    "    (md_bool, \"is_quote_status\"),\n",
    "    (md_bool, \"is_reply\"),\n",
    "    (md_bool, \"possibly_sensitive\"),\n",
    "    (md_bool, \"quoted_status.user.verified\"),\n",
    "    (md_bool, \"user.is_translator\"),\n",
    "    (md_bool, \"user.geo_enabled\"),\n",
    "    (md_bool, \"user.profile_use_background_image\"),\n",
    "    (md_bool, \"user.default_profile\"),\n",
    "    \n",
    "    (md_len, \"full_text\"),\n",
    "    (md_len, \"source\"),  # TODO: Analyze the contents\n",
    "    (md_len, \"in_reply_to_screen_name\"),  # TODO: Analyze the contents\n",
    "    (md_len, \"quoted_status.extended_tweet.entities.urls\"),\n",
    "    (md_len, \"quoted_status.extended_tweet.entities.user_mentions\"),\n",
    "    (md_len, \"quoted_status.extended_tweet.full_text\"),\n",
    "    (md_len, \"quoted_status.entities.urls\"),\n",
    "    (md_len, \"quoted_status.user.profile_image_url_https\"),\n",
    "    (md_len, \"quoted_status.user.profile_background_image_url\"),\n",
    "    (md_len, \"quoted_status.user.profile_background_image_url_https\"),\n",
    "    (md_len, \"quoted_status.user.screen_name\"),  # TODO: Analyze the contents\n",
    "    (md_len, \"quoted_status.user.name\"),  # TODO: Analyze the contents\n",
    "    (md_len, \"entities.hashtags\"),  # TODO: Analyze the contents\n",
    "    (md_len, \"entities.user_mentions\"),  # TODO: Analyze the contents\n",
    "    (md_len, \"user.profile_image_url_https\"),\n",
    "    (md_len, \"user.profile_background_image_url\"),\n",
    "    (md_len, \"user.description\"),\n",
    "    (md_len, \"user.translator_type\"),\n",
    "    (md_len, \"user.url\"),\n",
    "    (md_len, \"user.profile_banner_url\"),\n",
    "    (md_len, \"user.location\"),\n",
    "    (md_len, \"display_text_range\"),\n",
    "    (md_len, \"extended_tweet.entities.urls\"),\n",
    "    (md_len, \"extended_tweet.entities.hashtags\"),\n",
    "    (md_len, \"extended_tweet.entities.user_mentions\"),\n",
    "    \n",
    "    (md_time, \"created_at\"),\n",
    "    (md_time, \"user.created_at\"),\n",
    "    (md_time, \"quoted_status.created_at\"),\n",
    "    (md_time, \"quoted_status.user.created_at\"),\n",
    "    \n",
    "    (md_num, \"user.statuses_count\"),\n",
    "    (md_num, \"user.listed_count\"),\n",
    "    (md_num, \"user.favourites_count\"),\n",
    "    (md_num, \"user.profile_background_tile\"),\n",
    "    (md_num, \"quoted_status.quote_count\"),\n",
    "    (md_num, \"quoted_status.user.followers_count\"),\n",
    "    (md_num, \"quoted_status.user.favourites_count\"),\n",
    "]\n",
    "\n",
    "METADATA_DIM = len(METADATA)\n",
    "\n",
    "def extract_metadata(df: pd.DataFrame) -> torch.Tensor:\n",
    "    md: list[pd.Series] = []\n",
    "\n",
    "    for fn, col_name in METADATA:\n",
    "        md.append(fn(df[col_name]))\n",
    "\n",
    "    return torch.from_numpy(np.array(md)).transpose(0, 1).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1bc90d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    texts: list[str]\n",
    "    metadata: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "    device: torch.device\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, labels: pd.Series, device: torch.device = device):\n",
    "        self.texts = df[\"full_text\"].tolist()\n",
    "        self.metadata = extract_metadata(df).to(device)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"text\": self.texts[idx],\n",
    "            \"metadata\": self.metadata[idx],\n",
    "            \"label\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts = [x[\"text\"] for x in batch]\n",
    "    metadata = torch.stack([x[\"metadata\"] for x in batch])\n",
    "    labels = torch.stack([x[\"label\"] for x in batch])\n",
    "    return texts, metadata, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c7a47ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: TweetClassifier,\n",
    "    train_ds: Dataset,\n",
    "    epochs: int = 3,\n",
    "    lr: float = 2e-4,\n",
    "    weight_decay: float = 0.01,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    "    optimizer: torch.optim.Optimizer | None = None,\n",
    ") -> TweetClassifier:\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    if optimizer is None:\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        status_bar = tqdm(train_loader, desc=\"Training\")\n",
    "\n",
    "        for texts, metadata, labels in status_bar:\n",
    "            texts: list[str]\n",
    "            metadata: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "            \n",
    "            metadata = metadata.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            out = model(\n",
    "                texts=texts,\n",
    "                metadata=metadata,\n",
    "            )\n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)  # TODO: ?\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            status_bar.set_postfix({\"loss\": total_loss / (status_bar.n + 1)})\n",
    "\n",
    "        print(f\"Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "91fa5e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: TweetClassifier,\n",
    "    val_ds: Dataset,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> tuple[float, float]:\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        status_bar = tqdm(val_loader, desc=\"Evaluating\")\n",
    "        \n",
    "        for texts, metadata, labels in status_bar:\n",
    "            texts: list[str]\n",
    "            metadata: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "            metadata = metadata.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = model(\n",
    "                texts=texts,\n",
    "                metadata=metadata,\n",
    "            )\n",
    "            logits: torch.Tensor = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            count += labels.size(0)\n",
    "            \n",
    "            status_bar.set_postfix({\"loss\": total_loss / (status_bar.n + 1), \"acc\": correct / count})\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / len(val_loader),\n",
    "        \"acc\": correct / count,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "558a176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_model(\n",
    "    model: TweetClassifier,\n",
    "    df: pd.DataFrame,\n",
    "    out_file: pathlib.Path | str | None = None,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> torch.Tensor:\n",
    "    data_loader = DataLoader(\n",
    "        TweetDataset(df, torch.zeros(len(df), dtype=torch.long, device=device), device=device),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = torch.zeros(len(df), dtype=torch.long)\n",
    "    cur_idx = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, metadata, _ in tqdm(data_loader, desc=\"Inferring\"):\n",
    "            texts: list[str]\n",
    "            metadata: torch.Tensor\n",
    "            metadata = metadata.to(device)\n",
    "\n",
    "            out = model(\n",
    "                texts=texts,\n",
    "                metadata=metadata,\n",
    "            )\n",
    "            logits: torch.Tensor = out[\"logits\"].cpu()\n",
    "            \n",
    "            predictions[cur_idx:cur_idx+len(texts)] = logits.argmax(dim=-1)\n",
    "            cur_idx += len(texts)\n",
    "    \n",
    "    if out_file is not None:\n",
    "        output = pd.concat([df[\"challenge_id\"], pd.DataFrame(predictions)], axis=1, ignore_index=True)\n",
    "        output.columns = [\"ID\", \"Prediction\"]\n",
    "        output.to_csv(out_file, index=False)\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "55cd1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TweetClassifier(\n",
    "    # pretrained_encoder=\"camembert-base\",\n",
    "    metadata_dim=METADATA_DIM,\n",
    "    hidden_dim=128,\n",
    "    # max_length=256\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7f1b1bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abel\\AppData\\Local\\Temp\\ipykernel_27800\\489471857.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return col.apply(safe_len).fillna(0)\n",
      "C:\\Users\\Abel\\AppData\\Local\\Temp\\ipykernel_27800\\489471857.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return tmp.fillna(tmp.mean())\n"
     ]
    }
   ],
   "source": [
    "full_train_ds = TweetDataset(X_train, y_train, device=device)\n",
    "\n",
    "train_ds, val_ds = random_split(full_train_ds, [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8c1218d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.7305, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.md_batchnorm(full_train_ds.metadata)[760, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cb6c3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(version: str) -> pathlib.Path:\n",
    "    return pathlib.Path(\n",
    "        f\"./models/{version}/model-{version}.pt\"\n",
    "        if not IS_KAGGLE\n",
    "        else f\"/kaggle/input/model-{version}-pt/model-{version}.pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f699b6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [00:11<00:00, 185.15it/s, loss=0.482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4807\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [00:11<00:00, 190.94it/s, loss=0.444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4415\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [00:12<00:00, 181.01it/s, loss=0.434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4333\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [00:12<00:00, 168.37it/s, loss=0.429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4283\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [00:12<00:00, 172.17it/s, loss=0.427]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = get_model_path(\"v6\")\n",
    "\n",
    "# model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "model = train_model(model, train_ds, epochs=5, batch_size=64, device=device)\n",
    "torch.save(model.state_dict(), \"model-v6.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cb1426f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 243/243 [00:01<00:00, 234.71it/s, loss=0.46, acc=0.799] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.4410637799849726, 'acc': 0.7987218384868633}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Pre-encode all texts?\n",
    "evaluate_model(model, val_ds, batch_size=64, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c0d3dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abel\\AppData\\Local\\Temp\\ipykernel_27800\\489471857.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return col.apply(safe_len).fillna(0)\n",
      "C:\\Users\\Abel\\AppData\\Local\\Temp\\ipykernel_27800\\489471857.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return tmp.fillna(tmp.mean())\n",
      "C:\\Users\\Abel\\AppData\\Local\\Temp\\ipykernel_27800\\2988799321.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
      "Inferring: 100%|██████████| 1616/1616 [00:02<00:00, 715.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 1, 1, 0])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_with_model(model, X_kaggle, batch_size=64, out_file=\"predictions-v6.csv\", device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

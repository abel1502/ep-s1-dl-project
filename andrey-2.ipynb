{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb900036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import typing\n",
    "import json\n",
    "import pathlib\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn   \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import transformers\n",
    "import transformers.modeling_outputs\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sklearn\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f873bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_KAGGLE = \"KAGGLE_DOCKER_IMAGE\" in os.environ\n",
    "\n",
    "DATASETS = pathlib.Path(\n",
    "    \".\"\n",
    "    if not IS_KAGGLE\n",
    "    else \"/kaggle/input/influencers-or-observers-predicting-social-roles/Kaggle2025\"\n",
    ")\n",
    "\n",
    "DATASET_TRAIN = DATASETS / \"train.jsonl\"\n",
    "DATASET_KAGGLE = DATASETS / \"kaggle_test.jsonl\"\n",
    "\n",
    "CACHE_DIR = pathlib.Path(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fc921d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e20b6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f28f61",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e16f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: pathlib.Path, cache: bool = False) -> pd.DataFrame:\n",
    "    path_pq = (CACHE_DIR / path.name).with_stem(f\"{path.stem}_raw\").with_suffix(\".parquet\")\n",
    "    \n",
    "    if cache and path_pq.exists():\n",
    "        return pd.read_parquet(path_pq)\n",
    "    \n",
    "    # This leaves things to be desired, since there's no way to specify dtypes\n",
    "    # and it assumes float instead of int, causing a loss in precision...\n",
    "    # But I guess it only matters for ids, which we'll probably discard in preprocessing anyway\n",
    "    result = pd.json_normalize(list(map(json.loads, path.read_bytes().splitlines())))\n",
    "    \n",
    "    if cache:\n",
    "        result.to_parquet(path_pq)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80d1df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_json(DATASET_TRAIN, cache=True)\n",
    "kaggle_data = load_json(DATASET_KAGGLE, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bcf662",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ea7df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"is_reply\"] = df[\"in_reply_to_status_id\"].notna()\n",
    "    \n",
    "    # Remove various ID fields\n",
    "    # TODO: Could we actually use them for something?\n",
    "    # Note: challenge_id and label seem to be added for the kaggle challenge\n",
    "    df = df.drop(columns=[\n",
    "        \"in_reply_to_status_id_str\",\n",
    "        \"in_reply_to_status_id\",\n",
    "        \"in_reply_to_user_id_str\",\n",
    "        \"in_reply_to_user_id\",\n",
    "        \"quoted_status_id_str\",\n",
    "        \"quoted_status_id\",\n",
    "        # \"id_str\", # keeping so that we can split zee data grouped on this\n",
    "        \"quoted_status.in_reply_to_status_id_str\",\n",
    "        \"quoted_status.in_reply_to_status_id\",\n",
    "        \"quoted_status.in_reply_to_user_id_str\",\n",
    "        \"quoted_status.in_reply_to_user_id\",\n",
    "        \"quoted_status.id_str\",\n",
    "        \"quoted_status.id\",\n",
    "        \"quoted_status.user.id_str\",\n",
    "        \"quoted_status.user.id\",\n",
    "        \"quoted_status_permalink.expanded\",\n",
    "        \"quoted_status_permalink.display\",\n",
    "        \"quoted_status_permalink.url\",\n",
    "        \"quoted_status.quoted_status_id\",\n",
    "        \"quoted_status.quoted_status_id_str\",\n",
    "        \"quoted_status.place.id\",\n",
    "        \"place.id\",\n",
    "        \"lang\",  # Always \"fr\"\n",
    "        \"retweeted\",  # Always False\n",
    "        \"filter_level\",  # Always \"low\"\n",
    "        \"geo\",  # Always None\n",
    "        \"place\",  # Always None\n",
    "        \"coordinates\",  # Always None\n",
    "        \"contributors\",  # Always None\n",
    "        \"quote_count\",  # Always 0\n",
    "        \"reply_count\",  # Always 0\n",
    "        \"retweet_count\",  # Always 0\n",
    "        \"favorite_count\",  # Always 0\n",
    "        \"favorited\",  # Always False\n",
    "        \"quoted_status.geo\",  # Always None\n",
    "        \"quoted_status.place\",  # Always None\n",
    "        \"quoted_status.coordinates\",  # Always None\n",
    "        \"quoted_status.retweeted\",  # Always False\n",
    "        \"quoted_status.filter_level\",  # Always \"low\"\n",
    "        \"quoted_status.contributors\",  # Always None\n",
    "        \"quoted_status.user.utc_offset\",  # Always None\n",
    "        \"quoted_status.user.lang\",  # Always None\n",
    "        \"quoted_status.user.time_zone\",  # Always None\n",
    "        \"quoted_status.user.follow_request_sent\",  # Always None\n",
    "        \"quoted_status.user.following\",  # Always None\n",
    "        \"quoted_status.user.notifications\",  # Always None\n",
    "        \"user.default_profile_image\",  # Always False\n",
    "        \"user.protected\",  # Always False\n",
    "        \"user.contributors_enabled\",  # Always False\n",
    "        \"user.lang\",  # Always None\n",
    "        \"user.notifications\",  # Always None\n",
    "        \"user.following\",  # Always None\n",
    "        \"user.utc_offset\",  # Always None\n",
    "        \"user.time_zone\",  # Always None\n",
    "        \"user.follow_request_sent\",  # Always None\n",
    "    ])\n",
    "    \n",
    "    # TODO: Augment text with other string features?\n",
    "    df[\"full_text\"] = df.apply(lambda tweet: extract_full_text(tweet), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_full_text(tweet: pd.Series) -> str:\n",
    "    text: str = tweet[\"text\"]\n",
    "    \n",
    "    if not pd.isna(tweet[\"extended_tweet.full_text\"]):\n",
    "        text = tweet[\"extended_tweet.full_text\"]\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab1ee966",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(\"label\", axis=1)\n",
    "y_train = train_data[\"label\"]\n",
    "\n",
    "X_kaggle = kaggle_data\n",
    "\n",
    "X_train = preprocess(X_train)\n",
    "X_kaggle = preprocess(X_kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eaec88",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6396560b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_quote_status                       -0.018314\n",
       "truncated                             -0.009665\n",
       "challenge_id                           0.001228\n",
       "quoted_status.retweet_count            0.017500\n",
       "quoted_status.favorite_count           0.017766\n",
       "quoted_status.quote_count              0.046755\n",
       "quoted_status.reply_count              0.007355\n",
       "quoted_status.user.friends_count      -0.019404\n",
       "quoted_status.user.listed_count        0.001474\n",
       "quoted_status.user.favourites_count   -0.050869\n",
       "quoted_status.user.statuses_count     -0.000199\n",
       "quoted_status.user.followers_count     0.007178\n",
       "user.listed_count                      0.078584\n",
       "user.favourites_count                  0.146453\n",
       "user.is_translator                     0.013591\n",
       "user.geo_enabled                       0.296986\n",
       "user.profile_background_tile           0.180543\n",
       "user.statuses_count                    0.281050\n",
       "user.profile_use_background_image     -0.129781\n",
       "user.default_profile                  -0.324203\n",
       "is_reply                              -0.216044\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.corrwith(y_train, numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbea4f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at                      -0.026027\n",
       "quoted_status.created_at        -0.018908\n",
       "quoted_status.user.created_at    0.021092\n",
       "user.created_at                 -0.291983\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_cols = X_train[:10].apply(lambda col: pd.to_datetime(col, format=\"%a %b %d %H:%M:%S %z %Y\", errors=\"coerce\"))\n",
    "dt_cols = dt_cols.columns[dt_cols.notna().any()]\n",
    "\n",
    "pd.Series({\n",
    "    col: X_train[col].apply(lambda x: time.mktime(time.strptime(x, \"%a %b %d %H:%M:%S %z %Y\")) if pd.notnull(x) else pd.NA).corr(y_train)\n",
    "    for col in dt_cols\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35844e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gtw/.conda/envs/deep_learning/lib/python3.12/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/gtw/.conda/envs/deep_learning/lib/python3.12/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "/tmp/ipykernel_136743/3544080676.py:12: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  f\"len({col})\": X_train[col].apply(safe_len).fillna(0).corr(y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "len(created_at)                                                    NaN\n",
       "len(source)                                                  -0.084287\n",
       "len(in_reply_to_screen_name)                                 -0.217397\n",
       "len(id_str)                                                        NaN\n",
       "len(text)                                                    -0.011660\n",
       "len(timestamp_ms)                                                  NaN\n",
       "len(quoted_status.extended_tweet.entities.urls)              -0.031582\n",
       "len(quoted_status.extended_tweet.entities.hashtags)          -0.000267\n",
       "len(quoted_status.extended_tweet.entities.user_mentions)     -0.015762\n",
       "len(quoted_status.extended_tweet.entities.symbols)            0.004725\n",
       "len(quoted_status.extended_tweet.full_text)                  -0.031543\n",
       "len(quoted_status.extended_tweet.display_text_range)         -0.027365\n",
       "len(quoted_status.created_at)                                -0.018283\n",
       "len(quoted_status.source)                                    -0.014854\n",
       "len(quoted_status.text)                                      -0.023169\n",
       "len(quoted_status.lang)                                      -0.018569\n",
       "len(quoted_status.entities.urls)                             -0.035785\n",
       "len(quoted_status.entities.hashtags)                          0.004730\n",
       "len(quoted_status.entities.user_mentions)                    -0.014256\n",
       "len(quoted_status.entities.symbols)                          -0.000859\n",
       "len(quoted_status.user.profile_image_url_https)              -0.018149\n",
       "len(quoted_status.user.profile_background_image_url)         -0.022667\n",
       "len(quoted_status.user.description)                          -0.019241\n",
       "len(quoted_status.user.created_at)                           -0.018283\n",
       "len(quoted_status.user.profile_background_image_url_https)   -0.022669\n",
       "len(quoted_status.user.screen_name)                          -0.024267\n",
       "len(quoted_status.user.profile_link_color)                   -0.018283\n",
       "len(quoted_status.user.translator_type)                      -0.012573\n",
       "len(quoted_status.user.profile_background_color)             -0.018283\n",
       "len(quoted_status.user.profile_sidebar_border_color)         -0.018283\n",
       "len(quoted_status.user.profile_text_color)                   -0.018283\n",
       "len(quoted_status.user.profile_image_url)                    -0.018147\n",
       "len(quoted_status.user.url)                                   0.001281\n",
       "len(quoted_status.user.profile_banner_url)                   -0.009256\n",
       "len(quoted_status.user.name)                                 -0.027843\n",
       "len(quoted_status.user.location)                             -0.007102\n",
       "len(quoted_status.user.profile_sidebar_fill_color)           -0.018283\n",
       "len(entities.urls)                                            0.011930\n",
       "len(entities.hashtags)                                        0.073421\n",
       "len(entities.user_mentions)                                  -0.201323\n",
       "len(entities.symbols)                                         0.013301\n",
       "len(user.profile_image_url_https)                            -0.036512\n",
       "len(user.profile_background_image_url)                        0.275473\n",
       "len(user.description)                                         0.209465\n",
       "len(user.created_at)                                               NaN\n",
       "len(user.profile_background_image_url_https)                  0.275453\n",
       "len(user.profile_link_color)                                       NaN\n",
       "len(user.translator_type)                                     0.147421\n",
       "len(user.profile_background_color)                                 NaN\n",
       "len(user.profile_sidebar_border_color)                             NaN\n",
       "len(user.profile_text_color)                                       NaN\n",
       "len(user.profile_image_url)                                  -0.036512\n",
       "len(user.url)                                                 0.359260\n",
       "len(user.profile_banner_url)                                  0.206593\n",
       "len(user.location)                                            0.121540\n",
       "len(user.profile_sidebar_fill_color)                               NaN\n",
       "len(display_text_range)                                      -0.092617\n",
       "len(extended_tweet.entities.urls)                             0.124327\n",
       "len(extended_tweet.entities.hashtags)                         0.070111\n",
       "len(extended_tweet.entities.user_mentions)                   -0.111226\n",
       "len(extended_tweet.entities.symbols)                          0.009571\n",
       "len(extended_tweet.full_text)                                -0.007871\n",
       "len(extended_tweet.display_text_range)                       -0.009665\n",
       "len(quoted_status.extended_entities.media)                    0.013987\n",
       "len(quoted_status.entities.media)                             0.012334\n",
       "len(quoted_status.display_text_range)                        -0.009546\n",
       "len(full_text)                                               -0.007709\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def safe_len(x: typing.Any) -> int | float:\n",
    "    try:\n",
    "        return len(x)\n",
    "    except TypeError:\n",
    "        return pd.NA\n",
    "\n",
    "len_cols = X_train[:10].apply(lambda col: col.map(safe_len))\n",
    "len_cols = len_cols.columns[len_cols.notna().any()]\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(pd.Series({\n",
    "        f\"len({col})\": X_train[col].apply(safe_len).fillna(0).corr(y_train)\n",
    "        for col in len_cols\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda51842",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30b02e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: discard quoted_status.lang != \"fr\"?\n",
    "# TODO: some tweets are images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af4a722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "class TweetClassifier(nn.Module):\n",
    "    # tokenizer: nn.Module\n",
    "    # encoder: nn.Module\n",
    "    desc_tokenizer: nn.Module\n",
    "    desc_encoder: nn.Module\n",
    "    metadata_dim: int\n",
    "    md_batchnorm: nn.Module\n",
    "    fc1: nn.Module\n",
    "    fc2: nn.Module\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        # pretrained_encoder: str = \"distilbert-base-cased\", # \"camembert-base\", \"Geotrend/distilbert-base-en-fr-cased\", \"flaubert/flaubert_base_cased\", \"flaubert/flaubert_small_cased\"\n",
    "        desc_encoder_name: str = \"Geotrend/distilbert-base-en-fr-cased\",\n",
    "        desc_max_length: int = 15,\n",
    "        metadata_dim: int = 16,\n",
    "        hidden_dim: int = 128,\n",
    "        # max_length: int = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(pretrained_encoder)\n",
    "        # self.encoder = AutoModel.from_pretrained(pretrained_encoder)\n",
    "\n",
    "        # # Don't finetune the encoder... yet?\n",
    "        # for param in self.encoder.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        # self.encoder_dim = self.encoder.config.hidden_size\n",
    "        # self.max_length = max_length\n",
    "\n",
    "        self.metadata_dim = metadata_dim\n",
    "        self.md_batchnorm = nn.BatchNorm1d(metadata_dim)\n",
    "\n",
    "        # Description text encoding\n",
    "        self.desc_tokenizer = AutoTokenizer.from_pretrained(desc_encoder_name)\n",
    "        self.desc_encoder = AutoModel.from_pretrained(desc_encoder_name)\n",
    "        desc_config = AutoConfig.from_pretrained(desc_encoder_name)\n",
    "        self.desc_encoder_dim = desc_config.hidden_size\n",
    "        self.desc_max_length = desc_max_length\n",
    "        \n",
    "        # self.fc1 = nn.Linear(self.encoder_dim + metadata_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(metadata_dim + self.desc_encoder_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, NUM_CLASSES)\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    # def encode_text(self, texts: list[str]) -> torch.Tensor:\n",
    "    #     encoded: torch.Tensor = self.tokenizer(\n",
    "    #         texts,\n",
    "    #         padding=True,\n",
    "    #         truncation=True,\n",
    "    #         max_length=self.max_length,\n",
    "    #         return_tensors=\"pt\",\n",
    "    #     ).to(self.device)\n",
    "\n",
    "    #     outputs: transformers.modeling_outputs.BaseModelOutput = self.encoder(**encoded)\n",
    "    #     cls_embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "    #     return cls_embeddings  # [batch, encoder_dim]\n",
    "\n",
    "    def encode_desc_texts(self, desc_texts: list[str]) -> torch.Tensor:\n",
    "        encoded: torch.Tensor = self.desc_tokenizer(\n",
    "            desc_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.desc_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Get the [CLS] token embedding\n",
    "        outputs = self.desc_encoder(**encoded)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        return cls_embeddings  # [batch, sn_encoder_dim]\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        texts: list[str] | torch.Tensor,\n",
    "        metadata: torch.Tensor,\n",
    "        desc_texts: list[str],\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns dict with:\n",
    "            \"logits\": tensor [batch_size, num_classes]\n",
    "            \"probs\": tensor [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "        batch_size = len(texts)\n",
    "        \n",
    "        # if isinstance(texts, torch.Tensor):\n",
    "        #     text_vecs = texts\n",
    "        # else:\n",
    "        #     text_vecs = self.encode_text(texts)  # [B, encoder_dim]\n",
    "\n",
    "        # Encode the description texts\n",
    "        desc_vecs = self.encode_desc_texts(desc_texts)\n",
    "\n",
    "        metadata = metadata.to(device)\n",
    "        assert metadata.shape == (batch_size, self.metadata_dim)\n",
    "        metadata = self.md_batchnorm(metadata)\n",
    "        \n",
    "        # Concatenate text vectors and metadata\n",
    "        # x = torch.cat([text_vecs, metadata, desc_vecs], dim=1)\n",
    "        x = torch.cat([metadata, desc_vecs], dim=1)\n",
    "        # x = metadata\n",
    "\n",
    "        hidden = F.relu(self.fc1(x))\n",
    "        logits = self.fc2(hidden)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"probs\": probs,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5be1585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_bool(col: pd.Series):\n",
    "    return col.apply(lambda x: (1 if x else -1) if pd.notnull(x) else 0)\n",
    "\n",
    "def md_len(col: pd.Series):\n",
    "    return col.apply(safe_len).fillna(0)\n",
    "\n",
    "def md_time(col: pd.Series):\n",
    "    tmp = col.apply(lambda x: time.mktime(time.strptime(x, \"%a %b %d %H:%M:%S %z %Y\")) if pd.notnull(x) else pd.NA)\n",
    "    return tmp.fillna(tmp.mean())\n",
    "\n",
    "def md_num(col: pd.Series):\n",
    "    tmp = col.apply(pd.to_numeric)\n",
    "    return tmp.fillna(tmp.mean())\n",
    "\n",
    "\n",
    "METADATA = [\n",
    "    (md_bool, \"is_quote_status\"),\n",
    "    (md_bool, \"is_reply\"),\n",
    "    (md_bool, \"possibly_sensitive\"),\n",
    "    (md_bool, \"quoted_status.user.verified\"),\n",
    "    (md_bool, \"user.is_translator\"),\n",
    "    (md_bool, \"user.geo_enabled\"),\n",
    "    (md_bool, \"user.profile_use_background_image\"),\n",
    "    (md_bool, \"user.default_profile\"),\n",
    "    \n",
    "    (md_len, \"full_text\"),\n",
    "    (md_len, \"source\"),  # TODO: Analyze the contents\n",
    "    (md_len, \"in_reply_to_screen_name\"),  # TODO: Analyze the contents\n",
    "    (md_len, \"quoted_status.extended_tweet.entities.urls\"),\n",
    "    (md_len, \"quoted_status.extended_tweet.entities.user_mentions\"),\n",
    "    (md_len, \"quoted_status.extended_tweet.full_text\"),\n",
    "    (md_len, \"quoted_status.entities.urls\"),\n",
    "    (md_len, \"quoted_status.user.profile_image_url_https\"),\n",
    "    (md_len, \"quoted_status.user.profile_background_image_url\"),\n",
    "    (md_len, \"quoted_status.user.profile_background_image_url_https\"),\n",
    "    (md_len, \"quoted_status.user.screen_name\"),  # TODO: Analyze the contents\n",
    "    (md_len, \"quoted_status.user.name\"),  # TODO: Analyze the contents\n",
    "    (md_len, \"entities.hashtags\"),  # TODO: Analyze the contents\n",
    "    (md_len, \"entities.user_mentions\"),  # TODO: Analyze the contents\n",
    "    (md_len, \"user.profile_image_url_https\"),\n",
    "    (md_len, \"user.profile_background_image_url\"),\n",
    "    (md_len, \"user.description\"),\n",
    "    (md_len, \"user.translator_type\"),\n",
    "    (md_len, \"user.url\"),\n",
    "    (md_len, \"user.profile_banner_url\"),\n",
    "    (md_len, \"user.location\"),\n",
    "    (md_len, \"display_text_range\"),\n",
    "    (md_len, \"extended_tweet.entities.urls\"),\n",
    "    (md_len, \"extended_tweet.entities.hashtags\"),\n",
    "    (md_len, \"extended_tweet.entities.user_mentions\"),\n",
    "    \n",
    "    (md_time, \"created_at\"),\n",
    "    (md_time, \"user.created_at\"),\n",
    "    (md_time, \"quoted_status.created_at\"),\n",
    "    (md_time, \"quoted_status.user.created_at\"),\n",
    "    \n",
    "    (md_num, \"user.statuses_count\"),\n",
    "    (md_num, \"user.listed_count\"),\n",
    "    (md_num, \"user.favourites_count\"),\n",
    "    (md_num, \"user.profile_background_tile\"),\n",
    "    (md_num, \"quoted_status.quote_count\"),\n",
    "    (md_num, \"quoted_status.user.followers_count\"),\n",
    "    (md_num, \"quoted_status.user.favourites_count\"),\n",
    "]\n",
    "\n",
    "METADATA_DIM = len(METADATA)\n",
    "\n",
    "def extract_metadata(df: pd.DataFrame) -> torch.Tensor:\n",
    "    md: list[pd.Series] = []\n",
    "\n",
    "    for fn, col_name in METADATA:\n",
    "        md.append(fn(df[col_name]))\n",
    "\n",
    "    return torch.from_numpy(np.array(md)).transpose(0, 1).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1bc90d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    texts: list[str]\n",
    "    desc_texts: list[str]\n",
    "    metadata: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "    device: torch.device\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, labels: pd.Series, device: torch.device = device):\n",
    "        self.texts = df[\"full_text\"].tolist()\n",
    "        self.desc_texts = df[\"user.description\"].fillna(\"\").tolist()\n",
    "        self.metadata = extract_metadata(df).to(device)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long, device=device)\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"text\": self.texts[idx],\n",
    "            \"metadata\": self.metadata[idx],\n",
    "            \"description\": self.desc_texts[idx],\n",
    "            \"label\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts = [x[\"text\"] for x in batch]\n",
    "    metadata = torch.stack([x[\"metadata\"] for x in batch])\n",
    "    description_texts = [x[\"description\"] for x in batch]\n",
    "    labels = torch.stack([x[\"label\"] for x in batch])\n",
    "    return texts, metadata, description_texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7a47ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: TweetClassifier,\n",
    "    train_ds: Dataset,\n",
    "    val_ds: Dataset,\n",
    "    epochs: int = 3,\n",
    "    lr: float = 2e-4,\n",
    "    weight_decay: float = 0.01,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    "    patience: int = 3, # How many epochs to wait for improvement\n",
    "    optimizer: torch.optim.Optimizer | None = None,\n",
    ") -> TweetClassifier:\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    val_loader = DataLoader( # New: Validation DataLoader\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    if optimizer is None:\n",
    "        \n",
    "        # Slow learning rate for the description text encoder\n",
    "        ENCODER_LR = lr * 0.1\n",
    "        \n",
    "        # Define parameter groups\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": model.desc_encoder.parameters(), # The large, pre-trained encoder\n",
    "                \"lr\": ENCODER_LR, \n",
    "                \"weight_decay\": weight_decay,\n",
    "            },\n",
    "            {\n",
    "                # The \"Head\": FC layers, BatchNorm, Tokenizer, and other new parts\n",
    "                \"params\": [\n",
    "                    p for n, p in model.named_parameters() \n",
    "                    if p.requires_grad and \"desc_encoder\" not in n\n",
    "                ],\n",
    "                \"lr\": lr, # Use the default, higher LR (e.g., 2e-4)\n",
    "                \"weight_decay\": weight_decay,\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        optimizer = AdamW(optimizer_grouped_parameters)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize best loss and accuracy for early stopping logic\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        status_bar = tqdm(train_loader, desc=\"Training\")\n",
    "\n",
    "        for texts, metadata, desc_texts, labels in status_bar:\n",
    "            texts: list[str]\n",
    "            metadata: torch.Tensor\n",
    "            desc_texts: list[str]\n",
    "            labels: torch.Tensor\n",
    "            \n",
    "            metadata = metadata.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            out = model(\n",
    "                texts=texts,\n",
    "                metadata=metadata,\n",
    "                desc_texts=desc_texts\n",
    "            )\n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)  # TODO: ?\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            status_bar.set_postfix({\"loss\": total_loss / (status_bar.n + 1)})\n",
    "\n",
    "        print(f\"Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "        # The following validation set evaluation code courtesy of Gemini (AI)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        val_status_bar = tqdm(val_loader, desc=\"Validating\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for texts, metadata, desc_texts, labels in val_status_bar:\n",
    "                metadata = metadata.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                out = model(\n",
    "                    texts=texts,\n",
    "                    metadata=metadata,\n",
    "                    desc_texts=desc_texts\n",
    "                )\n",
    "                logits = out[\"logits\"]\n",
    "                \n",
    "                loss = criterion(logits, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = val_correct / total_samples\n",
    "        \n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # end of gemini code\n",
    "\n",
    "        # --- Early Stopping Logic --- (also Gemini code)\n",
    "        if val_loss < best_val_loss:\n",
    "            print(f\"Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. Saving model.\")\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # CRITICAL: Save the model's state dictionary\n",
    "            torch.save(model.state_dict(), \"best_model_state.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation loss did not improve. Patience: {patience_counter}/{patience}\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {patience} epochs of no improvement.\")\n",
    "                break # Exit the training loop\n",
    "        # --- End Early Stopping ---\n",
    "    \n",
    "    # Load the best weights before returning the model\n",
    "    if patience_counter >= patience:\n",
    "        model.load_state_dict(torch.load(\"best_model_state.pt\"))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91fa5e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: TweetClassifier,\n",
    "    val_ds: Dataset,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> tuple[float, float]:\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        status_bar = tqdm(val_loader, desc=\"Evaluating\")\n",
    "        \n",
    "        for texts, metadata, desc_texts, labels in status_bar:\n",
    "            texts: list[str]\n",
    "            metadata: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "            metadata = metadata.to(device)\n",
    "            desc_texts: list[str]\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = model(\n",
    "                texts=texts,\n",
    "                metadata=metadata,\n",
    "                desc_texts=desc_texts\n",
    "            )\n",
    "            logits: torch.Tensor = out[\"logits\"]\n",
    "            \n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            count += labels.size(0)\n",
    "            \n",
    "            status_bar.set_postfix({\"loss\": total_loss / (status_bar.n + 1), \"acc\": correct / count})\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / len(val_loader),\n",
    "        \"acc\": correct / count,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "558a176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_model(\n",
    "    model: TweetClassifier,\n",
    "    df: pd.DataFrame,\n",
    "    out_file: pathlib.Path | str | None = None,\n",
    "    device: torch.device = device,\n",
    "    batch_size: int = 32,\n",
    ") -> torch.Tensor:\n",
    "    data_loader = DataLoader(\n",
    "        TweetDataset(df, torch.zeros(len(df), dtype=torch.long, device=device), device=device),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = torch.zeros(len(df), dtype=torch.long)\n",
    "    cur_idx = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, metadata, desc_texts, _ in tqdm(data_loader, desc=\"Inferring\"):\n",
    "            texts: list[str]\n",
    "            metadata: torch.Tensor\n",
    "            desc_texts: list[str]\n",
    "            metadata = metadata.to(device)\n",
    "\n",
    "            out = model(\n",
    "                texts=texts,\n",
    "                metadata=metadata,\n",
    "                desc_texts=desc_texts\n",
    "            )\n",
    "            logits: torch.Tensor = out[\"logits\"].cpu()\n",
    "            \n",
    "            predictions[cur_idx:cur_idx+len(texts)] = logits.argmax(dim=-1)\n",
    "            cur_idx += len(texts)\n",
    "    \n",
    "    if out_file is not None:\n",
    "        output = pd.concat([df[\"challenge_id\"], pd.DataFrame(predictions)], axis=1, ignore_index=True)\n",
    "        output.columns = [\"ID\", \"Prediction\"]\n",
    "        output.to_csv(out_file, index=False)\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55cd1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TweetClassifier(\n",
    "    # pretrained_encoder=\"camembert-base\",\n",
    "    metadata_dim=METADATA_DIM,\n",
    "    hidden_dim=128,\n",
    "    # max_length=256\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f1b1bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136743/489471857.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return col.apply(safe_len).fillna(0)\n",
      "/tmp/ipykernel_136743/489471857.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return tmp.fillna(tmp.mean())\n"
     ]
    }
   ],
   "source": [
    "# Split training and validation datasets by user_id, so that there is no data leakage.\n",
    "# We were getting overly optimistic results because the same profile descriptions\n",
    "# were appearing in both the training set and validation set\n",
    "\n",
    "# 1. Get the unique user IDs from the training data\n",
    "unique_users = X_train['id_str'].unique()\n",
    "\n",
    "# 2. Split the unique user IDs into training and validation groups\n",
    "# We use train_test_split from sklearn here\n",
    "train_user_ids, val_user_ids = train_test_split(\n",
    "    unique_users, \n",
    "    test_size=0.1, # Keep 10% for validation\n",
    "    random_state=42 # Ensure reproducible split\n",
    ")\n",
    "\n",
    "# 3. Filter the original dataframes based on these IDs\n",
    "# X_train, y_train are the data used for the full training set (before random_split)\n",
    "X_train_filtered = X_train[X_train['id_str'].isin(train_user_ids)]\n",
    "y_train_filtered = y_train[X_train['id_str'].isin(train_user_ids)]\n",
    "\n",
    "X_val_filtered = X_train[X_train['id_str'].isin(val_user_ids)]\n",
    "y_val_filtered = y_train[X_train['id_str'].isin(val_user_ids)]\n",
    "\n",
    "# Remove 'id_str' columns cuz we don't want to train on it\n",
    "# (but we needed it for splitting the data)\n",
    "X_train_filtered = X_train_filtered.drop(columns=['id_str'])\n",
    "X_val_filtered = X_val_filtered.drop(columns=['id_str'])\n",
    "\n",
    "# Create the datasets using the newly filtered DataFrames\n",
    "train_ds = TweetDataset(X_train_filtered, y_train_filtered, device=device)\n",
    "val_ds = TweetDataset(X_val_filtered, y_val_filtered, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c1218d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.7305, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.md_batchnorm(full_train_ds.metadata)[760, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb6c3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(version: str) -> pathlib.Path:\n",
    "    return pathlib.Path(\n",
    "        f\"./models/{version}/model-{version}.pt\"\n",
    "        if not IS_KAGGLE\n",
    "        else f\"/kaggle/input/model-{version}-pt/model-{version}.pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f699b6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [02:01<00:00, 17.94it/s, loss=0.518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 243/243 [00:04<00:00, 60.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4657, Validation Accuracy: 0.7835\n",
      "Validation loss improved from inf to 0.4657. Saving model.\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [02:27<00:00, 14.78it/s, loss=0.444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 243/243 [00:04<00:00, 55.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4254, Validation Accuracy: 0.8088\n",
      "Validation loss improved from 0.4657 to 0.4254. Saving model.\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [02:34<00:00, 14.13it/s, loss=0.404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 243/243 [00:04<00:00, 52.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3788, Validation Accuracy: 0.8337\n",
      "Validation loss improved from 0.4254 to 0.3788. Saving model.\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [02:32<00:00, 14.25it/s, loss=0.358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 243/243 [00:04<00:00, 56.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3278, Validation Accuracy: 0.8619\n",
      "Validation loss improved from 0.3788 to 0.3278. Saving model.\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2179/2179 [02:25<00:00, 15.00it/s, loss=0.315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 243/243 [00:04<00:00, 56.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2917, Validation Accuracy: 0.8849\n",
      "Validation loss improved from 0.3278 to 0.2917. Saving model.\n"
     ]
    }
   ],
   "source": [
    "model_path = get_model_path(\"v7\")\n",
    "\n",
    "# model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "model = train_model(model, train_ds, val_ds, lr=2e-5, epochs=5, batch_size=64, device=device)\n",
    "torch.save(model.state_dict(), \"model-v7.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cb1426f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 243/243 [00:03<00:00, 66.96it/s, loss=0.292, acc=0.885]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.2916675413893574, 'acc': 0.8849157684115407}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Pre-encode all texts?\n",
    "evaluate_model(model, val_ds, batch_size=64, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0d3dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136743/489471857.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return col.apply(safe_len).fillna(0)\n",
      "/tmp/ipykernel_136743/489471857.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return tmp.fillna(tmp.mean())\n",
      "/tmp/ipykernel_136743/982839750.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
      "Inferring: 100%|██████████| 1616/1616 [00:25<00:00, 64.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_with_model(model, X_kaggle, batch_size=64, out_file=\"predictions-v7.csv\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa751f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
